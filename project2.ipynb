{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3f7987",
   "metadata": {
    "papermill": {
     "duration": 0.003214,
     "end_time": "2025-10-05T13:54:56.336109",
     "exception": false,
     "start_time": "2025-10-05T13:54:56.332895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "10add679",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-05T13:54:56.342314Z",
     "iopub.status.busy": "2025-10-05T13:54:56.342060Z",
     "iopub.status.idle": "2025-10-05T13:55:03.760472Z",
     "shell.execute_reply": "2025-10-05T13:55:03.759862Z"
    },
    "papermill": {
     "duration": 7.423101,
     "end_time": "2025-10-05T13:55:03.761952",
     "exception": false,
     "start_time": "2025-10-05T13:54:56.338851",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:55.705687Z",
     "start_time": "2025-10-05T14:30:55.700174Z"
    }
   },
   "source": [
    "# === Imports & Configuration ===\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Try to import LightGBM and XGBoost if available\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    lgb = None\n",
    "    LGB_AVAILABLE = False\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    xgb = None\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "# === Directories ===\n",
    "# Use local project structure. Set BASE_DIR to the current project root and prefer local `data/` and `train/` folders.\n",
    "BASE_DIR = os.path.abspath(os.getcwd())\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "# Use the existing `results/` folder (already present in workspace) for outputs; fall back to a working folder if missing\n",
    "OUT_DIR = os.path.join(BASE_DIR, \"results\")\n",
    "CLEANED_DIR = os.path.join(OUT_DIR, \"cleaned_originals\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "# === Helper Functions ===\n",
    "def save_csv(df, name):\n",
    "    path = os.path.join(OUT_DIR, name)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"✅ Saved {name} | shape: {df.shape} | path: {path}\")\n",
    "    return path\n",
    "\n",
    "# A robust RMSE helper that works with different sklearn versions\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def safe_rmse(y_true, y_pred):\n",
    "    try:\n",
    "        # newer sklearn supports squared=False\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "    except TypeError:\n",
    "        # fallback to sqrt of MSE\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def two_stage_score(y_true, y_pred):\n",
    "    \"\"\"Simple fallback score: RMSE on raw values. Keeps interface used later.\n",
    "    Replace with task-specific metric if available.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    # guard shapes\n",
    "    if y_true.size == 0:\n",
    "        return 0.0\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "6ed98aa9",
   "metadata": {
    "papermill": {
     "duration": 0.002268,
     "end_time": "2025-10-05T13:55:03.767150",
     "exception": false,
     "start_time": "2025-10-05T13:55:03.764882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Load All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "5bc9ba29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:03.773303Z",
     "iopub.status.busy": "2025-10-05T13:55:03.772821Z",
     "iopub.status.idle": "2025-10-05T13:55:03.957492Z",
     "shell.execute_reply": "2025-10-05T13:55:03.956472Z"
    },
    "papermill": {
     "duration": 0.189277,
     "end_time": "2025-10-05T13:55:03.958951",
     "exception": false,
     "start_time": "2025-10-05T13:55:03.769674",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:30:57.199709Z",
     "start_time": "2025-10-05T14:30:57.151556Z"
    }
   },
   "source": [
    "# === Load all raw datasets (try train/, data/ and project root) ===\n",
    "files = {\n",
    "    # These files are expected in the 'train' subdirectory\n",
    "    \"new_house\": \"new_house_transactions.csv\",\n",
    "    \"new_house_near\": \"new_house_transactions_nearby_sectors.csv\",\n",
    "    \"pre_owned\": \"pre_owned_house_transactions.csv\",\n",
    "    \"pre_owned_near\": \"pre_owned_house_transactions_nearby_sectors.csv\",\n",
    "    \"land\": \"land_transactions.csv\",\n",
    "    \"land_near\": \"land_transactions_nearby_sectors.csv\",\n",
    "    \"poi\": \"sector_POI.csv\",\n",
    "    \"city_search\": \"city_search_index.csv\",\n",
    "    \"city_index\": \"city_indexes.csv\",\n",
    "    \n",
    "    # These files may live in the data/ folder or project root\n",
    "    \"test\": \"test.csv\",\n",
    "    \"sample_sub\": \"sample_submission.csv\"\n",
    "}\n",
    "\n",
    "data = {}\n",
    "for key, fname in files.items():\n",
    "    # try candidate paths in order: train/, data/, project root\n",
    "    candidates = [os.path.join(TRAIN_DIR, fname), os.path.join(DATA_DIR, fname), os.path.join(BASE_DIR, fname)]\n",
    "    found = None\n",
    "    for p in candidates:\n",
    "        if os.path.exists(p):\n",
    "            found = p\n",
    "            break\n",
    "    if found is not None:\n",
    "        # use low_memory=False to avoid dtype inference warnings on large csvs\n",
    "        df = pd.read_csv(found, low_memory=False)\n",
    "        print(f\"Loaded {fname} → {df.shape} from {found}\")\n",
    "        data[key] = df\n",
    "    else:\n",
    "        print(f\"⚠️ Missing: {fname}. Tried: {candidates}\")\n",
    "\n",
    "# Quick dataset summary cell (separate cell for diagnostics)\n",
    "print('\\n=== Dataset summary ===')\n",
    "for k, df in data.items():\n",
    "    print(f\"{k}: {df.shape} | columns: {list(df.columns)[:8]}{('...' if df.shape[1]>8 else '')}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded new_house_transactions.csv → (5433, 11) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\new_house_transactions.csv\n",
      "Loaded new_house_transactions_nearby_sectors.csv → (5360, 11) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\new_house_transactions_nearby_sectors.csv\n",
      "Loaded pre_owned_house_transactions.csv → (5360, 6) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\pre_owned_house_transactions.csv\n",
      "Loaded pre_owned_house_transactions_nearby_sectors.csv → (5427, 6) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\pre_owned_house_transactions_nearby_sectors.csv\n",
      "Loaded land_transactions.csv → (5896, 6) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\land_transactions.csv\n",
      "Loaded land_transactions_nearby_sectors.csv → (5025, 6) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\land_transactions_nearby_sectors.csv\n",
      "Loaded sector_POI.csv → (86, 142) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\sector_POI.csv\n",
      "Loaded city_search_index.csv → (4020, 4) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\city_search_index.csv\n",
      "Loaded city_indexes.csv → (7, 74) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\train\\city_indexes.csv\n",
      "Loaded test.csv → (1152, 2) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\test.csv\n",
      "Loaded sample_submission.csv → (1152, 2) from C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\data\\sample_submission.csv\n",
      "\n",
      "=== Dataset summary ===\n",
      "new_house: (5433, 11) | columns: ['month', 'sector', 'num_new_house_transactions', 'area_new_house_transactions', 'price_new_house_transactions', 'amount_new_house_transactions', 'area_per_unit_new_house_transactions', 'total_price_per_unit_new_house_transactions']...\n",
      "new_house_near: (5360, 11) | columns: ['month', 'sector', 'num_new_house_transactions_nearby_sectors', 'area_new_house_transactions_nearby_sectors', 'price_new_house_transactions_nearby_sectors', 'amount_new_house_transactions_nearby_sectors', 'area_per_unit_new_house_transactions_nearby_sectors', 'total_price_per_unit_new_house_transactions_nearby_sectors']...\n",
      "pre_owned: (5360, 6) | columns: ['month', 'sector', 'area_pre_owned_house_transactions', 'amount_pre_owned_house_transactions', 'num_pre_owned_house_transactions', 'price_pre_owned_house_transactions']\n",
      "pre_owned_near: (5427, 6) | columns: ['month', 'sector', 'num_pre_owned_house_transactions_nearby_sectors', 'area_pre_owned_house_transactions_nearby_sectors', 'amount_pre_owned_house_transactions_nearby_sectors', 'price_pre_owned_house_transactions_nearby_sectors']\n",
      "land: (5896, 6) | columns: ['month', 'sector', 'num_land_transactions', 'construction_area', 'planned_building_area', 'transaction_amount']\n",
      "land_near: (5025, 6) | columns: ['month', 'sector', 'num_land_transactions_nearby_sectors', 'construction_area_nearby_sectors', 'planned_building_area_nearby_sectors', 'transaction_amount_nearby_sectors']\n",
      "poi: (86, 142) | columns: ['sector', 'sector_coverage', 'population_scale', 'residential_area', 'office_building', 'commercial_area', 'resident_population', 'office_population']...\n",
      "city_search: (4020, 4) | columns: ['month', 'keyword', 'source', 'search_volume']\n",
      "city_index: (7, 74) | columns: ['city_indicator_data_year', 'year_end_registered_population_10k', 'total_households_10k', 'year_end_resident_population_10k', 'year_end_total_employed_population_10k', 'year_end_urban_non_private_employees_10k', 'private_individual_and_other_employees_10k', 'private_individual_ratio']...\n",
      "test: (1152, 2) | columns: ['id', 'new_house_transaction_amount']\n",
      "sample_sub: (1152, 2) | columns: ['id', 'new_house_transaction_amount']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "97acb27b",
   "metadata": {
    "papermill": {
     "duration": 0.002449,
     "end_time": "2025-10-05T13:55:03.964113",
     "exception": false,
     "start_time": "2025-10-05T13:55:03.961664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "id": "71f3ac64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:03.969970Z",
     "iopub.status.busy": "2025-10-05T13:55:03.969745Z",
     "iopub.status.idle": "2025-10-05T13:55:04.191374Z",
     "shell.execute_reply": "2025-10-05T13:55:04.190641Z"
    },
    "papermill": {
     "duration": 0.226054,
     "end_time": "2025-10-05T13:55:04.192596",
     "exception": false,
     "start_time": "2025-10-05T13:55:03.966542",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:31:00.412250Z",
     "start_time": "2025-10-05T14:31:00.288690Z"
    }
   },
   "source": [
    "# === Clean Each Dataset Individually ===\n",
    "cleaned = {}\n",
    "\n",
    "# Generic cleaner\n",
    "def clean_transactions(df):\n",
    "    numcols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numcols] = df[numcols].fillna(0)\n",
    "    for c in df.select_dtypes(include=['object']).columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "for key in [\"new_house\",\"new_house_near\",\"pre_owned\",\"pre_owned_near\",\"land\",\"land_near\"]:\n",
    "    if key in data:\n",
    "        df = clean_transactions(data[key])\n",
    "        cleaned[key] = df\n",
    "        save_csv(df, f\"{key}_clean.csv\")\n",
    "\n",
    "# POI: Drop sparse cols, fill 0\n",
    "poi = data[\"poi\"]\n",
    "miss = poi.isna().mean()\n",
    "poi_clean = poi.drop(columns=miss[miss>0.7].index).fillna(0)\n",
    "cleaned[\"poi\"] = poi_clean\n",
    "save_csv(poi_clean, \"sector_POI_clean.csv\")\n",
    "\n",
    "# City Search Index\n",
    "cs = data[\"city_search\"]\n",
    "cs_clean = cs.groupby(\"month\", as_index=False)[\"search_volume\"].sum().rename(columns={\"search_volume\":\"city_search_volume\"})\n",
    "cleaned[\"city_search\"] = cs_clean\n",
    "save_csv(cs_clean, \"city_search_index_clean.csv\")\n",
    "\n",
    "# City Index\n",
    "ci = data[\"city_index\"]\n",
    "ci_clean = ci.dropna(thresh=int(ci.shape[0]*0.2), axis=1).fillna(method='ffill').fillna(method='bfill')\n",
    "cleaned[\"city_index\"] = ci_clean\n",
    "save_csv(ci_clean, \"city_indexes_clean.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new_house_clean.csv | shape: (5433, 11) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\new_house_clean.csv\n",
      "✅ Saved new_house_near_clean.csv | shape: (5360, 11) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\new_house_near_clean.csv\n",
      "✅ Saved pre_owned_clean.csv | shape: (5360, 6) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\pre_owned_clean.csv\n",
      "✅ Saved pre_owned_near_clean.csv | shape: (5427, 6) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\pre_owned_near_clean.csv\n",
      "✅ Saved land_clean.csv | shape: (5896, 6) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\land_clean.csv\n",
      "✅ Saved land_near_clean.csv | shape: (5025, 6) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\land_near_clean.csv\n",
      "✅ Saved sector_POI_clean.csv | shape: (86, 142) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\sector_POI_clean.csv\n",
      "✅ Saved city_search_index_clean.csv | shape: (67, 2) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\city_search_index_clean.csv\n",
      "✅ Saved city_indexes_clean.csv | shape: (7, 73) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\city_indexes_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mitudru\\\\Documents\\\\ML Project\\\\realestateprediction\\\\results\\\\city_indexes_clean.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "93a8a483",
   "metadata": {
    "papermill": {
     "duration": 0.002421,
     "end_time": "2025-10-05T13:55:04.197762",
     "exception": false,
     "start_time": "2025-10-05T13:55:04.195341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Merge and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4117718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:04.203664Z",
     "iopub.status.busy": "2025-10-05T13:55:04.203252Z",
     "iopub.status.idle": "2025-10-05T13:55:04.791161Z",
     "shell.execute_reply": "2025-10-05T13:55:04.790593Z"
    },
    "papermill": {
     "duration": 0.592061,
     "end_time": "2025-10-05T13:55:04.792241",
     "exception": false,
     "start_time": "2025-10-05T13:55:04.200180",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:31:02.591932Z",
     "start_time": "2025-10-05T14:31:02.245553Z"
    }
   },
   "source": [
    "# === Merge All Cleaned Datasets ===\n",
    "\n",
    "def aggregate(df):\n",
    "    if \"month\" not in df.columns or \"sector\" not in df.columns:\n",
    "        return None\n",
    "    numcols = df.select_dtypes(include=[np.number]).columns\n",
    "    return df.groupby([\"month\",\"sector\"])[numcols].sum().reset_index()\n",
    "\n",
    "agg_new = aggregate(cleaned[\"new_house\"])\n",
    "agg_pre = aggregate(cleaned[\"pre_owned\"])\n",
    "agg_land = aggregate(cleaned[\"land\"])\n",
    "\n",
    "merged = agg_new.copy()\n",
    "def merge_pref(base, other, pref):\n",
    "    if other is None: return base\n",
    "    otherc = other.copy()\n",
    "    for c in otherc.columns:\n",
    "        if c not in [\"month\",\"sector\"]:\n",
    "            otherc.rename(columns={c: f\"{pref}_{c}\"}, inplace=True)\n",
    "    return base.merge(otherc, on=[\"month\",\"sector\"], how=\"left\")\n",
    "\n",
    "merged = merge_pref(merged, agg_pre, \"pre\")\n",
    "merged = merge_pref(merged, agg_land, \"land\")\n",
    "\n",
    "# Add City & POI\n",
    "merged = merged.merge(cleaned[\"city_search\"], on=\"month\", how=\"left\")\n",
    "merged = merged.merge(cleaned[\"poi\"], on=\"sector\", how=\"left\")\n",
    "\n",
    "save_csv(merged, \"merged_full.csv\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved merged_full.csv | shape: (5433, 161) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\merged_full.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Mitudru\\\\Documents\\\\ML Project\\\\realestateprediction\\\\results\\\\merged_full.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "0080d678",
   "metadata": {
    "papermill": {
     "duration": 0.002483,
     "end_time": "2025-10-05T13:55:04.798130",
     "exception": false,
     "start_time": "2025-10-05T13:55:04.795647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "id": "42e5e92e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:04.804092Z",
     "iopub.status.busy": "2025-10-05T13:55:04.803875Z",
     "iopub.status.idle": "2025-10-05T13:55:05.452376Z",
     "shell.execute_reply": "2025-10-05T13:55:05.451624Z"
    },
    "papermill": {
     "duration": 0.652915,
     "end_time": "2025-10-05T13:55:05.453580",
     "exception": false,
     "start_time": "2025-10-05T13:55:04.800665",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:31:05.921221Z",
     "start_time": "2025-10-05T14:31:05.267592Z"
    }
   },
   "source": [
    "# === Feature Engineering ===\n",
    "m = merged.copy()\n",
    "# Ensure month parsing is robust: try to convert month to datetime, otherwise keep as string\n",
    "if \"month\" in m.columns:\n",
    "    m[\"month_str\"] = m[\"month\"].astype(str)\n",
    "    # pandas may not accept `infer_datetime_format` in some versions; keep only `errors`.\n",
    "    m[\"month_dt\"] = pd.to_datetime(m[\"month_str\"], errors=\"coerce\")\n",
    "    # fall back to extracting number from formats like '2019-04' or '2019-Apr'\n",
    "    m[\"month_num\"] = m[\"month_dt\"].dt.month.fillna(\n",
    "        m[\"month_str\"].str.extract(r\"-(\\d{1,2})$\", expand=False).astype(float).fillna(0)\n",
    "    ).astype(int)\n",
    "    m[\"season\"] = ((m[\"month_num\"] % 12) // 3) + 1\n",
    "else:\n",
    "    m[\"month_str\"] = \"\"\n",
    "    m[\"month_num\"] = 0\n",
    "    m[\"season\"] = 0\n",
    "\n",
    "# Create lags and rolling means\n",
    "for col in [\"num_new_house_transactions\",\"area_new_house_transactions\",\"price_new_house_transactions\",\"amount_new_house_transactions\"]:\n",
    "    if col in m.columns:\n",
    "        for lag in [1,3,6]:\n",
    "            m[f\"{col}_lag{lag}\"] = m.groupby(\"sector\")[col].shift(lag).fillna(0)\n",
    "        m[f\"{col}_roll3\"] = m.groupby(\"sector\")[col].rolling(3, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Supply-demand ratio\n",
    "if \"num_new_house_available_for_sale\" in m.columns:\n",
    "    m[\"supply_demand_ratio\"] = m[\"num_new_house_available_for_sale\"] / (m[\"num_new_house_transactions\"] + 1)\n",
    "    m[\"supply_demand_ratio\"].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "# Target\n",
    "if \"amount_new_house_transactions\" in m.columns:\n",
    "    m[\"y_log1p\"] = np.log1p(m[\"amount_new_house_transactions\"].clip(lower=0))\n",
    "else:\n",
    "    m[\"y_log1p\"] = 0\n",
    "\n",
    "save_csv(m, \"merged_engineered.csv\")\n",
    "\n",
    "# Diagnostics: compute correlations on numeric columns only (avoid ValueError from strings like '2019-Apr')\n",
    "if \"amount_new_house_transactions\" in m.columns:\n",
    "    numeric = m.select_dtypes(include=[np.number])\n",
    "    if \"amount_new_house_transactions\" in numeric.columns:\n",
    "        target = \"amount_new_house_transactions\"\n",
    "        corr = numeric.corr()[target].abs().sort_values(ascending=False)\n",
    "        print('\\nTop correlated features:')\n",
    "        print(corr.head(20))\n",
    "    else:\n",
    "        print('\\nTarget is not numeric in numeric subset, skipping correlation')\n",
    "else:\n",
    "    print('\\nTarget column not found in merged, skipping correlation')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved merged_engineered.csv | shape: (5433, 183) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\merged_engineered.csv\n",
      "\n",
      "Top correlated features:\n",
      "amount_new_house_transactions            1.000000\n",
      "area_new_house_transactions              0.871142\n",
      "amount_new_house_transactions_roll3      0.847126\n",
      "num_new_house_transactions               0.846628\n",
      "area_new_house_transactions_roll3        0.736229\n",
      "num_new_house_transactions_roll3         0.716164\n",
      "y_log1p                                  0.693480\n",
      "amount_new_house_transactions_lag1       0.583104\n",
      "area_new_house_transactions_lag1         0.570533\n",
      "num_new_house_transactions_lag1          0.557265\n",
      "amount_new_house_transactions_lag3       0.511882\n",
      "area_new_house_transactions_lag3         0.507815\n",
      "area_new_house_available_for_sale        0.499343\n",
      "num_new_house_transactions_lag3          0.495415\n",
      "amount_new_house_transactions_lag6       0.495047\n",
      "num_new_house_available_for_sale         0.492418\n",
      "area_new_house_transactions_lag6         0.483335\n",
      "num_new_house_transactions_lag6          0.469167\n",
      "pre_area_pre_owned_house_transactions    0.468158\n",
      "pre_num_pre_owned_house_transactions     0.464200\n",
      "Name: amount_new_house_transactions, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "c3ccee6a",
   "metadata": {
    "papermill": {
     "duration": 0.002734,
     "end_time": "2025-10-05T13:55:05.459403",
     "exception": false,
     "start_time": "2025-10-05T13:55:05.456669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Model Training (two stage)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab8b531c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:05.465744Z",
     "iopub.status.busy": "2025-10-05T13:55:05.465509Z",
     "iopub.status.idle": "2025-10-05T13:55:12.556688Z",
     "shell.execute_reply": "2025-10-05T13:55:12.556063Z"
    },
    "papermill": {
     "duration": 7.096059,
     "end_time": "2025-10-05T13:55:12.558093",
     "exception": false,
     "start_time": "2025-10-05T13:55:05.462034",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:32:05.112577Z",
     "start_time": "2025-10-05T14:32:00.703875Z"
    }
   },
   "source": [
    "# === Two-Stage Model: Classifier + Regressor ===\n",
    "\n",
    "if \"amount_new_house_transactions\" in m.columns:\n",
    "    # use np.where to avoid static-analysis warnings and ensure integer dtype\n",
    "    m[\"is_zero\"] = np.where(m[\"amount_new_house_transactions\"] == 0, 1, 0).astype('int8')\n",
    "else:\n",
    "    # create zero column so downstream code doesn't break\n",
    "    m[\"is_zero\"] = np.zeros(len(m), dtype='int8')\n",
    "\n",
    "# Split time-based\n",
    "months = sorted(m[\"month\"].dropna().unique()) if \"month\" in m.columns else []\n",
    "val_months = months[-6:] if len(months) > 0 else []\n",
    "train_mask = ~m[\"month\"].isin(val_months) if len(val_months) > 0 else np.ones(len(m), dtype=bool)\n",
    "val_mask = m[\"month\"].isin(val_months) if len(val_months) > 0 else np.zeros(len(m), dtype=bool)\n",
    "\n",
    "# Use only numeric features for training\n",
    "X = m.select_dtypes(include=[np.number]).drop(columns=[c for c in [\"y_log1p\"] if c in m.columns], errors='ignore')\n",
    "y_clf = m[\"is_zero\"]\n",
    "y_reg = m[\"y_log1p\"]\n",
    "\n",
    "X_train, X_val = X[train_mask], X[val_mask]\n",
    "y_clf_train, y_clf_val = y_clf[train_mask], y_clf[val_mask]\n",
    "y_reg_train, y_reg_val = y_reg[train_mask], y_reg[val_mask]\n",
    "\n",
    "# --- Classifier ---\n",
    "if LGB_AVAILABLE:\n",
    "    dtrain = lgb.Dataset(X_train, label=y_clf_train)\n",
    "    dval = lgb.Dataset(X_val, label=y_clf_val)\n",
    "    \n",
    "    params_clf = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 64,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    \n",
    "    # Use callback-based early stopping (works for all versions)\n",
    "    clf = lgb.train(\n",
    "        params_clf,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[dtrain, dval],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    p_zero_val = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "else:\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_clf_train)\n",
    "    p_zero_val = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --- Regressor ---\n",
    "if LGB_AVAILABLE:\n",
    "    dtrain_r = lgb.Dataset(X_train, label=y_reg_train)\n",
    "    dval_r = lgb.Dataset(X_val, label=y_reg_val)\n",
    "    \n",
    "    params_reg = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"num_leaves\": 128,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    \n",
    "    reg = lgb.train(\n",
    "        params_reg,\n",
    "        dtrain_r,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[dtrain_r, dval_r],\n",
    "        valid_names=[\"train\", \"val\"],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    p_reg_val = np.expm1(reg.predict(X_val, num_iteration=reg.best_iteration))\n",
    "else:\n",
    "    reg = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    reg.fit(X_train, y_reg_train)\n",
    "    p_reg_val = np.expm1(reg.predict(X_val))\n",
    "\n",
    "# --- Final predictions ---\n",
    "p_final_val = (1 - p_zero_val) * p_reg_val\n",
    "y_true = m.loc[val_mask, \"amount_new_house_transactions\"].values\n",
    "\n",
    "score = two_stage_score(y_true, p_final_val)\n",
    "mae = mean_absolute_error(y_true, p_final_val)\n",
    "rmse = safe_rmse(y_true, p_final_val)\n",
    "\n",
    "print(f\"✅ Validation Score: {score:.5f} | MAE: {mae:.2f} | RMSE: {rmse:.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 1\tval's auc: 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's l1: 0.0612232\tval's l1: 0.059071\n",
      "[200]\ttrain's l1: 0.00837149\tval's l1: 0.014474\n",
      "[300]\ttrain's l1: 0.00547851\tval's l1: 0.0139353\n",
      "[400]\ttrain's l1: 0.00403634\tval's l1: 0.0139093\n",
      "Early stopping, best iteration is:\n",
      "[381]\ttrain's l1: 0.00425181\tval's l1: 0.0138899\n",
      "✅ Validation Score: 2216.40549 | MAE: 432.68 | RMSE: 2216.41\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "5aad0db3",
   "metadata": {
    "papermill": {
     "duration": 0.004856,
     "end_time": "2025-10-05T13:55:12.567672",
     "exception": false,
     "start_time": "2025-10-05T13:55:12.562816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Final Submission File"
   ]
  },
  {
   "cell_type": "code",
   "id": "20a75fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T13:55:12.574969Z",
     "iopub.status.busy": "2025-10-05T13:55:12.574707Z",
     "iopub.status.idle": "2025-10-05T13:55:12.640972Z",
     "shell.execute_reply": "2025-10-05T13:55:12.639843Z"
    },
    "papermill": {
     "duration": 0.071392,
     "end_time": "2025-10-05T13:55:12.642280",
     "exception": false,
     "start_time": "2025-10-05T13:55:12.570888",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-05T14:32:09.718153Z",
     "start_time": "2025-10-05T14:32:09.677664Z"
    }
   },
   "source": [
    "# === Predict on Test and Build Submission (FINAL FIXED VERSION) ===\n",
    "\n",
    "# Keep original test copy before feature alignment\n",
    "test_original = data[\"test\"].copy()\n",
    "\n",
    "# --- Ensure feature alignment ---\n",
    "train_features = X.columns.tolist()  # features used during training\n",
    "missing_in_test = [f for f in train_features if f not in test_original.columns]\n",
    "extra_in_test = [f for f in test_original.columns if f not in train_features]\n",
    "\n",
    "# Fill missing training features in test with 0\n",
    "for f in missing_in_test:\n",
    "    test_original[f] = 0\n",
    "\n",
    "# Create aligned version for prediction\n",
    "X_test = test_original[train_features].fillna(0)\n",
    "\n",
    "print(f\"✅ Test alignment done: {len(train_features)} features used.\")\n",
    "print(f\"   → Added {len(missing_in_test)} missing columns.\")\n",
    "print(f\"   → Dropped {len(extra_in_test)} extra columns.\")\n",
    "\n",
    "# --- Predictions ---\n",
    "if LGB_AVAILABLE:\n",
    "    p_zero_test = clf.predict(X_test, num_iteration=getattr(clf, \"best_iteration\", None))\n",
    "    p_reg_test = np.expm1(reg.predict(X_test, num_iteration=getattr(reg, \"best_iteration\", None)))\n",
    "else:\n",
    "    p_zero_test = clf.predict_proba(X_test)[:, 1]\n",
    "    p_reg_test = np.expm1(reg.predict(X_test))\n",
    "\n",
    "# --- Combine Two-Stage Predictions ---\n",
    "p_final_test = (1 - p_zero_test) * p_reg_test\n",
    "p_final_test = np.clip(p_final_test, 0, None)\n",
    "p_final_test = np.where(p_final_test < 1.0, 0.0, p_final_test)\n",
    "\n",
    "# --- Build Submission ---\n",
    "if \"id\" in data[\"test\"].columns:\n",
    "    # ✅ Normal case: Kaggle test file includes `id`\n",
    "    sub = pd.DataFrame({\n",
    "        \"id\": data[\"test\"][\"id\"],\n",
    "        \"new_house_transaction_amount\": p_final_test\n",
    "    })\n",
    "else:\n",
    "    # ✅ Fallback: reconstruct from month & sector (if available)\n",
    "    if {\"month\", \"sector\"}.issubset(data[\"test\"].columns):\n",
    "        sub = pd.DataFrame({\n",
    "            \"id\": data[\"test\"][\"month\"].astype(str) + \"_sector \" + data[\"test\"][\"sector\"].astype(str),\n",
    "            \"new_house_transaction_amount\": p_final_test\n",
    "        })\n",
    "    else:\n",
    "        # ✅ Ultimate fallback if no id/month/sector available\n",
    "        sub = pd.DataFrame({\n",
    "            \"id\": [f\"sample_{i}\" for i in range(len(p_final_test))],\n",
    "            \"new_house_transaction_amount\": p_final_test\n",
    "        })\n",
    "\n",
    "save_csv(sub, \"submission.csv\")\n",
    "print(\"🏁 Submission file ready! Upload to Kaggle.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test alignment done: 179 features used.\n",
      "   → Added 179 missing columns.\n",
      "   → Dropped 2 extra columns.\n",
      "✅ Saved submission.csv | shape: (1152, 2) | path: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\submission.csv\n",
      "🏁 Submission file ready! Upload to Kaggle.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13320609,
     "sourceId": 111876,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.103273,
   "end_time": "2025-10-05T13:55:13.364462",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-05T13:54:52.261189",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
