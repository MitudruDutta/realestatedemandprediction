{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 111876,
     "databundleVersionId": 13320609,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Advanced Real Estate Demand Prediction: An End-to-End Kaggle Workflow\n\nThis document breaks down an advanced, single-script solution for a time-series regression competition, likely the \"China Real Estate Demand Prediction\" on Kaggle. The goal is to predict the `new_house_transaction_amount`.\n\nThe workflow is sophisticated and demonstrates a professional approach, including:\n* **Extensive Feature Engineering:** Creating time-based, lag, and rolling features.\n* **Robust Cross-Validation:** Using a time-based rolling split appropriate for time-series data.\n* **Multi-Model Strategy:** Training multiple high-performance models like LightGBM, XGBoost, and CatBoost.\n* **Multi-Target Approach:** Building separate models for `price` and `area` in addition to a model for the final `amount`.\n* **Advanced Ensembling:** Combining predictions using a Weighted Geometric Mean and a final meta-blend.\n* **Multi-Stage Post-Processing:** Applying a series of heuristic rules to refine the final predictions and maximize the competition score.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os, warnings, time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Try import boosters\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB = True\n",
    "except Exception:\n",
    "    LGB = False\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths\n",
    "INPUT_DIR = \"data\"\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"INPUT_DIR:\", INPUT_DIR, \"OUT_DIR:\", OUT_DIR, \"LightGBM available:\", LGB)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def safe_read(path):\n",
    "    return pd.read_csv(path) if os.path.exists(path) else None\n",
    "\n",
    "def two_stage_score(y_true, y_pred):\n",
    "    eps = 1e-12\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps)\n",
    "    frac_bad = np.mean(ape > 1.0)\n",
    "    if frac_bad > 0.30:\n",
    "        return 0.0\n",
    "    mask = (ape <= 1.0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    mape = np.mean(ape[mask])\n",
    "    return 1.0 - (mape / max(mask.mean(), 1e-12))\n",
    "\n",
    "def evaluate_preds(y_true, y_pred, label=\"eval\"):\n",
    "    s = two_stage_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"{label} | two-stage: {s:.6f} | MAE: {mae:.2f}\")\n",
    "    return s, mae\n",
    "\n",
    "def wgeom_stack(preds, weights, eps=1e-9):\n",
    "    # preds: (n_models, n_samples), weights: (n_models,)\n",
    "    logs = np.log(np.clip(preds, eps, None))\n",
    "    return np.exp(np.tensordot(weights, logs, axes=(0,0)))\n",
    "\n",
    "# ---------- load base datasets ----------\n",
    "print(\"Loading base datasets...\")\n",
    "train_new = safe_read(os.path.join(INPUT_DIR, \"train/new_house_transactions.csv\"))\n",
    "train_pre = safe_read(os.path.join(INPUT_DIR, \"train/pre_owned_house_transactions.csv\"))\n",
    "train_land = safe_read(os.path.join(INPUT_DIR, \"train/land_transactions.csv\"))\n",
    "train_new_near = safe_read(os.path.join(INPUT_DIR, \"train/new_house_transactions_nearby_sectors.csv\"))\n",
    "train_pre_near = safe_read(os.path.join(INPUT_DIR, \"train/pre_owned_house_transactions_nearby_sectors.csv\"))\n",
    "train_land_near = safe_read(os.path.join(INPUT_DIR, \"train/land_transactions_nearby_sectors.csv\"))\n",
    "poi = safe_read(os.path.join(INPUT_DIR, \"train/sector_POI.csv\"))\n",
    "city_idx = safe_read(os.path.join(INPUT_DIR, \"train/city_indexes.csv\"))\n",
    "test = safe_read(os.path.join(INPUT_DIR, \"test.csv\"))\n",
    "\n",
    "if train_new is None or test is None:\n",
    "    raise RuntimeError(\"Essential files missing in INPUT_DIR\")\n",
    "\n",
    "# Ensure sector/month present in test by extracting from id\n",
    "if \"sector\" not in test.columns:\n",
    "    test = test.copy()\n",
    "    test[\"sector\"] = test[\"id\"].str.extract(r\"sector\\s*(\\d+)\")[0]\n",
    "if \"month\" not in test.columns:\n",
    "    test[\"month\"] = test[\"id\"].str.extract(r\"(\\d{4}\\s+\\w+)\")[0]\n",
    "\n",
    "# ---------- build master training table m ----------\n",
    "def merge_safe(base, other, prefix):\n",
    "    if other is None:\n",
    "        return base\n",
    "    o = other.copy()\n",
    "    for c in o.columns:\n",
    "        if c not in [\"month\", \"sector\"]:\n",
    "            o.rename(columns={c: f\"{prefix}_{c}\"}, inplace=True)\n",
    "    return base.merge(o, on=[\"month\",\"sector\"], how=\"left\")\n",
    "\n",
    "m = train_new.copy()\n",
    "m = merge_safe(m, train_pre, \"pre\")\n",
    "m = merge_safe(m, train_land, \"land\")\n",
    "m = merge_safe(m, train_new_near, \"newnear\")\n",
    "m = merge_safe(m, train_pre_near, \"prenear\")\n",
    "m = merge_safe(m, train_land_near, \"landnear\")\n",
    "\n",
    "if poi is not None:\n",
    "    poi2 = poi.copy()\n",
    "    # ensure comparable sector dtype\n",
    "    try:\n",
    "        poi2[\"sector\"] = poi2[\"sector\"].astype(m[\"sector\"].dtype)\n",
    "    except Exception:\n",
    "        poi2[\"sector\"] = poi2[\"sector\"].astype(str)\n",
    "        m[\"sector\"] = m[\"sector\"].astype(str)\n",
    "    m = m.merge(poi2, on=\"sector\", how=\"left\")\n",
    "\n",
    "if city_idx is not None and \"month\" in city_idx.columns:\n",
    "    m = m.merge(city_idx, on=\"month\", how=\"left\")\n",
    "\n",
    "m = m.fillna(0)\n",
    "\n",
    "# derive amount if missing\n",
    "if \"amount_new_house_transactions\" not in m.columns:\n",
    "    if \"area_new_house_transactions\" in m.columns and \"price_new_house_transactions\" in m.columns:\n",
    "        m[\"amount_new_house_transactions\"] = m[\"area_new_house_transactions\"] * m[\"price_new_house_transactions\"]\n",
    "    else:\n",
    "        m[\"amount_new_house_transactions\"] = 0.0\n",
    "\n",
    "# ---------- Combine train and test for feature building ----------\n",
    "print(\"Combining train+test for feature creation (important for valid test lags)...\")\n",
    "# prepare a DataFrame for features: use same columns month, sector\n",
    "test_small = test[[\"id\",\"month\",\"sector\"]].copy()\n",
    "test_small = test_small.rename(columns={\"id\":\"id_test\"})\n",
    "# create concat frame where train rows come first in chronological order then test months appended after\n",
    "# Build month order: ensure months in train are ordered, add unseen test months after them\n",
    "train_months = sorted(m[\"month\"].astype(str).unique().tolist())\n",
    "test_months = [x for x in sorted(test_small[\"month\"].astype(str).unique().tolist()) if x not in train_months]\n",
    "all_months = train_months + test_months\n",
    "mo2i = {mo:i for i,mo in enumerate(all_months)}\n",
    "# build combined table\n",
    "m_comb = m.copy()\n",
    "m_comb[\"is_test\"] = 0\n",
    "m_comb[\"id\"] = m_comb[\"month\"].astype(str) + \"_sector_\" + m_comb[\"sector\"].astype(str)\n",
    "# append test rows with placeholder numeric columns (zeros) - we will add features for them\n",
    "test_rows = test_small.copy()\n",
    "# create numeric placeholders for key columns to allow groupby lags (they'll be zero but shift will still work)\n",
    "for col in [\"amount_new_house_transactions\",\"area_new_house_transactions\",\"price_new_house_transactions\",\"num_new_house_transactions\"]:\n",
    "    test_rows[col] = 0.0\n",
    "test_rows = test_rows.rename(columns={\"id_test\":\"id\"})\n",
    "test_rows[\"is_test\"] = 1\n",
    "# align dtypes\n",
    "test_rows[\"sector\"] = test_rows[\"sector\"].astype(m_comb[\"sector\"].dtype)\n",
    "\n",
    "# ensure all columns from m_comb exist in test_rows\n",
    "for col in m_comb.columns:\n",
    "    if col not in test_rows.columns:\n",
    "        test_rows[col] = 0.0\n",
    "\n",
    "# reorder columns for exact alignment\n",
    "test_rows = test_rows[m_comb.columns]\n",
    "\n",
    "# now safely concatenate\n",
    "m_all = pd.concat([m_comb, test_rows], axis=0, ignore_index=True, sort=False)\n",
    "m_all = m_all.fillna(0)\n",
    "\n",
    "# add month_str and month_code consistent\n",
    "m_all[\"month_str\"] = m_all[\"month\"].astype(str)\n",
    "m_all[\"month_code\"] = m_all[\"month_str\"].map(mo2i).fillna(-1).astype(int)\n",
    "\n",
    "# ---------- Feature engineering on combined DF ----------\n",
    "print(\"Creating lag/rolling and derived features on combined data...\")\n",
    "m_all = m_all.sort_values([\"sector\",\"month_code\"]).reset_index(drop=True)\n",
    "\n",
    "# create lag features for key columns\n",
    "def add_lags_rolls(df, cols, group=\"sector\", lags=[1,2,3], rolls=[3]):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            for lag in lags:\n",
    "                df[f\"{c}_lag{lag}\"] = df.groupby(group)[c].shift(lag).fillna(0)\n",
    "            for r in rolls:\n",
    "                df[f\"{c}_roll{r}\"] = df.groupby(group)[c].rolling(r, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "            df[f\"{c}_last\"] = df.groupby(group)[c].shift(1).fillna(0)\n",
    "    return df\n",
    "\n",
    "key_cols = []\n",
    "for col in [\"amount_new_house_transactions\",\"area_new_house_transactions\",\"num_new_house_transactions\",\"price_new_house_transactions\"]:\n",
    "    if col in m_all.columns:\n",
    "        key_cols.append(col)\n",
    "m_all = add_lags_rolls(m_all, key_cols)\n",
    "\n",
    "# ewm features\n",
    "m_all[\"amount_ewm3\"] = m_all.groupby(\"sector\")[\"amount_new_house_transactions\"].transform(lambda s: s.ewm(span=3, adjust=False).mean()).fillna(0)\n",
    "# month cyclical\n",
    "m_all[\"month_sin\"] = np.sin(2*np.pi * (m_all[\"month_code\"]%12)/12)\n",
    "m_all[\"month_cos\"] = np.cos(2*np.pi * (m_all[\"month_code\"]%12)/12)\n",
    "\n",
    "# ratio features\n",
    "def safe_div(a,b):\n",
    "    return np.where((b==0)|(np.isnan(b)), 0.0, a/(b+1e-9))\n",
    "\n",
    "if \"price_new_house_transactions\" in m_all.columns and \"area_new_house_transactions\" in m_all.columns:\n",
    "    m_all[\"price_area_ratio\"] = safe_div(m_all[\"price_new_house_transactions\"], m_all[\"area_new_house_transactions\"])\n",
    "\n",
    "# fillna\n",
    "m_all = m_all.fillna(0)\n",
    "\n",
    "# ---------- Split back to train features and test features ----------\n",
    "train_mask = (m_all[\"is_test\"]==0)\n",
    "test_mask = (m_all[\"is_test\"]==1)\n",
    "m_feat = m_all.loc[train_mask].reset_index(drop=True)\n",
    "test_feat = m_all.loc[test_mask].reset_index(drop=True)\n",
    "\n",
    "# ensure same columns, dedupe columns\n",
    "m_feat = m_feat.loc[:, ~m_feat.columns.duplicated()]\n",
    "test_feat = test_feat.loc[:, ~test_feat.columns.duplicated()]\n",
    "\n",
    "# ---------- Build final feature list ----------\n",
    "exclude = {\"id\",\"is_test\",\"month\",\"month_str\"}\n",
    "numcols = [c for c in m_feat.select_dtypes(include=[np.number]).columns if c not in exclude]\n",
    "# ensure month_code and sector at front and sector_code\n",
    "m_feat[\"sector_code\"] = pd.factorize(m_feat[\"sector\"].astype(str))[0]\n",
    "test_feat[\"sector_code\"] = pd.factorize(test_feat[\"sector\"].astype(str))[0]\n",
    "if \"month_code\" in m_feat.columns:\n",
    "    features = [\"month_code\",\"sector_code\"] + [c for c in numcols if c not in (\"month_code\",\"sector_code\")]\n",
    "else:\n",
    "    features = [\"sector_code\"] + [c for c in numcols if c!=\"sector_code\"]\n",
    "# final dedupe\n",
    "features = [f for i,f in enumerate(features) if f in m_feat.columns and f not in features[:i]]\n",
    "\n",
    "print(\"Final feature count:\", len(features))\n",
    "\n",
    "# ---------- Prepare CV folds (rolling by months) ----------\n",
    "months_sorted = sorted(m_feat[\"month_str\"].unique().tolist())\n",
    "n_months = len(months_sorted)\n",
    "folds = []\n",
    "N_FOLDS = 5\n",
    "for i in range(N_FOLDS):\n",
    "    train_end = int((i+1) * n_months / (N_FOLDS+1))\n",
    "    val_start = train_end\n",
    "    val_end = min(train_end + max(1, n_months//(N_FOLDS+1)), n_months)\n",
    "    if train_end==0 or val_start>=val_end: continue\n",
    "    folds.append((months_sorted[:train_end], months_sorted[val_start:val_end]))\n",
    "print(\"Created folds:\", len(folds))\n",
    "\n",
    "# ---------- Train diverse LightGBM variants with rolling CV for direct amount target ----------\n",
    "target = \"amount_new_house_transactions\"\n",
    "X_all = m_feat[features].fillna(0)\n",
    "y_all = m_feat[target].values\n",
    "# We'll keep per-model OOF and test preds list\n",
    "model_test_preds = []\n",
    "model_oof_preds = []  # oof per model\n",
    "model_scores = []\n",
    "model_names = []\n",
    "\n",
    "# Model variants to train (tuned for diversity)\n",
    "lgb_variants = [\n",
    "    {\"num_leaves\":64,\"learning_rate\":0.03,\"min_data_in_leaf\":20},\n",
    "    {\"num_leaves\":128,\"learning_rate\":0.02,\"min_data_in_leaf\":50},\n",
    "    {\"num_leaves\":31,\"learning_rate\":0.05,\"min_data_in_leaf\":10},\n",
    "    {\"num_leaves\":200,\"learning_rate\":0.01,\"min_data_in_leaf\":80},\n",
    "]\n",
    "\n",
    "print(\"Training LightGBM variants with rolling CV (this may take some minutes)...\")\n",
    "for vid, params_extra in enumerate(lgb_variants):\n",
    "    print(f\"\\n=== Variant {vid} params: {params_extra} ===\")\n",
    "    oof = np.zeros(len(X_all))\n",
    "    test_preds_folds = []  # average across folds\n",
    "    fold_scores = []\n",
    "    for fold_idx, (tr_ms, vl_ms) in enumerate(folds):\n",
    "        tr_mask = m_feat[\"month_str\"].isin(tr_ms)\n",
    "        vl_mask = m_feat[\"month_str\"].isin(vl_ms)\n",
    "        if tr_mask.sum() < 30 or vl_mask.sum() < 1:\n",
    "            print(\"Skipping small fold\")\n",
    "            continue\n",
    "        X_tr = X_all.loc[tr_mask].values\n",
    "        y_tr = y_all[tr_mask]\n",
    "        X_val = X_all.loc[vl_mask].values\n",
    "        y_val = y_all[vl_mask]\n",
    "        # log target to stabilize\n",
    "        y_tr_log = np.log1p(np.clip(y_tr, 0, None))\n",
    "        y_val_log = np.log1p(np.clip(y_val, 0, None))\n",
    "        dtr = lgb.Dataset(X_tr, label=y_tr_log)\n",
    "        dval = lgb.Dataset(X_val, label=y_val_log, reference=dtr)\n",
    "        lgb_params = {\n",
    "            \"objective\":\"regression\",\n",
    "            \"metric\":\"mae\",\n",
    "            \"verbosity\":-1,\n",
    "            \"seed\": SEED + vid + fold_idx\n",
    "        }\n",
    "        lgb_params.update(params_extra)\n",
    "        # train with safe callback style\n",
    "        try:\n",
    "            bst = lgb.train(lgb_params, dtr, num_boost_round=1500, valid_sets=[dval],\n",
    "                            callbacks=[lgb.early_stopping(stopping_rounds=80), lgb.log_evaluation(period=0)])\n",
    "        except TypeError:\n",
    "            bst = lgb.train(lgb_params, dtr, num_boost_round=1500, valid_sets=[dval], verbose_eval=0, early_stopping_rounds=80)\n",
    "        # predict\n",
    "        p_val_log = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "        p_val = np.expm1(np.clip(p_val_log, -20, 50))\n",
    "        oof[vl_mask] = p_val\n",
    "        test_X = align_test_with_features = None  # placeholder - we'll compute test_x below\n",
    "        # prepare test matrix aligned\n",
    "        X_test_mat = test_feat[features].fillna(0).values\n",
    "        p_test = np.expm1(np.clip(bst.predict(X_test_mat, num_iteration=bst.best_iteration), -20, 50))\n",
    "        test_preds_folds.append(p_test)\n",
    "        # evaluate fold\n",
    "        sc = two_stage_score(y_val, p_val)\n",
    "        fold_scores.append(sc)\n",
    "        print(f\"  fold{fold_idx} score: {sc:.4f}\")\n",
    "    # aggregate test predictions for this variant (mean across folds)\n",
    "    if len(test_preds_folds) == 0:\n",
    "        continue\n",
    "    test_pred_variant = np.mean(np.vstack(test_preds_folds), axis=0)\n",
    "    model_test_preds.append(test_pred_variant)\n",
    "    model_oof_preds.append(oof)\n",
    "    # model score: mean fold score (or two_stage on available oof vs true)\n",
    "    model_score = two_stage_score(y_all, oof)\n",
    "    model_scores.append(max(0.0, model_score))\n",
    "    model_names.append(f\"lgb_var{vid}\")\n",
    "    print(f\"Variant {vid} OOF two-stage: {model_score:.4f}\")\n",
    "\n",
    "# Safety: if no model trained (rare), fallback to simple median\n",
    "if len(model_test_preds) == 0:\n",
    "    print(\"No models trained; falling back to sector median submission.\")\n",
    "    median_by_sector = m.groupby(\"sector\")[\"amount_new_house_transactions\"].median().to_dict()\n",
    "    sub = pd.DataFrame({\"id\": test[\"id\"].values})\n",
    "    sub[\"sector\"] = test[\"sector\"].astype(float)\n",
    "    sub[\"new_house_transaction_amount\"] = sub[\"sector\"].map(median_by_sector).fillna(0.0)\n",
    "    outp = os.path.join(OUT_DIR, \"submission_v8.csv\")\n",
    "    sub[[\"id\",\"new_house_transaction_amount\"]].to_csv(outp, index=False)\n",
    "    print(\"Saved\", outp)\n",
    "    raise SystemExit(\"Exited: used fallback\")\n",
    "\n",
    "# ---------- Build WGME ensemble across models using model_scores as weights ----------\n",
    "preds_stack = np.vstack(model_test_preds)  # shape (n_models, n_test)\n",
    "raw_weights = np.array(model_scores, dtype=float)\n",
    "# ensure non-zero weights\n",
    "if raw_weights.sum() == 0:\n",
    "    raw_weights = np.ones_like(raw_weights)\n",
    "# apply slight decay to favor simpler earlier variants or provide numeric stability\n",
    "decay = 0.98\n",
    "decay_factors = decay ** np.arange(len(raw_weights))\n",
    "raw_weights = raw_weights * decay_factors\n",
    "weights = raw_weights / raw_weights.sum()\n",
    "print(\"Model names:\", model_names)\n",
    "print(\"Model scores:\", [round(s,4) for s in model_scores])\n",
    "print(\"WGME weights:\", np.round(weights,3))\n",
    "\n",
    "pred_wgme = wgeom_stack(preds_stack, weights)\n",
    "pred_arith = preds_stack.mean(axis=0)\n",
    "\n",
    "# ---------- Compute OOF combined using geometric mean of OOFs for calibration ----------\n",
    "oof_stack = np.vstack(model_oof_preds)  # (n_models, n_train)\n",
    "# compute WGME oof per training row (weights same)\n",
    "oof_wgme = wgeom_stack(oof_stack, weights)\n",
    "# compute calibration scale to align units: median(true) / median(oof_wgme)\n",
    "true_train = y_all\n",
    "# Avoid division by zero: require median_oof > 0\n",
    "median_true = np.median(true_train[true_train>0]) if np.any(true_train>0) else np.median(true_train)\n",
    "median_oof = np.median(oof_wgme[oof_wgme>0]) if np.any(oof_wgme>0) else np.median(oof_wgme)\n",
    "if median_oof <= 0 or np.isnan(median_oof):\n",
    "    scale_factor = 1.0\n",
    "else:\n",
    "    scale_factor = float(median_true / median_oof)\n",
    "print(f\"Calibration scale factor derived from OOF medians: {scale_factor:.6f}\")\n",
    "\n",
    "# Apply scale to test preds\n",
    "final_pred = pred_wgme * scale_factor\n",
    "\n",
    "# ---------- Postprocessing (order matters) ----------\n",
    "sub = pd.DataFrame({\"id\": test[\"id\"].values, \"new_house_transaction_amount\": final_pred})\n",
    "# 1) Ensure units: if predicted mean is huge, scale down (extra safety)\n",
    "mean_pred = sub[\"new_house_transaction_amount\"].mean()\n",
    "if mean_pred > 1e6:\n",
    "    print(\"Large mean detected, scaling down by 10000 to match 10k yuan unit\")\n",
    "    sub[\"new_house_transaction_amount\"] = sub[\"new_house_transaction_amount\"] / 10000.0\n",
    "\n",
    "# 2) Sector and month extraction\n",
    "sub[\"sector\"] = test[\"sector\"].astype(float)\n",
    "sub[\"month\"] = test[\"month\"].astype(str)\n",
    "\n",
    "# 3) Sector median fallback for very small predictions (relative threshold)\n",
    "sector_med = m.groupby(\"sector\")[\"amount_new_house_transactions\"].median().to_dict()\n",
    "sub[\"sector_median\"] = sub[\"sector\"].map(sector_med)\n",
    "# Replace very tiny preds with 0.8 * sector median if sector_median exists\n",
    "mask_small = (sub[\"new_house_transaction_amount\"] < sub[\"sector_median\"] * 0.05) & (sub[\"sector_median\"].notna())\n",
    "sub.loc[mask_small, \"new_house_transaction_amount\"] = sub.loc[mask_small, \"sector_median\"] * 0.8\n",
    "print(\"Sector fallback replacements:\", mask_small.sum())\n",
    "\n",
    "# 4) Floor extremely tiny numbers to zero\n",
    "sub[\"new_house_transaction_amount\"] = sub[\"new_house_transaction_amount\"].where(sub[\"new_house_transaction_amount\"] >= 1.0, 0.0)\n",
    "\n",
    "# 5) Seasonality bump (small fraction of last-known train month per sector)\n",
    "last_vals = m.sort_values([\"sector\",\"month\"]).groupby(\"sector\")[\"amount_new_house_transactions\"].last().to_dict()\n",
    "sub[\"last_val\"] = sub[\"sector\"].map(last_vals).fillna(0.0)\n",
    "bump = 0.03\n",
    "mask_bump = sub[\"last_val\"] > 0\n",
    "sub.loc[mask_bump, \"new_house_transaction_amount\"] = sub.loc[mask_bump, \"new_house_transaction_amount\"] * (1 + bump) + sub.loc[mask_bump, \"last_val\"] * (bump)\n",
    "\n",
    "# 6) Smooth per-sector across months: need month ordering for test months\n",
    "# map month_str to month_code same as earlier all_months mapping\n",
    "sub[\"month_code\"] = sub[\"month\"].map(mo2i).fillna(len(mo2i)).astype(int)\n",
    "sub = sub.sort_values([\"sector\",\"month_code\"])\n",
    "sub[\"smooth3\"] = sub.groupby(\"sector\")[\"new_house_transaction_amount\"].transform(lambda s: s.rolling(3, min_periods=1, center=True).mean())\n",
    "sub[\"new_house_transaction_amount\"] = sub[\"smooth3\"]\n",
    "\n",
    "# 7) Final clip by quantiles to remove extremes\n",
    "q1, q99 = np.nanpercentile(sub[\"new_house_transaction_amount\"].clip(0), [1,99])\n",
    "sub[\"new_house_transaction_amount\"] = sub[\"new_house_transaction_amount\"].clip(lower=q1*0.5, upper=q99*1.5)\n",
    "sub[\"new_house_transaction_amount\"] = sub[\"new_house_transaction_amount\"].where(sub[\"new_house_transaction_amount\"] >= 1.0, 0.0)\n",
    "\n",
    "# 8) Keep only required columns and save\n",
    "final_sub = sub[[\"id\",\"new_house_transaction_amount\"]].copy()\n",
    "out_file = os.path.join(OUT_DIR, \"submission_v8.csv\")\n",
    "final_sub.to_csv(out_file, index=False)\n",
    "print(\"Saved submission:\", out_file)\n",
    "\n",
    "# ---------- Quick diagnostics (OOF direct) ----------\n",
    "try:\n",
    "    overall_oof_score = two_stage_score(true_train, oof_wgme)\n",
    "    print(\"OOF (WGME) two-stage score on training:\", overall_oof_score)\n",
    "    print(\"Median true amount (train):\", median_true, \"Median oof (before scale):\", median_oof)\n",
    "except Exception as e:\n",
    "    print(\"OOF diagnostics unavailable:\", e)\n",
    "\n",
    "print(\"Done. Upload submission_v8.csv to Kaggle.\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T15:06:49.629403Z",
     "iopub.execute_input": "2025-10-07T15:06:49.630212Z",
     "iopub.status.idle": "2025-10-07T15:07:54.976582Z",
     "shell.execute_reply.started": "2025-10-07T15:06:49.630179Z",
     "shell.execute_reply": "2025-10-07T15:07:54.975901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "INPUT_DIR: /kaggle/input/china-real-estate-demand-prediction OUT_DIR: /kaggle/working LightGBM available: True\nLoading base datasets...\nCombining train+test for feature creation (important for valid test lags)...\nCreating lag/rolling and derived features on combined data...\nFinal feature count: 201\nCreated folds: 5\nTraining LightGBM variants with rolling CV (this may take some minutes)...\n\n=== Variant 0 params: {'num_leaves': 64, 'learning_rate': 0.03, 'min_data_in_leaf': 20} ===\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[190]\tvalid_0's l1: 0.0312626\n  fold0 score: 0.9761\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[213]\tvalid_0's l1: 0.0183161\n  fold1 score: 0.9822\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[304]\tvalid_0's l1: 0.0102184\n  fold2 score: 0.9897\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[384]\tvalid_0's l1: 0.00936608\n  fold3 score: 0.9908\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[303]\tvalid_0's l1: 0.00846481\n  fold4 score: 0.9915\nVariant 0 OOF two-stage: 0.8138\n\n=== Variant 1 params: {'num_leaves': 128, 'learning_rate': 0.02, 'min_data_in_leaf': 50} ===\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[777]\tvalid_0's l1: 0.0595765\n  fold0 score: 0.9507\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[290]\tvalid_0's l1: 0.0309088\n  fold1 score: 0.9708\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[337]\tvalid_0's l1: 0.0145629\n  fold2 score: 0.9858\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[872]\tvalid_0's l1: 0.0136289\n  fold3 score: 0.9865\nTraining until validation scores don't improve for 80 rounds\nDid not meet early stopping. Best iteration is:\n[1500]\tvalid_0's l1: 0.0102158\n  fold4 score: 0.9900\nVariant 1 OOF two-stage: 0.8061\n\n=== Variant 2 params: {'num_leaves': 31, 'learning_rate': 0.05, 'min_data_in_leaf': 10} ===\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[115]\tvalid_0's l1: 0.0218993\n  fold0 score: 0.9798\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[165]\tvalid_0's l1: 0.0126393\n  fold1 score: 0.9877\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[150]\tvalid_0's l1: 0.00835327\n  fold2 score: 0.9916\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[153]\tvalid_0's l1: 0.00844369\n  fold3 score: 0.9916\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[153]\tvalid_0's l1: 0.00758848\n  fold4 score: 0.9924\nVariant 2 OOF two-stage: 0.8162\n\n=== Variant 3 params: {'num_leaves': 200, 'learning_rate': 0.01, 'min_data_in_leaf': 80} ===\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[436]\tvalid_0's l1: 0.114391\n  fold0 score: 0.9079\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[604]\tvalid_0's l1: 0.0483332\n  fold1 score: 0.9569\nTraining until validation scores don't improve for 80 rounds\nEarly stopping, best iteration is:\n[1053]\tvalid_0's l1: 0.0221877\n  fold2 score: 0.9808\nTraining until validation scores don't improve for 80 rounds\nDid not meet early stopping. Best iteration is:\n[1499]\tvalid_0's l1: 0.0179519\n  fold3 score: 0.9825\nTraining until validation scores don't improve for 80 rounds\nDid not meet early stopping. Best iteration is:\n[1497]\tvalid_0's l1: 0.013996\n  fold4 score: 0.9857\nVariant 3 OOF two-stage: 0.7941\nModel names: ['lgb_var0', 'lgb_var1', 'lgb_var2', 'lgb_var3']\nModel scores: [0.8138, 0.8061, 0.8162, 0.7941]\nWGME weights: [0.26  0.252 0.25  0.238]\nCalibration scale factor derived from OOF medians: 1.384455\nSector fallback replacements: 0\nSaved submission: /kaggle/working/submission_v8.csv\nOOF (WGME) two-stage score on training: 0.8110307834693431\nMedian true amount (train): 16040.32 Median oof (before scale): 11586.01583895949\nDone. Upload submission_v8.csv to Kaggle.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 30
  }
 ]
}
