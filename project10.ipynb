{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T07:46:20.931353Z",
     "start_time": "2025-10-12T07:27:00.780206Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "âš¡ FAST SUPREME MODEL - Optimized for Speed\n",
      "======================================================================\n",
      "\n",
      "ðŸ“‚ Loading data...\n",
      "ðŸ”§ Building optimized features...\n",
      "  â†’ Adding strategic lags...\n",
      "  â†’ Adding key rolling features...\n",
      "  â†’ Adding EWMA features...\n",
      "  â†’ Adding momentum features...\n",
      "  â†’ Adding statistical features...\n",
      "âœ“ Total features: 1519\n",
      "\n",
      "ðŸŽ¯ Preparing training...\n",
      "âœ“ Training: 3,936, Validation: 576\n",
      "\n",
      "âš¡ Training FAST model...\n",
      "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 322ms\tremaining: 1h 9m 41s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 335\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# FIXED: Removed subsample and colsample_bylevel (incompatible with Bayesian bootstrap)\u001b[39;00m\n\u001b[32m    314\u001b[39m cb = CatBoostRegressor(\n\u001b[32m    315\u001b[39m     iterations=\u001b[32m13000\u001b[39m,\n\u001b[32m    316\u001b[39m     learning_rate=\u001b[32m0.011\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    332\u001b[39m     thread_count=-\u001b[32m1\u001b[39m,\n\u001b[32m    333\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m \u001b[43mcb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainPool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtestPool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m testPool:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ“ Best iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcb.get_best_iteration()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mitudru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:5873\u001b[39m, in \u001b[36mCatBoostRegressor.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5872\u001b[39m     CatBoostRegressor._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5874\u001b[39m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5875\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5876\u001b[39m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mitudru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mitudru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conservatively Improved Real Estate Demand Prediction\n",
    "Based on successful baseline (0.58) with careful enhancements\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "pth = \"data\"\n",
    "\n",
    "def add_prefix(df, prefix, exclude=(\"sector\", \"month\")):\n",
    "    return df.rename(lambda c: c if c in exclude else f\"{prefix}{c}\")\n",
    "\n",
    "# Load all datasets\n",
    "ci = (\n",
    "    pl.read_csv(f\"{pth}/train/city_indexes.csv\")\n",
    "      .head(6)\n",
    "      .fill_null(-1)\n",
    "      .drop(\"total_fixed_asset_investment_10k\")\n",
    "      .pipe(add_prefix, prefix=\"ci_\")\n",
    ")\n",
    "\n",
    "sp = (\n",
    "    pl.read_csv(f\"{pth}/train/sector_POI.csv\")\n",
    "      .fill_null(-1)\n",
    "      .pipe(add_prefix, prefix=\"sp_\")\n",
    ")\n",
    "\n",
    "train_lt = (\n",
    "    pl.read_csv(f\"{pth}/train/land_transactions.csv\", infer_schema_length=10000)\n",
    "      .pipe(add_prefix, prefix=\"lt_\")\n",
    ")\n",
    "\n",
    "train_ltns = (\n",
    "    pl.read_csv(f\"{pth}/train/land_transactions_nearby_sectors.csv\")\n",
    "      .pipe(add_prefix, prefix=\"ltns_\")\n",
    ")\n",
    "\n",
    "train_pht = (\n",
    "    pl.read_csv(f\"{pth}/train/pre_owned_house_transactions.csv\")\n",
    "      .pipe(add_prefix, prefix=\"pht_\")\n",
    ")\n",
    "\n",
    "train_phtns = (\n",
    "    pl.read_csv(f\"{pth}/train/pre_owned_house_transactions_nearby_sectors.csv\")\n",
    "      .pipe(add_prefix, prefix=\"phtns_\")\n",
    ")\n",
    "\n",
    "train_nht = (\n",
    "    pl.read_csv(f\"{pth}/train/new_house_transactions.csv\")\n",
    "      .pipe(add_prefix, prefix=\"nht_\")\n",
    ")\n",
    "\n",
    "train_nhtns = (\n",
    "    pl.read_csv(f\"{pth}/train/new_house_transactions_nearby_sectors.csv\")\n",
    "      .pipe(add_prefix, prefix=\"nhtns_\")\n",
    ")\n",
    "\n",
    "test = (\n",
    "    pl.read_csv(f\"{pth}/test.csv\")\n",
    "      .with_columns(id_split=pl.col(\"id\").str.split(\"_\"))\n",
    "      .with_columns(\n",
    "          month=pl.col(\"id_split\").list.get(0),\n",
    "          sector=pl.col(\"id_split\").list.get(1),\n",
    "      )\n",
    "      .drop(\"id_split\")\n",
    ")\n",
    "\n",
    "month_codes = {m: i for i, m in enumerate(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], 1)}\n",
    "\n",
    "print(\"Building features...\")\n",
    "\n",
    "# Create base dataset (same as original)\n",
    "data = (\n",
    "    pl.DataFrame(train_nht[\"month\"].unique())\n",
    "    .join(\n",
    "        pl.DataFrame(train_nht[\"sector\"].unique().to_list() + [\"sector 95\"])\n",
    "        .rename({\"column_0\": \"sector\"}),\n",
    "        how=\"cross\",\n",
    "    )\n",
    "    .with_columns(\n",
    "        sector_id=pl.col(\"sector\").str.split(\" \").list.get(1).cast(pl.Int8),\n",
    "        year=pl.col(\"month\").str.split(\"-\").list.get(0).cast(pl.Int16),\n",
    "        month_num=pl.col(\"month\").str.split(\"-\").list.get(1)\n",
    "            .replace(month_codes)\n",
    "            .cast(pl.Int8),\n",
    "    )\n",
    "    .with_columns(\n",
    "        time=((pl.col(\"year\") - 2019) * 12 + pl.col(\"month_num\") - 1).cast(pl.Int8)\n",
    "    )\n",
    "    .sort(\"sector_id\", \"time\")\n",
    "    .join(train_nht, on=[\"sector\", \"month\"], how=\"left\")\n",
    "    .fill_null(0)\n",
    "    .join(train_nhtns, on=[\"sector\", \"month\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .join(train_pht, on=[\"sector\", \"month\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .join(train_phtns, on=[\"sector\", \"month\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .join(ci.rename({\"ci_city_indicator_data_year\": \"year\"}), on=[\"year\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .join(sp, on=[\"sector\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .join(train_lt, on=[\"sector\", \"month\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .join(train_ltns, on=[\"sector\", \"month\"], how=\"left\")\n",
    "    .fill_null(-1)\n",
    "    .with_columns(cs.float().cast(pl.Float32))\n",
    ")\n",
    "\n",
    "# Optimize data types (same as original)\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == pl.Int64:\n",
    "        c_min, c_max = data[col].min(), data[col].max()\n",
    "        if c_min == 0 and c_max == 0:\n",
    "            data = data.drop(col)\n",
    "            continue\n",
    "        if np.iinfo(np.int8).min < c_min < np.iinfo(np.int8).max and c_max < np.iinfo(np.int8).max:\n",
    "            data = data.with_columns(pl.col(col).cast(pl.Int8))\n",
    "        elif np.iinfo(np.int16).min < c_min < np.iinfo(np.int16).max and c_max < np.iinfo(np.int16).max:\n",
    "            data = data.with_columns(pl.col(col).cast(pl.Int16))\n",
    "        elif np.iinfo(np.int32).min < c_min < np.iinfo(np.int32).max and c_max < np.iinfo(np.int32).max:\n",
    "            data = data.with_columns(pl.col(col).cast(pl.Int32))\n",
    "\n",
    "data = data.drop(\"month\", \"sector\", \"year\")\n",
    "\n",
    "# Add lag features (same as original)\n",
    "print(\"Adding lag features...\")\n",
    "data2 = data.sort(\"time\", \"sector_id\")\n",
    "\n",
    "for m in [1, 2, 12]:\n",
    "    data2 = data2.join(\n",
    "        data.drop(\"month_num\").with_columns(pl.col(\"time\") + m),\n",
    "        on=[\"sector_id\", \"time\"],\n",
    "        how=\"left\",\n",
    "        suffix=f\"_{m}\"\n",
    "    )\n",
    "\n",
    "data2 = data2.sort(\"time\", \"sector_id\")\n",
    "\n",
    "# Add ONLY proven helpful features\n",
    "print(\"Adding selected advanced features...\")\n",
    "\n",
    "# Add rolling means (these are generally helpful)\n",
    "for window in [3, 6, 12]:\n",
    "    data2 = data2.with_columns([\n",
    "        pl.col(\"nht_amount_new_house_transactions\")\n",
    "          .rolling_mean(window).over(\"sector_id\")\n",
    "          .alias(f\"nht_rolling_mean_{window}\"),\n",
    "        # Add rolling std for volatility measure\n",
    "        pl.col(\"nht_amount_new_house_transactions\")\n",
    "          .rolling_std(window).over(\"sector_id\")\n",
    "          .alias(f\"nht_rolling_std_{window}\"),\n",
    "    ])\n",
    "\n",
    "# Add exponentially weighted moving averages with multiple alphas\n",
    "data2 = data2.with_columns([\n",
    "    pl.col(\"nht_amount_new_house_transactions\")\n",
    "      .ewm_mean(alpha=0.3).over(\"sector_id\")\n",
    "      .alias(\"nht_ewm_mean_03\"),\n",
    "    pl.col(\"nht_amount_new_house_transactions\")\n",
    "      .ewm_mean(alpha=0.5).over(\"sector_id\")\n",
    "      .alias(\"nht_ewm_mean_05\"),\n",
    "    pl.col(\"nht_amount_new_house_transactions\")\n",
    "      .ewm_mean(alpha=0.7).over(\"sector_id\")\n",
    "      .alias(\"nht_ewm_mean_07\"),\n",
    "])\n",
    "\n",
    "# Add momentum features (rate of change)\n",
    "data2 = data2.with_columns([\n",
    "    # 1-month momentum\n",
    "    (pl.col(\"nht_amount_new_house_transactions\") - \n",
    "     pl.col(\"nht_amount_new_house_transactions\").shift(1).over(\"sector_id\"))\n",
    "    .alias(\"nht_momentum_1\"),\n",
    "    # 3-month momentum\n",
    "    (pl.col(\"nht_amount_new_house_transactions\") - \n",
    "     pl.col(\"nht_amount_new_house_transactions\").shift(3).over(\"sector_id\"))\n",
    "    .alias(\"nht_momentum_3\"),\n",
    "    # Year-over-year growth\n",
    "    (pl.col(\"nht_amount_new_house_transactions\") / \n",
    "     (pl.col(\"nht_amount_new_house_transactions\").shift(12).over(\"sector_id\") + 1))\n",
    "    .alias(\"nht_yoy_ratio\"),\n",
    "])\n",
    "\n",
    "# Add supply/demand indicators\n",
    "data2 = data2.with_columns([\n",
    "    # Inventory ratio\n",
    "    (pl.col(\"nht_num_new_house_available_for_sale\") / \n",
    "     (pl.col(\"nht_num_new_house_transactions\") + 1))\n",
    "    .alias(\"inventory_ratio\"),\n",
    "    # Price per unit trend\n",
    "    (pl.col(\"nht_area_per_unit_new_house_transactions\") / \n",
    "     (pl.col(\"nht_area_per_unit_new_house_transactions\").shift(1).over(\"sector_id\") + 1))\n",
    "    .alias(\"price_per_unit_trend\"),\n",
    "])\n",
    "\n",
    "# Create label and seasonality features (exactly like original)\n",
    "lag = -1\n",
    "data3 = data2.with_columns(\n",
    "    pl.col(\"nht_amount_new_house_transactions\")\n",
    "      .shift(lag)\n",
    "      .over(\"sector_id\")\n",
    "      .alias(\"label\"),\n",
    "\n",
    "    cs=((pl.col(\"month_num\") - 1) / 6 * np.pi).cos(),\n",
    "    sn=((pl.col(\"month_num\") - 1) / 6 * np.pi).sin(),\n",
    "    cs6=((pl.col(\"month_num\") - 1) / 3 * np.pi).cos(),\n",
    "    sn6=((pl.col(\"month_num\") - 1) / 3 * np.pi).sin(),\n",
    "    cs3=((pl.col(\"month_num\") - 1) / 1.5 * np.pi).cos(),\n",
    "    sn3=((pl.col(\"month_num\") - 1) / 1.5 * np.pi).sin(),\n",
    ")\n",
    "\n",
    "data3 = data3.drop(\"sector_id\")\n",
    "\n",
    "print(f\"Feature engineering complete! Shape: {data3.shape}\")\n",
    "\n",
    "# Custom metrics (same as original)\n",
    "def custom_score(y_true, y_pred, eps=1e-12):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    \n",
    "    if y_true.size == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    ape = np.abs((y_true - np.maximum(y_pred, 0)) / np.maximum(y_true, eps))\n",
    "    bad_rate = np.mean(ape > 1.0)\n",
    "    \n",
    "    if bad_rate > 0.30:\n",
    "        return 0.0\n",
    "    \n",
    "    mask = ape <= 1.0\n",
    "    good_ape = ape[mask]\n",
    "    \n",
    "    if good_ape.size == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mape = np.mean(good_ape)\n",
    "    fraction = good_ape.size / y_true.size\n",
    "    scaled_mape = mape / (fraction + eps)\n",
    "    score = max(0.0, 1.0 - scaled_mape)\n",
    "    \n",
    "    return score\n",
    "\n",
    "class CustomMetric:\n",
    "    def is_max_optimal(self):\n",
    "        return True\n",
    "    \n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        approx = approxes[0]\n",
    "        score = custom_score(target, approx)\n",
    "        return score, 1\n",
    "    \n",
    "    def get_final_error(self, error, weight):\n",
    "        return error\n",
    "\n",
    "class CustomObjective:\n",
    "    def calc_ders_range(self, approxes, targets, weights):\n",
    "        result = []\n",
    "        for i in range(len(targets)):\n",
    "            diff = targets[i] - approxes[i]\n",
    "            der1 = np.sign(diff) if (2*targets[i] - approxes[i]) < 0 else np.sign(diff)*5\n",
    "            der2 = 0\n",
    "            result.append((der1, der2))\n",
    "        return result\n",
    "\n",
    "# Training setup (same as original)\n",
    "print(\"\\nPreparing training data...\")\n",
    "cat_features = [\"month_num\"]\n",
    "\n",
    "border = 66 + lag - 12 * 0 - 1\n",
    "border1 = 6 * 3\n",
    "\n",
    "trainPool = Pool(\n",
    "    data=data3\n",
    "        .filter(pl.col(\"time\") <= border)\n",
    "        .filter(pl.col(\"time\") > border1)\n",
    "        .drop([\"label\"])\n",
    "        .to_pandas()\n",
    "        .fillna(-2),\n",
    "    \n",
    "    label=data3\n",
    "        .filter(pl.col(\"time\") <= border)\n",
    "        .filter(pl.col(\"time\") > border1)[\"label\"]\n",
    "        .to_pandas(),\n",
    "    \n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "testPool = Pool(\n",
    "    data=data3\n",
    "        .filter(pl.col(\"time\") > border)\n",
    "        .filter(pl.col(\"time\") <= 66 + lag)\n",
    "        .drop([\"label\"])\n",
    "        .to_pandas()\n",
    "        .fillna(-2),\n",
    "    \n",
    "    label=data3\n",
    "        .filter(pl.col(\"time\") > border)\n",
    "        .filter(pl.col(\"time\") <= 66 + lag)[\"label\"]\n",
    "        .to_pandas(),\n",
    "    \n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {trainPool.num_row()}\")\n",
    "print(f\"Validation samples: {testPool.num_row()}\")\n",
    "\n",
    "print(\"\\nTraining CatBoost model...\")\n",
    "\n",
    "# Slightly improved hyperparameters but conservative\n",
    "cb = CatBoostRegressor(\n",
    "    iterations=23000,  # Slightly more than original\n",
    "    learning_rate=0.011,  # Slightly lower for better convergence\n",
    "    depth=7,  # Slightly deeper\n",
    "    l2_leaf_reg=0.5,  # Slightly more regularization\n",
    "    random_strength=0.3,\n",
    "    one_hot_max_size=256,\n",
    "    custom_metric=[\"RMSE\", \"MAPE\", \"SMAPE\", \"MAE\"],\n",
    "    loss_function=CustomObjective(),\n",
    "    eval_metric=CustomMetric(),\n",
    "    random_seed=42,\n",
    "    verbose=1000,\n",
    ")\n",
    "\n",
    "cb.fit(trainPool, eval_set=testPool)\n",
    "\n",
    "print(f\"\\nBest iteration: {cb.get_best_iteration()}\")\n",
    "print(f\"Best score: {cb.get_best_score()['validation']['CustomMetric']:.6f}\")\n",
    "\n",
    "# Prediction\n",
    "print(\"\\nGenerating predictions...\")\n",
    "testPool2 = Pool(\n",
    "    data=data3\n",
    "        .filter(pl.col(\"time\") == 66)\n",
    "        .drop([\"label\"])\n",
    "        .to_pandas()\n",
    "        .fillna(-2),\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "month = np.maximum(cb.predict(testPool2), 0)\n",
    "\n",
    "# Ensemble prediction functions (from original)\n",
    "def ewgm_per_sector(a_tr, sector, n_lags, alpha):\n",
    "    weights = np.array([alpha**(n_lags - 1 - i) for i in range(n_lags)], dtype=float)\n",
    "    weights = weights / weights.sum()\n",
    "    recent_vals = a_tr.tail(n_lags)[sector].values\n",
    "    if (len(recent_vals) != n_lags) or (recent_vals <= 0).all():\n",
    "        return 0.0\n",
    "    mask = recent_vals > 0\n",
    "    pos_vals = recent_vals[mask]\n",
    "    pos_w = weights[mask]\n",
    "    if pos_vals.size == 0:\n",
    "        return 0.0\n",
    "    pos_w = pos_w / pos_w.sum()\n",
    "    log_vals = np.log(pos_vals + 1e-12)\n",
    "    wlm = np.sum(pos_w * log_vals) / pos_w.sum()\n",
    "    return float(np.exp(wlm))\n",
    "\n",
    "# Build submission with original ensemble approach\n",
    "print(\"\\nBuilding submission...\")\n",
    "\n",
    "def build_month_codes():\n",
    "    return {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "            'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}\n",
    "\n",
    "def add_time_and_sector_fields(df, month_codes):\n",
    "    if 'sector' in df.columns:\n",
    "        df['sector_id'] = df.sector.str.slice(7, None).astype(int)\n",
    "    if 'month' not in df.columns:\n",
    "        df['month'] = df['month_text'].str.slice(5, None).map(month_codes)\n",
    "        df['year'] = df['month_text'].str.slice(0, 4).astype(int)\n",
    "        df['time'] = (df['year'] - 2019) * 12 + df['month'] - 1\n",
    "    else:\n",
    "        df['year'] = df.month.str.slice(0, 4).astype(int)\n",
    "        df['month'] = df.month.str.slice(5, None).map(month_codes)\n",
    "        df['time'] = (df['year'] - 2019) * 12 + df['month'] - 1\n",
    "    return df\n",
    "\n",
    "def build_amount_matrix(train_nht, month_codes):\n",
    "    train_nht = add_time_and_sector_fields(train_nht.copy(), month_codes)\n",
    "    # Handle both prefixed and non-prefixed column names\n",
    "    amount_col = 'nht_amount_new_house_transactions' if 'nht_amount_new_house_transactions' in train_nht.columns else 'amount_new_house_transactions'\n",
    "    pivot = train_nht.set_index(['time', 'sector_id'])[amount_col].unstack()\n",
    "    pivot = pivot.fillna(0)\n",
    "    all_sectors = np.arange(1, 97)\n",
    "    for s in all_sectors:\n",
    "        if s not in pivot.columns:\n",
    "            pivot[s] = 0\n",
    "    pivot = pivot[all_sectors]\n",
    "    return pivot\n",
    "\n",
    "def compute_december_multipliers(a_tr, eps=1e-9, min_dec_obs=1, clip_low=0.85, clip_high=1.4):\n",
    "    is_december = (a_tr.index.values % 12) == 11\n",
    "    dec_means = a_tr[is_december].mean(axis=0)\n",
    "    nondec_means = a_tr[~is_december].mean(axis=0)\n",
    "    dec_counts = a_tr[is_december].notna().sum(axis=0)\n",
    "    raw_mult = dec_means / (nondec_means + eps)\n",
    "    overall_mult = float(dec_means.mean() / (nondec_means.mean() + eps))\n",
    "    raw_mult = raw_mult.where(dec_counts >= min_dec_obs, overall_mult)\n",
    "    raw_mult = raw_mult.replace([np.inf, -np.inf], 1.0).fillna(1.0)\n",
    "    clipped_mult = raw_mult.clip(lower=clip_low, upper=clip_high)\n",
    "    return clipped_mult.to_dict()\n",
    "\n",
    "def apply_december_bump(a_pred, sector_to_mult):\n",
    "    dec_rows = [t for t in a_pred.index.values if (t % 12) == 11]\n",
    "    if len(dec_rows) == 0:\n",
    "        return a_pred\n",
    "    for sector in a_pred.columns:\n",
    "        m = sector_to_mult.get(sector, 1.0)\n",
    "        a_pred.loc[dec_rows, sector] = a_pred.loc[dec_rows, sector] * m\n",
    "    return a_pred\n",
    "\n",
    "def predict_horizon(a_tr, alpha, n_lags, t2, allow_zeros, catboost_month_preds):\n",
    "    idx = np.arange(67, 79)\n",
    "    cols = a_tr.columns\n",
    "    a_pred = pd.DataFrame(index=idx, columns=cols, dtype=float)\n",
    "    for sector in cols:\n",
    "        if (a_tr.tail(t2)[sector] == 0).mean() > allow_zeros / t2 + 1e-8 or (a_tr[sector].sum() == 0):\n",
    "            a_pred[sector] = 0.0\n",
    "            continue\n",
    "        base_last_value = a_tr[sector].iloc[-1]\n",
    "        base_ewgm = ewgm_per_sector(a_tr=a_tr, sector=sector, n_lags=n_lags, alpha=alpha)\n",
    "        \n",
    "        # Improved ensemble weights\n",
    "        a_pred[sector] = 0.32*base_last_value + 0.32*base_ewgm + 0.36*catboost_month_preds[sector-1]\n",
    "        \n",
    "    a_pred.index.rename('time', inplace=True)\n",
    "    return a_pred\n",
    "\n",
    "def build_submission_df(a_pred, test_raw, month_codes):\n",
    "    test = test_raw.copy()\n",
    "    test['month_text'] = test['id'].str.split('_').str[0]\n",
    "    test['sector'] = test['id'].str.split('_').str[1]\n",
    "    test = add_time_and_sector_fields(test, month_codes)\n",
    "    lookup = a_pred.stack().rename('pred').reset_index().rename(columns={'level_1': 'sector_id'})\n",
    "    merged = test.merge(lookup, how='left', on=['time', 'sector_id'])\n",
    "    merged['pred'] = merged['pred'].fillna(0.0)\n",
    "    out = merged[['id', 'pred']].rename(columns={'pred': 'new_house_transaction_amount'})\n",
    "    return out\n",
    "\n",
    "# Generate final submission\n",
    "month_codes = build_month_codes()\n",
    "train_nht_pd = train_nht.to_pandas()\n",
    "test_pd = test.to_pandas()\n",
    "\n",
    "a_tr = build_amount_matrix(train_nht_pd, month_codes)\n",
    "a_pred = predict_horizon(a_tr=a_tr, alpha=0.5, n_lags=12, t2=10, allow_zeros=2, catboost_month_preds=month)\n",
    "sector_to_mult = compute_december_multipliers(a_tr=a_tr, eps=1e-9, min_dec_obs=1, clip_low=0.85, clip_high=1.4)\n",
    "a_pred = apply_december_bump(a_pred=a_pred, sector_to_mult=sector_to_mult)\n",
    "submission = build_submission_df(a_pred=a_pred, test_raw=test_pd, month_codes=month_codes)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Total predictions: {len(submission)}\")\n",
    "print(f\"Non-zero predictions: {(submission['new_house_transaction_amount'] > 0).sum()}\")\n",
    "print(f\"Mean prediction: {submission['new_house_transaction_amount'].mean():.2f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSample predictions:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a675cdee9ddd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
