{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 111876,
     "databundleVersionId": 13320609,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 1. Imports & Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport os, re, warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Ridge\nfrom tqdm.notebook import tqdm\n\nLGB_AVAILABLE, XGB_AVAILABLE = True, True\nSEED = 42\nnp.random.seed(SEED)\n\n# Helper save/load\ndef save_df(df, path):\n    df.to_csv(path, index=False)\n    print(f\"üíæ Saved: {path} ({df.shape})\")\n\ndef evaluate_preds(y_true, y_pred, name=\"\"):\n    ape = np.abs((y_pred - y_true) / (y_true + 1e-6))\n    frac_bad = np.mean(ape > 1)\n    if frac_bad > 0.3:\n        score = 0.0\n    else:\n        mape = np.mean(ape[ape <= 1])\n        score = 1 - mape / max(1 - frac_bad, 1e-6)\n    mae = mean_absolute_error(y_true, y_pred)\n    print(f\"{name} | two-stage: {score:.6f} | MAE: {mae:.4f} | frac_bad: {frac_bad:.4f}\")\n    return {\"score\": score, \"mae\": mae, \"frac_bad\": frac_bad}\n",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:04:39.004905Z",
     "iopub.execute_input": "2025-10-05T16:04:39.005448Z",
     "iopub.status.idle": "2025-10-05T16:04:44.441059Z",
     "shell.execute_reply.started": "2025-10-05T16:04:39.005423Z",
     "shell.execute_reply": "2025-10-05T16:04:44.440305Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Load Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Define local data path\n",
    "BASE_DIR = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Load all major datasets\n",
    "data = {\n",
    "    \"new_house\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"new_house_transactions.csv\")),\n",
    "    \"new_house_near\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"new_house_transactions_nearby_sectors.csv\")),\n",
    "    \"pre_owned\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"pre_owned_house_transactions.csv\")),\n",
    "    \"land\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"land_transactions.csv\")),\n",
    "    \"land_near\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"land_transactions_nearby_sectors.csv\")),\n",
    "    \"poi\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"sector_POI.csv\")),\n",
    "    \"city_index\": pd.read_csv(os.path.join(DATA_PATH, \"train\", \"city_indexes.csv\")),\n",
    "    \"test\": pd.read_csv(os.path.join(DATA_PATH, \"test.csv\")),\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Loaded datasets:\")\n",
    "for k, v in data.items():\n",
    "    print(f\"{k:20s} {v.shape}\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:04:49.842135Z",
     "iopub.execute_input": "2025-10-05T16:04:49.842771Z",
     "iopub.status.idle": "2025-10-05T16:04:49.948493Z",
     "shell.execute_reply.started": "2025-10-05T16:04:49.842745Z",
     "shell.execute_reply": "2025-10-05T16:04:49.947811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Loaded datasets:\nnew_house            (5433, 11)\nnew_house_near       (5360, 11)\npre_owned            (5360, 6)\nland                 (5896, 6)\nland_near            (5025, 6)\npoi                  (86, 142)\ncity_index           (7, 74)\ntest                 (1152, 2)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Clean & Merge Core Datasets",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Clean base dataframes\nfor k in [\"new_house\", \"pre_owned\", \"land\"]:\n    df = data[k]\n    df = df.drop_duplicates().fillna(0)\n    data[k] = df\n\n# Merge datasets on ['month','sector']\nm = data[\"new_house\"].merge(data[\"pre_owned\"], on=[\"month\",\"sector\"], suffixes=(\"\", \"_pre\"), how=\"left\")\nm = m.merge(data[\"land\"], on=[\"month\",\"sector\"], suffixes=(\"\", \"_land\"), how=\"left\")\nm = m.merge(data[\"poi\"], on=[\"sector\"], how=\"left\")\n\n# Target transformation\nm[\"y_log1p\"] = np.log1p(m[\"amount_new_house_transactions\"])\nm = m.fillna(0)\nprint(\"‚úÖ Merged master table:\", m.shape)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:04:52.652574Z",
     "iopub.execute_input": "2025-10-05T16:04:52.652839Z",
     "iopub.status.idle": "2025-10-05T16:04:52.732649Z",
     "shell.execute_reply.started": "2025-10-05T16:04:52.652819Z",
     "shell.execute_reply": "2025-10-05T16:04:52.731926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Merged master table: (5433, 161)\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Lag and Rolling Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create temporal lags & rolling windows\ndef create_lag_features(df, group_col=\"sector\", time_col=\"month\", cols=None, lags=[1,2], rolls=[3]):\n    out = df.copy()\n    for col in cols:\n        for lag in lags:\n            out[f\"{col}_lag{lag}\"] = out.groupby(group_col)[col].shift(lag)\n        for r in rolls:\n            out[f\"{col}_roll{r}\"] = out.groupby(group_col)[col].transform(lambda x: x.rolling(r,1).mean())\n    return out\n\nlag_cols = [\"amount_new_house_transactions\",\"area_new_house_transactions\",\"num_new_house_transactions\"]\nm = create_lag_features(m, cols=lag_cols)\nm = m.fillna(0)\nprint(\"‚úÖ Added lag & rolling features:\", [f for f in m.columns if \"lag\" in f or \"roll\" in f])\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:04:54.870823Z",
     "iopub.execute_input": "2025-10-05T16:04:54.871083Z",
     "iopub.status.idle": "2025-10-05T16:04:54.948668Z",
     "shell.execute_reply.started": "2025-10-05T16:04:54.871063Z",
     "shell.execute_reply": "2025-10-05T16:04:54.947900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Added lag & rolling features: ['amount_new_house_transactions_lag1', 'amount_new_house_transactions_lag2', 'amount_new_house_transactions_roll3', 'area_new_house_transactions_lag1', 'area_new_house_transactions_lag2', 'area_new_house_transactions_roll3', 'num_new_house_transactions_lag1', 'num_new_house_transactions_lag2', 'num_new_house_transactions_roll3']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Add Ratio Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# === Cell: Add key ratio/interaction features (auto-detect safe columns) ===\n\ndef safe_div(a, b):\n    \"\"\"Safe division that handles missing or zero denominators.\"\"\"\n    return np.where(b != 0, a / b, 0)\n\ndef get_col(df, possible_names):\n    \"\"\"Return first column name that exists from a list.\"\"\"\n    for name in possible_names:\n        if name in df.columns:\n            return name\n    return None\n\n# Auto-detect correct land column names\nland_amount_col = get_col(m, [\"land_transaction_amount\", \"land_transaction_amount_land\"])\nconstruction_col = get_col(m, [\"construction_area\", \"construction_area_land\"])\n\n# Create ratio features robustly\nm[\"price_area_ratio\"] = safe_div(m.get(\"price_new_house_transactions\", 0), m.get(\"area_new_house_transactions\", 0) + 1)\n\nif land_amount_col and construction_col:\n    m[\"land_value_density\"] = safe_div(m[land_amount_col], m[construction_col] + 1)\nelse:\n    m[\"land_value_density\"] = 0\n    print(\"‚ö†Ô∏è land_value_density: skipped (columns not found)\")\n\nm[\"new_vs_pre_owned_price\"] = safe_div(\n    m.get(\"price_new_house_transactions\", 0),\n    m.get(\"price_pre_owned_house_transactions\", 0) + 1\n)\n\n# Clean and validate\nratio_features = [\"price_area_ratio\", \"land_value_density\", \"new_vs_pre_owned_price\"]\nm[ratio_features] = m[ratio_features].replace([np.inf, -np.inf], 0).fillna(0)\n\nprint(\"‚úÖ Added ratio features:\", ratio_features)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:16:54.211257Z",
     "iopub.execute_input": "2025-10-05T16:16:54.211780Z",
     "iopub.status.idle": "2025-10-05T16:16:54.224296Z",
     "shell.execute_reply.started": "2025-10-05T16:16:54.211757Z",
     "shell.execute_reply": "2025-10-05T16:16:54.223566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚ö†Ô∏è land_value_density: skipped (columns not found)\n‚úÖ Added ratio features: ['price_area_ratio', 'land_value_density', 'new_vs_pre_owned_price']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Time Weighting",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply progressive month weighting (+10% per month)\nmonths = sorted(m[\"month\"].unique().tolist())\nmonth_to_idx = {mo:i for i,mo in enumerate(months)}\nm[\"month_weight\"] = m[\"month\"].map(lambda mo: 1.0 + 0.10 * month_to_idx[mo])\nprint(f\"‚úÖ Month weights range: {m['month_weight'].min():.2f}-{m['month_weight'].max():.2f}\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:16:57.186259Z",
     "iopub.execute_input": "2025-10-05T16:16:57.186807Z",
     "iopub.status.idle": "2025-10-05T16:16:57.194279Z",
     "shell.execute_reply.started": "2025-10-05T16:16:57.186781Z",
     "shell.execute_reply": "2025-10-05T16:16:57.193708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Month weights range: 1.00-7.60\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Feature Selection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Collect numeric features (excluding target & obvious non-features)\nexclude_cols = [\"y_log1p\", \"amount_new_house_transactions\", \"month\", \"sector\"]\nfeatures = [c for c in m.select_dtypes(include=[np.number]).columns if c not in exclude_cols]\nprint(\"‚úÖ Final feature count:\", len(features))\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:16:59.429996Z",
     "iopub.execute_input": "2025-10-05T16:16:59.430268Z",
     "iopub.status.idle": "2025-10-05T16:16:59.445019Z",
     "shell.execute_reply.started": "2025-10-05T16:16:59.430245Z",
     "shell.execute_reply": "2025-10-05T16:16:59.444178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Final feature count: 170\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## 8. LightGBM + XGBoost Ensemble Training",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Safe test alignment\ntest_df = data[\"test\"].copy()\nmissing_cols = [c for c in features if c not in test_df.columns]\nextra_cols = [c for c in test_df.columns if c not in features]\n\nfor c in missing_cols:\n    test_df[c] = 0\ntest_df = test_df.drop(columns=extra_cols, errors=\"ignore\")\nX_test = test_df[features].fillna(0)\nprint(f\"‚úÖ Test alignment: {len(features)} features used ({len(missing_cols)} added)\")\n\n# Prepare train\nX = m[features].fillna(0)\ny = m[\"amount_new_house_transactions\"].values\ny_log = m[\"y_log1p\"].values\n\n# Rolling folds\nfolds = []\nn_months = len(months)\nfor i in range(5):\n    train_end = int((i + 1) * n_months / 6)\n    val_start = train_end\n    val_end = min(train_end + n_months // 6, n_months)\n    if train_end == 0 or val_start >= val_end:\n        continue\n    folds.append((months[:train_end], months[val_start:val_end]))\nprint(\"‚úÖ Created folds:\", len(folds))\n\n# LightGBM params\nlgb_params = {\n    \"objective\": \"regression\",\n    \"metric\": \"mae\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"min_data_in_leaf\": 50,\n    \"feature_fraction\": 0.75,\n    \"bagging_fraction\": 0.75,\n    \"lambda_l1\": 0.5,\n    \"lambda_l2\": 1.0,\n    \"verbosity\": -1,\n    \"seed\": SEED\n}\n\noof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\ntest_lgb, test_xgb = [], []\n\nfor fold, (tr_mo, vl_mo) in enumerate(folds):\n    print(f\"\\n=== Fold {fold} | train {len(tr_mo)} val {len(vl_mo)} ===\")\n    tr_idx = m[\"month\"].isin(tr_mo)\n    vl_idx = m[\"month\"].isin(vl_mo)\n    X_tr, X_val = X.loc[tr_idx], X.loc[vl_idx]\n    y_tr, y_val = y_log[tr_idx], y[vl_idx]\n    w_tr = m.loc[tr_idx, \"month_weight\"].values\n\n    # LightGBM\n    if LGB_AVAILABLE:\n        dtr = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n        dval = lgb.Dataset(X_val, label=y_log[vl_idx])\n        lgbm = lgb.train(lgb_params, dtr, num_boost_round=3000,\n                         valid_sets=[dtr, dval],\n                         callbacks=[lgb.early_stopping(stopping_rounds=100),\n                                    lgb.log_evaluation(0)])\n        p_val_lgb = np.expm1(np.clip(lgbm.predict(X_val, num_iteration=lgbm.best_iteration), -20, 50))\n        p_test_lgb = np.expm1(np.clip(lgbm.predict(X_test, num_iteration=lgbm.best_iteration), -20, 50))\n    else:\n        p_val_lgb = np.zeros(len(X_val)); p_test_lgb = np.zeros(len(X_test))\n\n    # XGBoost\n    if XGB_AVAILABLE:\n        dtr_x = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n        dval_x = xgb.DMatrix(X_val, label=y_log[vl_idx])\n        xgb_params = {\n            \"objective\": \"reg:squarederror\",\n            \"eta\": 0.02,\n            \"max_depth\": 8,\n            \"subsample\": 0.8,\n            \"colsample_bytree\": 0.8,\n            \"lambda\": 1.0,\n            \"alpha\": 0.5,\n            \"seed\": SEED\n        }\n        xgbm = xgb.train(xgb_params, dtr_x, num_boost_round=2000,\n                         evals=[(dval_x, \"val\")],\n                         early_stopping_rounds=100, verbose_eval=False)\n        p_val_xgb = np.expm1(np.clip(xgbm.predict(xgb.DMatrix(X_val)), -20, 50))\n        p_test_xgb = np.expm1(np.clip(xgbm.predict(xgb.DMatrix(X_test)), -20, 50))\n    else:\n        p_val_xgb = np.zeros(len(X_val)); p_test_xgb = np.zeros(len(X_test))\n\n    p_val_blend = 0.7 * p_val_lgb + 0.3 * p_val_xgb\n    oof_lgb[vl_idx], oof_xgb[vl_idx] = p_val_lgb, p_val_xgb\n    test_lgb.append(p_test_lgb)\n    test_xgb.append(p_test_xgb)\n    evaluate_preds(y[vl_idx], p_val_blend, name=f\"fold{fold}_blend\")\n\n# Blend OOF/test\noof_blend = 0.7 * oof_lgb + 0.3 * oof_xgb\nevaluate_preds(y, oof_blend, name=\"OOF_blend\")\ntest_pred_blend = 0.7 * np.mean(test_lgb, axis=0) + 0.3 * np.mean(test_xgb, axis=0)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:17:01.659991Z",
     "iopub.execute_input": "2025-10-05T16:17:01.660569Z",
     "iopub.status.idle": "2025-10-05T16:18:52.861431Z",
     "shell.execute_reply.started": "2025-10-05T16:17:01.660544Z",
     "shell.execute_reply": "2025-10-05T16:18:52.860830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "‚úÖ Test alignment: 170 features used (170 added)\n‚úÖ Created folds: 5\n\n=== Fold 0 | train 11 val 11 ===\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1889]\ttraining's l1: 0.0371586\tvalid_1's l1: 0.142065\nfold0_blend | two-stage: 0.871094 | MAE: 5587.8262 | frac_bad: 0.0057\n\n=== Fold 1 | train 22 val 11 ===\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1043]\ttraining's l1: 0.0313842\tvalid_1's l1: 0.081758\nfold1_blend | two-stage: 0.926946 | MAE: 5210.5523 | frac_bad: 0.0011\n\n=== Fold 2 | train 33 val 11 ===\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1989]\ttraining's l1: 0.0160451\tvalid_1's l1: 0.0674053\nfold2_blend | two-stage: 0.940072 | MAE: 2100.2712 | frac_bad: 0.0011\n\n=== Fold 3 | train 44 val 11 ===\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1359]\ttraining's l1: 0.0169726\tvalid_1's l1: 0.0579297\nfold3_blend | two-stage: 0.948002 | MAE: 1925.5769 | frac_bad: 0.0000\n\n=== Fold 4 | train 55 val 11 ===\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1431]\ttraining's l1: 0.0149304\tvalid_1's l1: 0.0528517\nfold4_blend | two-stage: 0.951108 | MAE: 1229.5328 | frac_bad: 0.0000\nOOF_blend | two-stage: 0.765712 | MAE: 7298.5231 | frac_bad: 0.0013\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Sector Smoothing & Submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Apply 3-month rolling smoothing by sector\ntest_original = data[\"test\"].copy()\nsub = pd.DataFrame({\n    \"id\": test_original[\"id\"],\n    \"new_house_transaction_amount\": test_pred_blend\n})\nsub[\"month\"] = sub[\"id\"].str.extract(r\"(\\d{4} \\w+)\")[0]\nsub[\"sector\"] = sub[\"id\"].str.extract(r\"sector (\\d+)\")[0].astype(int)\n\nsub = sub.sort_values([\"sector\",\"month\"])\nsub[\"smooth_pred\"] = sub.groupby(\"sector\")[\"new_house_transaction_amount\"].transform(\n    lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n)\nsub[\"new_house_transaction_amount\"] = np.clip(sub[\"smooth_pred\"], 0, None)\nsub = sub[[\"id\",\"new_house_transaction_amount\"]]\n\nsave_df(sub, \"submission_v3_1_ensemble_smooth.csv\")\nprint(\"üèÅ submission_v3_1_ensemble_smooth.csv ready for Kaggle upload!\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T16:19:28.357169Z",
     "iopub.execute_input": "2025-10-05T16:19:28.357789Z",
     "iopub.status.idle": "2025-10-05T16:19:28.396932Z",
     "shell.execute_reply.started": "2025-10-05T16:19:28.357766Z",
     "shell.execute_reply": "2025-10-05T16:19:28.396336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üíæ Saved: submission_v3_1_ensemble_smooth.csv ((1152, 2))\nüèÅ submission_v3_1_ensemble_smooth.csv ready for Kaggle upload!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 10
  }
 ]
}
