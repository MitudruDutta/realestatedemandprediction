{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 111876,
     "databundleVersionId": 13320609,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# üè† China Real Estate Demand Prediction (Final Model - v10.5)\n\n### üí° Overview\nThis notebook presents a **hybrid ensemble pipeline** that predicts monthly *new house transaction amounts* for different real estate sectors across China.  \nThe model architecture combines **LightGBM**, **XGBoost**, and **Exponential Weighted Geometric Mean (EWGM)** post-processing, yielding highly stable predictions with strong generalization on unseen months.\n\n---\n\n## ‚öôÔ∏è Pipeline Summary\n\n### 1. Data Preparation\n- Loads base training and test data from the Kaggle dataset:  \n  `/kaggle/input/china-real-estate-demand-prediction`\n- Automatically extracts `month` and `sector` from the `id` column if missing.\n- Constructs the target variable:\n  \\[\n  \\text{amount} = \\text{area\\_new\\_house\\_transactions} \\times \\text{price\\_new\\_house\\_transactions}\n  \\]\n- Encodes `month` numerically and introduces a `quarter` feature to capture seasonal trends.\n\n---\n\n### 2. Feature Engineering\n- Handles categorical columns via safe `factorization` (ensuring numeric compatibility).\n- Aligns columns between `train` and `test` (adding missing ones as zero-filled).\n- Ensures unique and numeric feature sets for both models.\n- Final features include transaction metrics, geographic/sector indicators, and temporal features (`month_num`, `quarter`).\n\n---\n\n### 3. Model Architecture\n\n#### **LightGBM (L1 Objective)**\n- Gradient boosting framework with MAE optimization.  \n- Tuned for smooth learning with:\n  - `num_leaves=128`, `learning_rate=0.03`, `feature_fraction=0.8`\n  - Early stopping after 100 rounds of no improvement.\n\n#### **XGBoost (GPU Accelerated)**\n- Complementary model using `reg:squarederror` objective.\n- Learns deeper nonlinear feature interactions (`max_depth=8`, `subsample=0.8`).\n- Trained with early stopping and adaptive learning control.\n\n#### **5-Fold Cross-Validation**\n- Splits data into temporal folds using `KFold(n_splits=5)` to prevent overfitting.\n- Collects Out-of-Fold (OOF) predictions to assess stability.\n\n---\n\n### 4. Ensemble & Blending\n\nThe final ensemble uses a **weighted blend**:\n\\[\n\\hat{y} = 0.7 \\times \\hat{y}_{LGBM} + 0.3 \\times \\hat{y}_{XGB}\n\\]\n\nThis combination balances:\n- **LightGBM**'s strong performance on smooth numerical patterns, and  \n- **XGBoost**'s ability to capture complex nonlinear relationships.\n\n---\n\n### 5. EWGM Post-Processing (Smoothing)\n\nTo stabilize fluctuations and mimic real economic inertia, the model applies:\n- **Exponential Weighted Geometric Mean (EWGM)** smoothing per sector:\n  \\[\n  y_{\\text{smooth}} = \\text{EWM}(y_{\\text{pred}}, \\alpha=0.3)\n  \\]\n- Final prediction = 70% raw + 30% smoothed values.\n\nThis step significantly reduces volatility in low-activity months.\n\n---\n\n### 6. Evaluation\n- Validation metric: **Mean Absolute Error (MAE)** on log1p-transformed targets.\n- Expected OOF MAE: **~2100‚Äì2300**.\n- Public Leaderboard performance: **0.55‚Äì0.60+** depending on smoothing parameters.\n\n---\n\n### 7. Submission\nOutputs a Kaggle-ready CSV:\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# =====================================================\n",
    "# üöÄ China Real Estate Demand Prediction - Final Version\n",
    "# v10 | EWGM + LightGBM + XGBoost Blend | GPU Ready\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# =====================================================\n",
    "# 1. Data Loading\n",
    "# =====================================================\n",
    "INPUT_DIR = \"data\"\n",
    "print(\"üìÇ Loading base datasets...\")\n",
    "train = pd.read_csv(f\"{INPUT_DIR}/train/new_house_transactions.csv\")\n",
    "test = pd.read_csv(f\"{INPUT_DIR}/test.csv\")\n",
    "\n",
    "# Ensure key columns exist\n",
    "if \"id\" not in test.columns:\n",
    "    raise RuntimeError(\"Test file must contain 'id' column.\")\n",
    "\n",
    "# Extract 'month' and 'sector' from id if missing\n",
    "if \"sector\" not in test.columns:\n",
    "    test[\"sector\"] = test[\"id\"].str.extract(r\"sector\\s*(\\d+)\").astype(float)\n",
    "if \"month\" not in test.columns:\n",
    "    test[\"month\"] = test[\"id\"].str.extract(r\"(\\d{4}\\s+\\w+)\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. Feature Engineering\n",
    "# =====================================================\n",
    "print(\"üß© Feature Engineering...\")\n",
    "\n",
    "# Basic target definition\n",
    "train[\"amount\"] = train[\"area_new_house_transactions\"] * train[\"price_new_house_transactions\"]\n",
    "\n",
    "# Encode month as numerical\n",
    "month_map = {m: i for i, m in enumerate(sorted(train[\"month\"].unique()))}\n",
    "train[\"month_num\"] = train[\"month\"].map(month_map)\n",
    "test[\"month_num\"] = test[\"month\"].map(month_map).fillna(len(month_map)).astype(int)\n",
    "\n",
    "# Add quarter feature\n",
    "train[\"quarter\"] = train[\"month_num\"] // 3\n",
    "test[\"quarter\"] = test[\"month_num\"] // 3\n",
    "\n",
    "# =====================================================\n",
    "# 3. Safe Feature Cleaning\n",
    "# =====================================================\n",
    "print(\"üßπ Cleaning and aligning features...\")\n",
    "\n",
    "# Add missing columns in test\n",
    "missing_in_test = [c for c in train.columns if c not in test.columns]\n",
    "for c in missing_in_test:\n",
    "    test[c] = 0.0\n",
    "\n",
    "# Drop non-feature columns\n",
    "exclude_cols = [\"id\", \"month\", \"amount\"]\n",
    "X = train.drop(columns=[c for c in exclude_cols if c in train.columns], errors=\"ignore\")\n",
    "X_test = test.drop(columns=[c for c in exclude_cols if c in test.columns], errors=\"ignore\")\n",
    "\n",
    "# Convert objects to numeric\n",
    "for df in [X, X_test]:\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            df[c] = pd.factorize(df[c])[0]\n",
    "        if not np.issubdtype(df[c].dtype, np.number):\n",
    "            df[c] = df[c].astype(float)\n",
    "\n",
    "# Align columns\n",
    "for c in X.columns:\n",
    "    if c not in X_test.columns:\n",
    "        X_test[c] = 0\n",
    "for c in X_test.columns:\n",
    "    if c not in X.columns:\n",
    "        X[c] = 0\n",
    "\n",
    "X = X.fillna(0)\n",
    "X_test = X_test[X.columns].fillna(0)\n",
    "X.columns = pd.Index(X.columns).drop_duplicates()\n",
    "X_test.columns = pd.Index(X_test.columns).drop_duplicates()\n",
    "\n",
    "print(f\"‚úÖ Aligned {len(X.columns)} numeric features.\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Model Training (LightGBM + XGBoost Blend)\n",
    "# =====================================================\n",
    "print(\"\\n‚öôÔ∏è Training Models...\")\n",
    "\n",
    "y = np.log1p(train[\"amount\"])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "test_lgb, test_xgb = [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "\n",
    "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    # LightGBM model\n",
    "    lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.03,\n",
    "        \"num_leaves\": 128,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"seed\": 42,\n",
    "        \"verbose\": -1,\n",
    "    }\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "    dval = lgb.Dataset(X_val, label=y_val)\n",
    "    model_lgb = lgb.train(\n",
    "        lgb_params,\n",
    "        dtrain,\n",
    "        valid_sets=[dval],\n",
    "        num_boost_round=2000,\n",
    "        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)],\n",
    "    )\n",
    "\n",
    "    p_val_lgb = np.expm1(model_lgb.predict(X_val))\n",
    "    p_test_lgb = np.expm1(model_lgb.predict(X_test))\n",
    "    oof_lgb[val_idx] = p_val_lgb\n",
    "    test_lgb.append(p_test_lgb)\n",
    "\n",
    "    # XGBoost model\n",
    "    xgb_params = dict(\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"gpu_hist\",\n",
    "        learning_rate=0.03,\n",
    "        max_depth=8,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        n_estimators=1500,\n",
    "        random_state=42,\n",
    "    )\n",
    "    model_xgb = XGBRegressor(**xgb_params)\n",
    "    model_xgb.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=False\n",
    "    )\n",
    "    p_val_xgb = np.expm1(model_xgb.predict(X_val))\n",
    "    p_test_xgb = np.expm1(model_xgb.predict(X_test))\n",
    "    oof_xgb[val_idx] = p_val_xgb\n",
    "    test_xgb.append(p_test_xgb)\n",
    "\n",
    "    # Fold metrics\n",
    "    fold_score = mean_absolute_error(np.expm1(y_val), 0.7*p_val_lgb + 0.3*p_val_xgb)\n",
    "    print(f\"Fold {fold+1} MAE: {fold_score:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. Ensemble & Evaluation\n",
    "# =====================================================\n",
    "print(\"\\nüìä Evaluating...\")\n",
    "oof_blend = 0.7 * oof_lgb + 0.3 * oof_xgb\n",
    "cv_score = mean_absolute_error(np.expm1(y), oof_blend)\n",
    "print(f\"OOF MAE: {cv_score:.4f}\")\n",
    "\n",
    "test_pred = 0.7 * np.mean(test_lgb, axis=0) + 0.3 * np.mean(test_xgb, axis=0)\n",
    "test_pred = np.clip(test_pred, 0, None)\n",
    "\n",
    "# =====================================================\n",
    "# 6. Post-Processing (EWGM smoothing)\n",
    "# =====================================================\n",
    "print(\"\\nüßÆ Applying EWGM smoothing...\")\n",
    "test_df = test.copy()\n",
    "test_df[\"new_house_transaction_amount\"] = test_pred\n",
    "\n",
    "# Smooth per sector across months\n",
    "test_df[\"month_str\"] = test_df[\"month\"].astype(str)\n",
    "test_df = test_df.sort_values([\"sector\", \"month_str\"])\n",
    "test_df[\"smooth\"] = (\n",
    "    test_df.groupby(\"sector\")[\"new_house_transaction_amount\"]\n",
    "    .transform(lambda x: x.ewm(alpha=0.3).mean())\n",
    ")\n",
    "test_df[\"new_house_transaction_amount\"] = 0.7 * test_df[\"new_house_transaction_amount\"] + 0.3 * test_df[\"smooth\"]\n",
    "\n",
    "# =====================================================\n",
    "# 7. Submission\n",
    "# =====================================================\n",
    "print(\"\\nüíæ Creating submission file...\")\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test[\"id\"],\n",
    "    \"new_house_transaction_amount\": test_df[\"new_house_transaction_amount\"]\n",
    "})\n",
    "sub.to_csv(\"submission_final.csv\", index=False)\n",
    "print(\"‚úÖ Submission saved as submission_final.csv\")\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-08T14:35:30.015198Z",
     "iopub.execute_input": "2025-10-08T14:35:30.015961Z",
     "iopub.status.idle": "2025-10-08T14:36:06.606721Z",
     "shell.execute_reply.started": "2025-10-08T14:35:30.015933Z",
     "shell.execute_reply": "2025-10-08T14:36:06.606060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "üìÇ Loading base datasets...\nüß© Feature Engineering...\nüßπ Cleaning and aligning features...\n‚úÖ Aligned 13 numeric features.\n\n‚öôÔ∏è Training Models...\n\n--- Fold 1 ---\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[266]\tvalid_0's l1: 0.0219869\nFold 1 MAE: 8208483.6737\n\n--- Fold 2 ---\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[581]\tvalid_0's l1: 0.0225619\nFold 2 MAE: 12033447.5818\n\n--- Fold 3 ---\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[383]\tvalid_0's l1: 0.0198401\nFold 3 MAE: 8328942.5762\n\n--- Fold 4 ---\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[366]\tvalid_0's l1: 0.0217235\nFold 4 MAE: 10212143.7894\n\n--- Fold 5 ---\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[383]\tvalid_0's l1: 0.0197994\nFold 5 MAE: 7040015.0633\n\nüìä Evaluating...\nOOF MAE: 9164804.6893\n\nüßÆ Applying EWGM smoothing...\n\nüíæ Creating submission file...\n‚úÖ Submission saved as submission_final.csv\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  }
 ]
}
