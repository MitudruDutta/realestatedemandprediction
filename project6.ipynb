{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 111876,
     "databundleVersionId": 13320609,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Predicting Real Estate Demand: A Two-Model Approach with Post-Processing\n\nThis document outlines a complete machine learning workflow for predicting the `new_house_transaction_amount` in a real estate demand prediction competition.\n\nThe core strategy is more advanced than simply predicting the final amount directly. Instead, it builds two separate, more stable models:\n1.  A model to predict the **price per area**.\n2.  A model to predict the **total area**.\n\nThe predictions from these two models are then combined and refined through a series of intelligent post-processing steps to improve the final score and handle real-world data complexities.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 1. Setup and Data Loading\n\nFirst, we import the necessary libraries and set up our environment. The code is designed to be flexible, using the high-performance **LightGBM** library if it's available, and falling back to **RandomForestRegressor** otherwise.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Try to use LightGBM if present\n",
    "LGB_AVAILABLE = False\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define input/output directories\n",
    "INPUT_DIR = \"data\"\n",
    "OUT_DIR = \"outputs\"\n",
    "print(\"INPUT_DIR:\", INPUT_DIR, \"OUT_DIR:\", OUT_DIR, \"LightGBM:\", LGB_AVAILABLE)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(os.path.join(INPUT_DIR, \"train/new_house_transactions.csv\"))\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\n",
    "\n",
    "print(\"train shape:\", train.shape, \"test shape:\", test.shape)"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T13:24:40.915427Z",
     "iopub.execute_input": "2025-10-07T13:24:40.915675Z",
     "iopub.status.idle": "2025-10-07T13:24:46.473902Z",
     "shell.execute_reply.started": "2025-10-07T13:24:40.915645Z",
     "shell.execute_reply": "2025-10-07T13:24:46.473061Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-07T13:36:29.049450Z",
     "start_time": "2025-10-07T13:36:21.417664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_DIR: data OUT_DIR: outputs LightGBM: True\n",
      "train shape: (5433, 11) test shape: (1152, 2)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Data Preparation and Feature Engineering\n\nBefore modeling, we clean the data and create a consistent set of features for both the training and test sets.\n\n* The target columns (`area`, `price`, `amount`) are identified. If the `amount` column is missing, it's calculated as `area * price`.\n* Missing numerical values are filled with 0.\n* Categorical features like `sector` and `month` are converted into numerical codes so the model can use them.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Robustly find column names\ndef find_col(df, candidates):\n    for c in candidates:\n        if c in df.columns:\n            return c\n    return None\n\narea_col_train = find_col(train, [\"area_new_house_transactions\"])\nprice_col_train = find_col(train, [\"price_new_house_transactions\"])\n\n# --- Fix missing sector/month in test ---\nif \"sector\" not in test.columns:\n    # Extract from \"id\"\n    test[\"sector\"] = test[\"id\"].str.extract(r\"sector (\\d+)\").astype(float)\nif \"month\" not in test.columns:\n    test[\"month\"] = test[\"id\"].str.extract(r\"(\\d{4} \\w+)\")\n\n# Basic cleaning and type conversion\ntrain = train.copy()\ntest = test.copy()\ntrain[area_col_train] = train[area_col_train].fillna(0).astype(float)\ntrain[price_col_train] = train[price_col_train].fillna(0).astype(float)\ntrain[\"amount_new_house_transactions\"] = train[area_col_train] * train[price_col_train]\n\n# Define the feature set, excluding identifiers and targets\nexclude = set([\"month\", \"sector\", \"id\", area_col_train, price_col_train, \"amount_new_house_transactions\"])\nfeatures = [c for c in train.columns if c not in exclude and train[c].dtype in [np.int64, np.float64]]\n\n# Add encoded categorical features\ntrain[\"sector_code\"] = pd.factorize(train[\"sector\"].astype(str))[0]\ntest[\"sector_code\"] = pd.factorize(test[\"sector\"].astype(str))[0]\n\n# Month encoding\nmonths = sorted(train[\"month\"].astype(str).unique().tolist())\nmo2i = {m: i for i, m in enumerate(months)}\ntrain[\"month_code\"] = train[\"month\"].astype(str).map(mo2i).fillna(-1).astype(int)\ntest[\"month_code\"] = test[\"month\"].astype(str).map(mo2i).fillna(-1).astype(int)\n\n# Final feature list\nfeatures = [\"month_code\", \"sector_code\"] + features\n\nprint(\"âœ… Using features:\", features[:20], f\"(total {len(features)})\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T13:26:00.124133Z",
     "iopub.execute_input": "2025-10-07T13:26:00.124405Z",
     "iopub.status.idle": "2025-10-07T13:26:00.149680Z",
     "shell.execute_reply.started": "2025-10-07T13:26:00.124384Z",
     "shell.execute_reply": "2025-10-07T13:26:00.149052Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-07T13:36:39.372369Z",
     "start_time": "2025-10-07T13:36:39.351970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using features: ['month_code', 'sector_code', 'num_new_house_transactions', 'area_per_unit_new_house_transactions', 'total_price_per_unit_new_house_transactions', 'num_new_house_available_for_sale', 'area_new_house_available_for_sale', 'period_new_house_sell_through'] (total 8)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Modeling Strategy: Predicting Price and Area Separately\n\nInstead of predicting the total `amount` directly, we build two separate models. This approach can be more robust because `price per area` and `area` might have different relationships with the input features.\n\n**Log Transformation:** We apply a `log1p` transformation (`log(1+x)`) to our target variables. This is a common technique in regression that helps to normalize skewed data and can significantly improve model performance. The predictions are later converted back to their original scale using `expm1`.\n",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### a) Training the Price Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- Deduplicate columns (critical for LightGBM) ---\nfeatures = list(dict.fromkeys(features))  # removes duplicates\ntrain = train.loc[:, ~train.columns.duplicated()]\ntest = test.loc[:, ~test.columns.duplicated()]\nprint(f\"âœ… Deduplicated features: {len(features)} remain.\")\n\n# --- Prepare aligned feature matrices ---\nX_price = train[features].fillna(0)\ny_price = np.log1p(train[price_col_train].clip(lower=0).astype(float).values)\n\n# Align test columns with train features\nX_test = test.copy()\nfor c in features:\n    if c not in X_test.columns:\n        X_test[c] = 0.0\nX_test = X_test[features].fillna(0)\n\nprint(f\"âœ… Feature alignment done: {len(features)} features used.\")\n\n# --- LightGBM parameters ---\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"mae\",\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 64,\n    \"seed\": 42,\n    \"verbosity\": -1,\n}\n\n# --- Train model ---\nprint(\"ðŸš€ Training LightGBM price model...\")\ndtrain = lgb.Dataset(X_price, label=y_price)\nmodel_price = lgb.train(params, dtrain, num_boost_round=800)\n\n# --- Predict ---\npred_price = np.expm1(model_price.predict(X_test))\npred_price = np.clip(pred_price, 0, None)\nprint(f\"âœ… Price predictions ready. Shape: {pred_price.shape}, mean={pred_price.mean():.2f}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T13:31:21.021902Z",
     "iopub.execute_input": "2025-10-07T13:31:21.022423Z",
     "iopub.status.idle": "2025-10-07T13:31:22.317160Z",
     "shell.execute_reply.started": "2025-10-07T13:31:21.022397Z",
     "shell.execute_reply": "2025-10-07T13:31:22.316539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "âœ… Deduplicated features: 8 remain.\nâœ… Feature alignment done: 8 features used.\nðŸš€ Training LightGBM price model...\nâœ… Price predictions ready. Shape: (1152,), mean=22286.24\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "### b) Training the Area Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "X_area = X_price  # Use the same features\ny_area = np.log1p(train[area_col_train].clip(lower=0).astype(float).values)\ndtrain2 = lgb.Dataset(X_area, label=y_area)\nparams2 = { **params, \"seed\": SEED + 1 } # Use a different seed for model diversity\n\nprint(\"Training LightGBM area model...\")\nmodel_area = lgb.train(params2, dtrain2, num_boost_round=800)\npred_area = np.expm1(model_area.predict(X_test))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T13:31:53.059815Z",
     "iopub.execute_input": "2025-10-07T13:31:53.060400Z",
     "iopub.status.idle": "2025-10-07T13:31:54.396815Z",
     "shell.execute_reply.started": "2025-10-07T13:31:53.060374Z",
     "shell.execute_reply": "2025-10-07T13:31:54.396200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Training LightGBM area model...\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Post-Processing: Refining the Predictions\n\nThe raw model predictions are refined through several intelligent steps to handle potential issues and improve the final score.\n\n1.  **Combine & Scale:** The final `amount` is calculated by multiplying the predictions (`price * area`). A check is performed to scale the result by 10,000 if the values are too large, ensuring they match the competition's expected units.\n2.  **Sector Fallback:** To handle cases where the model predicts an unreasonably small value (e.g., near zero), these predictions are replaced with 80% of the median amount for that specific sector (calculated from the training data).\n3.  **Smoothing:** A 3-month centered rolling mean is applied to the predictions within each sector. This smooths out sharp, unrealistic month-to-month spikes or dips.\n4.  **Outlier Clipping & Flooring:** Finally, predictions are clipped at the top and bottom (based on the 1st and 99th percentiles) to control for extreme outliers, and any tiny values are floored to zero.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# --- Combine, Scale, and Post-Process ---\npred_amount = pred_price * pred_area\npred_amount = np.clip(pred_amount, 0, None)\nmean_pred = np.nanmean(pred_amount)\nif mean_pred > 1e5:\n    print(f\"Detected large-scale predictions (mean {mean_pred:.1f}), scaling down by 10,000.\")\n    pred_amount /= 10000.0\n\npred_df = pd.DataFrame({\"id\": test[\"id\"], \"sector\": test[\"sector\"], \"pred_amount\": pred_amount})\n\n# Sector fallback for near-zero predictions\ntrain_sector_median = train.groupby(\"sector\")[\"amount_new_house_transactions\"].median().to_dict()\nmask_zero = pred_df[\"pred_amount\"] < 1.0\npred_df[\"sector_median\"] = pred_df[\"sector\"].map(train_sector_median)\nn_replaced = mask_zero & pred_df[\"sector_median\"].notna()\npred_df.loc[n_replaced, \"pred_amount\"] = pred_df.loc[n_replaced, \"sector_median\"] * 0.8\nprint(f\"Sector fallback replaced {int(n_replaced.sum())} near-zero predictions.\")\n\n# Smoothing with rolling mean\npred_df[\"month\"] = pred_df[\"id\"].str.extract(r\"(\\d{4} \\w+)\", expand=False)\npred_df[\"sector_int\"] = pred_df[\"sector\"].astype(int)\npred_df = pred_df.sort_values([\"sector_int\",\"month\"]).reset_index(drop=True)\npred_df[\"smoothed\"] = pred_df.groupby(\"sector_int\")[\"pred_amount\"].transform(lambda s: s.rolling(window=3, min_periods=1, center=True).mean())\npred_df[\"pred_amount\"] = np.clip(pred_df[\"smoothed\"], 0, None)\n\n# Outlier clipping and flooring\nq1, q99 = pred_df[\"pred_amount\"].quantile(0.01), pred_df[\"pred_amount\"].quantile(0.99)\npred_df[\"pred_amount\"] = pred_df[\"pred_amount\"].clip(lower=q1*0.5, upper=q99*1.5)\npred_df[\"pred_amount\"] = pred_df[\"pred_amount\"].where(pred_df[\"pred_amount\"] >= 1.0, 0.0)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T13:32:34.861180Z",
     "iopub.execute_input": "2025-10-07T13:32:34.861435Z",
     "iopub.status.idle": "2025-10-07T13:32:34.904588Z",
     "shell.execute_reply.started": "2025-10-07T13:32:34.861417Z",
     "shell.execute_reply": "2025-10-07T13:32:34.904023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Detected large-scale predictions (mean 733262.7), scaling down by 10,000.\nSector fallback replaced 0 near-zero predictions.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Final Submission\n\nThe fully processed predictions are now saved to a `submission.csv` file in the format required by the competition.\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "submission = pred_df[[\"id\",\"pred_amount\"]].rename(columns={\"pred_amount\":\"new_house_transaction_amount\"})\nout_path = os.path.join(OUT_DIR, \"submission_v6_price_area.csv\")\nsubmission.to_csv(out_path, index=False)\n\nprint(\"Saved submission to:\", out_path)\nprint(\"\\nDone. Upload the generated CSV to Kaggle.\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-07T13:33:02.082911Z",
     "iopub.execute_input": "2025-10-07T13:33:02.083290Z",
     "iopub.status.idle": "2025-10-07T13:33:02.096718Z",
     "shell.execute_reply.started": "2025-10-07T13:33:02.083268Z",
     "shell.execute_reply": "2025-10-07T13:33:02.096042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Saved submission to: /kaggle/working/submission_v6_price_area.csv\n\nDone. Upload the generated CSV to Kaggle.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 9
  }
 ]
}
