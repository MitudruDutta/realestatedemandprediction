{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7cd0ef",
   "metadata": {
    "papermill": {
     "duration": 0.004987,
     "end_time": "2025-10-07T14:29:55.892932",
     "exception": false,
     "start_time": "2025-10-07T14:29:55.887945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Advanced Real Estate Demand Prediction: An End-to-End Kaggle Workflow\n",
    "\n",
    "This document breaks down an advanced, single-script solution for a time-series regression competition, likely the \"China Real Estate Demand Prediction\" on Kaggle. The goal is to predict the `new_house_transaction_amount`.\n",
    "\n",
    "The workflow is sophisticated and demonstrates a professional approach, including:\n",
    "* **Extensive Feature Engineering:** Creating time-based, lag, and rolling features.\n",
    "* **Robust Cross-Validation:** Using a time-based rolling split appropriate for time-series data.\n",
    "* **Multi-Model Strategy:** Training multiple high-performance models like LightGBM, XGBoost, and CatBoost.\n",
    "* **Multi-Target Approach:** Building separate models for `price` and `area` in addition to a model for the final `amount`.\n",
    "* **Advanced Ensembling:** Combining predictions using a Weighted Geometric Mean and a final meta-blend.\n",
    "* **Multi-Stage Post-Processing:** Applying a series of heuristic rules to refine the final predictions and maximize the competition score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e4648c",
   "metadata": {
    "papermill": {
     "duration": 0.003575,
     "end_time": "2025-10-07T14:29:55.902183",
     "exception": false,
     "start_time": "2025-10-07T14:29:55.898608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Imports and Configuration\n",
    "Sets up all necessary libraries and global parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4ebd998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:29:55.910930Z",
     "iopub.status.busy": "2025-10-07T14:29:55.910613Z",
     "iopub.status.idle": "2025-10-07T14:30:03.579114Z",
     "shell.execute_reply": "2025-10-07T14:30:03.578290Z"
    },
    "papermill": {
     "duration": 7.674355,
     "end_time": "2025-10-07T14:30:03.580307",
     "exception": false,
     "start_time": "2025-10-07T14:29:55.905952",
     "status": "completed"
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-10-07T15:14:39.064077Z",
     "start_time": "2025-10-07T15:14:28.568707Z"
    }
   },
   "source": [
    "import os, sys, math, warnings, time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Try boosters (optional)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB = True\n",
    "except Exception:\n",
    "    LGB = False\n",
    "    print(\"LightGBM not available\")\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB = True\n",
    "except Exception:\n",
    "    XGB = False\n",
    "    print(\"XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool\n",
    "    CAT = True\n",
    "except Exception:\n",
    "    CAT = False\n",
    "    print(\"CatBoost not available\")\n",
    "\n",
    "# Global settings\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Paths configuration\n",
    "# KAGGLE_INPUT = \"/kaggle/input/china-real-estate-demand-prediction\"\n",
    "# ALT_INPUT = \"/mnt/data\"\n",
    "\n",
    "# if os.path.exists(KAGGLE_INPUT):\n",
    "#     INPUT_DIR = KAGGLE_INPUT\n",
    "# elif os.path.exists(ALT_INPUT):\n",
    "#     INPUT_DIR = ALT_INPUT\n",
    "# else:\n",
    "#     raise FileNotFoundError(\"Can't find dataset directory\")\n",
    "\n",
    "INPUT_DIR = \"data\"\n",
    "OUT_DIR = \"outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUT_DIR}\")\n",
    "print(f\"Available boosters - LGB: {LGB}, XGB: {XGB}, CAT: {CAT}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory: data\n",
      "Output directory: outputs\n",
      "Available boosters - LGB: True, XGB: True, CAT: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "2eecf9e2",
   "metadata": {
    "papermill": {
     "duration": 0.003552,
     "end_time": "2025-10-07T14:30:03.587903",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.584351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "Core utility functions for evaluation, data loading, and ensemble calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2388cf86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:03.596468Z",
     "iopub.status.busy": "2025-10-07T14:30:03.595950Z",
     "iopub.status.idle": "2025-10-07T14:30:03.604899Z",
     "shell.execute_reply": "2025-10-07T14:30:03.604152Z"
    },
    "papermill": {
     "duration": 0.014416,
     "end_time": "2025-10-07T14:30:03.605957",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.591541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def two_stage_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom two-stage evaluation metric:\n",
    "    - Stage 1: Check if more than 30% of predictions have APE > 1.0\n",
    "    - Stage 2: Calculate MAPE on predictions with APE <= 1.0\n",
    "    \"\"\"\n",
    "    eps = 1e-12\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps)\n",
    "    frac_bad = np.mean(ape > 1.0)\n",
    "    \n",
    "    if frac_bad > 0.3: \n",
    "        return 0.0\n",
    "    \n",
    "    mask = (ape <= 1.0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    mape = np.mean(ape[mask])\n",
    "    return 1.0 - (mape / max(1 - frac_bad, 1e-12))\n",
    "\n",
    "\n",
    "def evaluate_preds(y_true, y_pred, name=\"eval\"):\n",
    "    \"\"\"Evaluate predictions using both two-stage score and MAE\"\"\"\n",
    "    s = two_stage_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"{name} | two-stage: {s:.6f} | MAE: {mae:.2f}\")\n",
    "    return {\"score\": s, \"mae\": mae}\n",
    "\n",
    "\n",
    "def safe_read(path):\n",
    "    \"\"\"Safely read CSV file, return None if doesn't exist\"\"\"\n",
    "    return pd.read_csv(path) if os.path.exists(path) else None\n",
    "\n",
    "\n",
    "def align_test_with_features(test_df, features):\n",
    "    \"\"\"Ensure test dataframe has all required features\"\"\"\n",
    "    test_a = test_df.copy()\n",
    "    for c in features:\n",
    "        if c not in test_a.columns:\n",
    "            test_a[c] = 0.0\n",
    "    return test_a[features].fillna(0)\n",
    "\n",
    "\n",
    "def wgeom(preds, weights, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Weighted Geometric Mean Ensemble (WGME)\n",
    "    preds: array (n_models, n_samples)\n",
    "    weights: (n_models,) -> normalized\n",
    "    Returns: exp(sum(w_i * log(pred_i + eps)))\n",
    "    \"\"\"\n",
    "    logs = np.log(np.clip(preds, eps, None))\n",
    "    return np.exp(np.dot(weights, logs))\n",
    "\n",
    "\n",
    "def safe_div(a, b):\n",
    "    \"\"\"Safe division avoiding divide by zero\"\"\"\n",
    "    return np.where((b == 0) | (np.isnan(b)), 0.0, a / (b + 1e-9))\n",
    "\n",
    "\n",
    "def merge_safe(base, other, prefix):\n",
    "    \"\"\"Safely merge dataframes with column prefixing\"\"\"\n",
    "    if other is None: \n",
    "        return base\n",
    "    o = other.copy()\n",
    "    for c in o.columns:\n",
    "        if c not in [\"month\", \"sector\"]:\n",
    "            o = o.rename(columns={c: f\"{prefix}_{c}\"})\n",
    "    return base.merge(o, on=[\"month\", \"sector\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba7ca5",
   "metadata": {
    "papermill": {
     "duration": 0.003421,
     "end_time": "2025-10-07T14:30:03.613018",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.609597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load all training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:03.620965Z",
     "iopub.status.busy": "2025-10-07T14:30:03.620753Z",
     "iopub.status.idle": "2025-10-07T14:30:03.782626Z",
     "shell.execute_reply": "2025-10-07T14:30:03.781778Z"
    },
    "papermill": {
     "duration": 0.167292,
     "end_time": "2025-10-07T14:30:03.783880",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.616588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base datasets...\n",
      "✓ Loaded train_new: (5433, 11)\n",
      "✓ Loaded test: (1152, 2)\n",
      "✓ Additional datasets loaded: 4\n",
      "✓ Extracted sector from test IDs\n",
      "✓ Extracted month from test IDs\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading base datasets...\")\n",
    "\n",
    "# Load training datasets\n",
    "train_new = safe_read(os.path.join(INPUT_DIR, \"train/new_house_transactions.csv\"))\n",
    "train_pre = safe_read(os.path.join(INPUT_DIR, \"train/pre_owned_house_transactions.csv\"))\n",
    "train_land = safe_read(os.path.join(INPUT_DIR, \"train/land_transactions.csv\"))\n",
    "train_new_near = safe_read(os.path.join(INPUT_DIR, \"train/new_house_transactions_nearby_sectors.csv\"))\n",
    "train_pre_near = safe_read(os.path.join(INPUT_DIR, \"train/pre_owned_house_transactions_nearby_sectors.csv\"))\n",
    "train_land_near = safe_read(os.path.join(INPUT_DIR, \"train/land_transactions_nearby_sectors.csv\"))\n",
    "poi = safe_read(os.path.join(INPUT_DIR, \"train/sector_POI.csv\"))\n",
    "city_idx = safe_read(os.path.join(INPUT_DIR, \"train/city_indexes.csv\"))\n",
    "\n",
    "# Load test data\n",
    "test = safe_read(os.path.join(INPUT_DIR, \"test.csv\"))\n",
    "\n",
    "if train_new is None or test is None:\n",
    "    raise RuntimeError(\"Missing essential files; check INPUT_DIR path\")\n",
    "\n",
    "print(f\"✓ Loaded train_new: {train_new.shape}\")\n",
    "print(f\"✓ Loaded test: {test.shape}\")\n",
    "print(f\"✓ Additional datasets loaded: {sum([x is not None for x in [train_pre, train_land, poi, city_idx]])}\")\n",
    "\n",
    "# Fix test data if sector/month columns are missing\n",
    "if \"sector\" not in test.columns:\n",
    "    test[\"sector\"] = test[\"id\"].str.extract(r\"sector\\s*(\\d+)\").astype(float)\n",
    "    print(\"✓ Extracted sector from test IDs\")\n",
    "\n",
    "if \"month\" not in test.columns:\n",
    "    test[\"month\"] = test[\"id\"].str.extract(r\"(\\d{4}\\s+\\w+)\")\n",
    "    print(\"✓ Extracted month from test IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea495b8",
   "metadata": {
    "papermill": {
     "duration": 0.003657,
     "end_time": "2025-10-07T14:30:03.791599",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.787942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Data Merging\n",
    "\n",
    "Merge all datasets into a master training table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f64c21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:03.800905Z",
     "iopub.status.busy": "2025-10-07T14:30:03.800486Z",
     "iopub.status.idle": "2025-10-07T14:30:03.876515Z",
     "shell.execute_reply": "2025-10-07T14:30:03.875726Z"
    },
    "papermill": {
     "duration": 0.081772,
     "end_time": "2025-10-07T14:30:03.877824",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.796052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets into master table...\n",
      "✓ Merged pre-owned data. Shape: (5433, 15)\n",
      "✓ Merged land data. Shape: (5433, 19)\n",
      "✓ Merged nearby sector data. Shape: (5433, 36)\n",
      "✓ Merged POI data. Shape: (5433, 177)\n",
      "Final merged table shape: (5433, 177)\n",
      "Columns: 177\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging datasets into master table...\")\n",
    "\n",
    "# Start with new house transactions as base\n",
    "m = train_new.copy()\n",
    "\n",
    "# Merge pre-owned house data\n",
    "m = merge_safe(m, train_pre, \"pre\")\n",
    "print(f\"✓ Merged pre-owned data. Shape: {m.shape}\")\n",
    "\n",
    "# Merge land transactions\n",
    "m = merge_safe(m, train_land, \"land\")\n",
    "print(f\"✓ Merged land data. Shape: {m.shape}\")\n",
    "\n",
    "# Merge nearby sector data\n",
    "m = merge_safe(m, train_new_near, \"newnear\")\n",
    "m = merge_safe(m, train_pre_near, \"prenear\")\n",
    "m = merge_safe(m, train_land_near, \"landnear\")\n",
    "print(f\"✓ Merged nearby sector data. Shape: {m.shape}\")\n",
    "\n",
    "# Merge POI data\n",
    "if poi is not None:\n",
    "    try:\n",
    "        poi[\"sector\"] = poi[\"sector\"].astype(m[\"sector\"].dtype)\n",
    "    except Exception:\n",
    "        poi[\"sector\"] = poi[\"sector\"].astype(str)\n",
    "        m[\"sector\"] = m[\"sector\"].astype(str)\n",
    "    m = m.merge(poi, on=\"sector\", how=\"left\")\n",
    "    print(f\"✓ Merged POI data. Shape: {m.shape}\")\n",
    "\n",
    "# Merge city indexes\n",
    "if city_idx is not None:\n",
    "    if \"month\" in city_idx.columns:\n",
    "        m = m.merge(city_idx, on=\"month\", how=\"left\")\n",
    "        print(f\"✓ Merged city indexes. Shape: {m.shape}\")\n",
    "\n",
    "# Fill missing values\n",
    "m = m.fillna(0)\n",
    "\n",
    "# Compute amount if not present\n",
    "if \"amount_new_house_transactions\" not in m.columns:\n",
    "    if \"area_new_house_transactions\" in m.columns and \"price_new_house_transactions\" in m.columns:\n",
    "        m[\"amount_new_house_transactions\"] = m[\"area_new_house_transactions\"] * m[\"price_new_house_transactions\"]\n",
    "        print(\"✓ Computed amount from area × price\")\n",
    "    else:\n",
    "        m[\"amount_new_house_transactions\"] = 0.0\n",
    "        print(\"⚠ Warning: Could not compute amount\")\n",
    "\n",
    "print(f\"Final merged table shape: {m.shape}\")\n",
    "print(f\"Columns: {len(m.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4296724",
   "metadata": {
    "papermill": {
     "duration": 0.003926,
     "end_time": "2025-10-07T14:30:03.886038",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.882112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Feature Engineering\n",
    "Create time-based, lag, rolling, and interaction features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52f8e062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:03.895285Z",
     "iopub.status.busy": "2025-10-07T14:30:03.894496Z",
     "iopub.status.idle": "2025-10-07T14:30:04.007453Z",
     "shell.execute_reply": "2025-10-07T14:30:04.006498Z"
    },
    "papermill": {
     "duration": 0.118682,
     "end_time": "2025-10-07T14:30:04.008663",
     "exception": false,
     "start_time": "2025-10-07T14:30:03.889981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering...\n",
      "✓ Created time features (month_code, sin, cos)\n",
      "✓ Created lag and rolling features\n",
      "✓ Created EWM features\n",
      "✓ Created price/area ratio\n",
      "Feature engineering complete. Total columns: 199\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature engineering...\")\n",
    "\n",
    "# ===== Time Features =====\n",
    "m[\"month_str\"] = m[\"month\"].astype(str)\n",
    "months = sorted(m[\"month_str\"].unique().tolist())\n",
    "mo2i = {mth: i for i, mth in enumerate(months)}\n",
    "m[\"month_code\"] = m[\"month_str\"].map(mo2i).fillna(-1).astype(int)\n",
    "\n",
    "# Cyclical encoding for seasonality\n",
    "m[\"month_sin\"] = np.sin(2 * np.pi * m[\"month_code\"] / 12)\n",
    "m[\"month_cos\"] = np.cos(2 * np.pi * m[\"month_code\"] / 12)\n",
    "print(f\"✓ Created time features (month_code, sin, cos)\")\n",
    "\n",
    "\n",
    "# ===== Lag and Rolling Features =====\n",
    "def add_lags_rolls(df, group=\"sector\", \n",
    "                   cols=[\"amount_new_house_transactions\",\n",
    "                         \"area_new_house_transactions\",\n",
    "                         \"num_new_house_transactions\"], \n",
    "                   lags=[1, 2, 3], rolls=[3]):\n",
    "    \"\"\"Add lag and rolling window features\"\"\"\n",
    "    df = df.sort_values([group, \"month\"]).reset_index(drop=True)\n",
    "    \n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            # Lag features\n",
    "            for lag in lags:\n",
    "                df[f\"{c}_lag{lag}\"] = df.groupby(group)[c].shift(lag).fillna(0)\n",
    "            \n",
    "            # Rolling mean features\n",
    "            for r in rolls:\n",
    "                df[f\"{c}_roll{r}\"] = df.groupby(group)[c].rolling(\n",
    "                    r, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "            \n",
    "            # Last non-zero value\n",
    "            df[f\"{c}_last\"] = df.groupby(group)[c].shift(1).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "m = add_lags_rolls(m)\n",
    "print(f\"✓ Created lag and rolling features\")\n",
    "\n",
    "\n",
    "# ===== Exponential Weighted Mean =====\n",
    "m[\"amount_ewm_3\"] = m.groupby(\"sector\")[\"amount_new_house_transactions\"].transform(\n",
    "    lambda s: s.ewm(span=3, adjust=False).mean()).fillna(0)\n",
    "m[\"amount_ewm_6\"] = m.groupby(\"sector\")[\"amount_new_house_transactions\"].transform(\n",
    "    lambda s: s.ewm(span=6, adjust=False).mean()).fillna(0)\n",
    "print(f\"✓ Created EWM features\")\n",
    "\n",
    "\n",
    "# ===== Interaction Features =====\n",
    "if \"price_new_house_transactions\" in m.columns and \"area_new_house_transactions\" in m.columns:\n",
    "    m[\"price_area_ratio\"] = safe_div(\n",
    "        m[\"price_new_house_transactions\"], \n",
    "        m[\"area_new_house_transactions\"])\n",
    "    print(f\"✓ Created price/area ratio\")\n",
    "else:\n",
    "    m[\"price_area_ratio\"] = 0.0\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "m = m.fillna(0)\n",
    "\n",
    "print(f\"Feature engineering complete. Total columns: {len(m.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b86169",
   "metadata": {
    "papermill": {
     "duration": 0.003968,
     "end_time": "2025-10-07T14:30:04.016797",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.012829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Feature Selection\n",
    "Build final feature list for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8782c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:04.025447Z",
     "iopub.status.busy": "2025-10-07T14:30:04.024997Z",
     "iopub.status.idle": "2025-10-07T14:30:04.046270Z",
     "shell.execute_reply": "2025-10-07T14:30:04.045289Z"
    },
    "papermill": {
     "duration": 0.027005,
     "end_time": "2025-10-07T14:30:04.047552",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.020547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building feature list...\n",
      "✓ Feature count: 196\n",
      "✓ First 10 features: ['month_code', 'sector_code', 'num_new_house_transactions', 'area_new_house_transactions', 'price_new_house_transactions', 'area_per_unit_new_house_transactions', 'total_price_per_unit_new_house_transactions', 'num_new_house_available_for_sale', 'area_new_house_available_for_sale', 'period_new_house_sell_through']\n",
      "✓ Last 10 features: ['area_new_house_transactions_roll3', 'area_new_house_transactions_last', 'num_new_house_transactions_lag1', 'num_new_house_transactions_lag2', 'num_new_house_transactions_lag3', 'num_new_house_transactions_roll3', 'num_new_house_transactions_last', 'amount_ewm_3', 'amount_ewm_6', 'price_area_ratio']\n",
      "\n",
      "Feature data types summary:\n",
      "float64    190\n",
      "int64        6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Building feature list...\")\n",
    "\n",
    "# Columns to exclude from features\n",
    "exclude = {\n",
    "    \"month\", \"month_str\", \"sector\", \"id\", \n",
    "    \"amount_new_house_transactions\", \"y_log1p\"\n",
    "}\n",
    "\n",
    "# Select numeric features only\n",
    "features = [c for c in m.select_dtypes(include=[np.number]).columns \n",
    "            if c not in exclude]\n",
    "\n",
    "# Add sector encoding\n",
    "m[\"sector_code\"] = pd.factorize(m[\"sector\"].astype(str))[0]\n",
    "\n",
    "# Prioritize important features at the beginning\n",
    "if \"month_code\" in features:\n",
    "    features = [\"month_code\"] + [f for f in features if f != \"month_code\"]\n",
    "\n",
    "features = [\"month_code\", \"sector_code\"] + [\n",
    "    f for f in features if f not in (\"month_code\", \"sector_code\")\n",
    "]\n",
    "\n",
    "# Remove duplicates and ensure all features exist\n",
    "features = list(dict.fromkeys(features))\n",
    "features = [f for f in features if f in m.columns]\n",
    "\n",
    "print(f\"✓ Feature count: {len(features)}\")\n",
    "print(f\"✓ First 10 features: {features[:10]}\")\n",
    "print(f\"✓ Last 10 features: {features[-10:]}\")\n",
    "\n",
    "# Verify data types\n",
    "print(\"\\nFeature data types summary:\")\n",
    "print(m[features].dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a7982",
   "metadata": {
    "papermill": {
     "duration": 0.003866,
     "end_time": "2025-10-07T14:30:04.055546",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.051680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Cross-Validation Setup\n",
    "Create time-based rolling validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12234701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:04.064115Z",
     "iopub.status.busy": "2025-10-07T14:30:04.063714Z",
     "iopub.status.idle": "2025-10-07T14:30:04.070001Z",
     "shell.execute_reply": "2025-10-07T14:30:04.069161Z"
    },
    "papermill": {
     "duration": 0.011777,
     "end_time": "2025-10-07T14:30:04.071194",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.059417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time-based cross-validation folds...\n",
      "Fold 0: Train months 11, Val months 11\n",
      "Fold 1: Train months 22, Val months 11\n",
      "Fold 2: Train months 33, Val months 11\n",
      "Fold 3: Train months 44, Val months 11\n",
      "Fold 4: Train months 55, Val months 11\n",
      "\n",
      "✓ Created 5 validation folds\n",
      "\n",
      "Fold Details:\n",
      "  Fold 0: Train=2019-Apr to 2019-Oct, Val=2019-Sep to 2020-Nov\n",
      "  Fold 1: Train=2019-Apr to 2020-Nov, Val=2020-Oct to 2021-May\n",
      "  Fold 2: Train=2019-Apr to 2021-May, Val=2021-Nov to 2022-Mar\n",
      "  Fold 3: Train=2019-Apr to 2022-Mar, Val=2022-May to 2023-Jun\n",
      "  Fold 4: Train=2019-Apr to 2023-Jun, Val=2023-Mar to 2024-Mar\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating time-based cross-validation folds...\")\n",
    "\n",
    "# Get all unique months sorted\n",
    "all_months = months\n",
    "n_months = len(all_months)\n",
    "\n",
    "# Create rolling validation folds\n",
    "N_FOLDS = 5\n",
    "folds = []\n",
    "\n",
    "for i in range(N_FOLDS):\n",
    "    # Progressive training window\n",
    "    train_end = int((i + 1) * n_months / (N_FOLDS + 1))\n",
    "    val_start = train_end\n",
    "    val_end = min(train_end + max(1, n_months // (N_FOLDS + 1)), n_months)\n",
    "    \n",
    "    # Skip if insufficient data\n",
    "    if train_end == 0 or val_start >= val_end: \n",
    "        continue\n",
    "    \n",
    "    train_months = all_months[:train_end]\n",
    "    val_months = all_months[val_start:val_end]\n",
    "    \n",
    "    folds.append((train_months, val_months))\n",
    "    \n",
    "    print(f\"Fold {i}: Train months {len(train_months)}, Val months {len(val_months)}\")\n",
    "\n",
    "print(f\"\\n✓ Created {len(folds)} validation folds\")\n",
    "\n",
    "# Display fold details\n",
    "print(\"\\nFold Details:\")\n",
    "for i, (tr, val) in enumerate(folds):\n",
    "    print(f\"  Fold {i}: Train={tr[0]} to {tr[-1]}, Val={val[0]} to {val[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dcb13d",
   "metadata": {
    "papermill": {
     "duration": 0.003722,
     "end_time": "2025-10-07T14:30:04.079038",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.075316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Model Training Function\n",
    "Train multiple models (LGB/XGB/CAT) with rolling CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01570bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:04.087955Z",
     "iopub.status.busy": "2025-10-07T14:30:04.087771Z",
     "iopub.status.idle": "2025-10-07T14:30:04.103491Z",
     "shell.execute_reply": "2025-10-07T14:30:04.102791Z"
    },
    "papermill": {
     "duration": 0.021678,
     "end_time": "2025-10-07T14:30:04.104590",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.082912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_models_target(df, features, target_col, target_log=True, \n",
    "                       use_lgb=LGB, use_xgb=XGB, use_cat=CAT):\n",
    "    \"\"\"\n",
    "    Train multiple models with rolling CV\n",
    "    Returns: oof_preds, preds_stack, weights, pred_test_wgme, pred_test_arith, model_info\n",
    "    \"\"\"\n",
    "    X = df[features].fillna(0)\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Log transform target if requested\n",
    "    if target_log:\n",
    "        y_log = np.log1p(np.clip(y, 0, None))\n",
    "    else:\n",
    "        y_log = y\n",
    "    \n",
    "    # Initialize storage\n",
    "    oof_preds = np.zeros(len(df))\n",
    "    test_preds_per_model = []\n",
    "    model_info = []\n",
    "    model_scores = []\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_df = test.copy()\n",
    "    if \"sector\" not in test_df.columns:\n",
    "        test_df[\"sector\"] = test_df[\"id\"].str.extract(r\"sector\\s*(\\d+)\").astype(float)\n",
    "    if \"month\" not in test_df.columns:\n",
    "        test_df[\"month\"] = test_df[\"id\"].str.extract(r\"(\\d{4}\\s+\\w+)\")\n",
    "    \n",
    "    test_df[\"month_str\"] = test_df[\"month\"].astype(str)\n",
    "    test_df[\"month_code\"] = test_df[\"month_str\"].map(mo2i).fillna(-1).astype(int)\n",
    "    test_df[\"sector_code\"] = pd.factorize(test_df[\"sector\"].astype(str))[0]\n",
    "    \n",
    "    # Ensure test has all features\n",
    "    test_X_all = test_df.copy()\n",
    "    for c in features:\n",
    "        if c not in test_X_all.columns:\n",
    "            test_X_all[c] = 0.0\n",
    "    X_test = test_X_all[features].fillna(0)\n",
    "    \n",
    "    # Train on each fold\n",
    "    for fold_idx, (tr_months, vl_months) in enumerate(folds):\n",
    "        print(f\"Fold {fold_idx} | train months {len(tr_months)}, val months {len(vl_months)}\")\n",
    "        \n",
    "        tr_mask = df[\"month_str\"].isin(tr_months)\n",
    "        vl_mask = df[\"month_str\"].isin(vl_months)\n",
    "        \n",
    "        if tr_mask.sum() < 50 or vl_mask.sum() < 1:\n",
    "            print(\"  Skipping fold due to too small data\")\n",
    "            continue\n",
    "        \n",
    "        X_tr, X_val = X.loc[tr_mask], X.loc[vl_mask]\n",
    "        y_tr_log, y_val_log = y_log[tr_mask], y_log[vl_mask]\n",
    "        y_val_orig = y[vl_mask]\n",
    "        \n",
    "        # Train LightGBM\n",
    "        if use_lgb:\n",
    "            lgb_params = {\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"mae\",\n",
    "                \"learning_rate\": 0.02,\n",
    "                \"num_leaves\": 128,\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"verbosity\": -1,\n",
    "                \"seed\": SEED\n",
    "            }\n",
    "            dtr = lgb.Dataset(X_tr, label=y_tr_log)\n",
    "            dval = lgb.Dataset(X_val, label=y_val_log, reference=dtr)\n",
    "            bst = lgb.train(\n",
    "                    lgb_params,\n",
    "                    dtr,\n",
    "                    num_boost_round=1500,\n",
    "                    valid_sets=[dval],\n",
    "                    callbacks=[\n",
    "                                lgb.early_stopping(stopping_rounds=80),\n",
    "                                lgb.log_evaluation(period=0)  # disable per-iteration logging\n",
    "                            ]\n",
    "                    )\n",
    "\n",
    "            p_val_log = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "            p_val = np.expm1(np.clip(p_val_log, -20, 50))\n",
    "            p_test = np.expm1(np.clip(bst.predict(X_test, num_iteration=bst.best_iteration), -20, 50))\n",
    "            \n",
    "            oof_preds[vl_mask] = p_val\n",
    "            test_preds_per_model.append(p_test)\n",
    "            model_info.append((\"lgb_fold\", fold_idx, bst))\n",
    "            sc = two_stage_score(y_val_orig, p_val)\n",
    "            model_scores.append((\"lgb_fold\", fold_idx, sc))\n",
    "            print(f\"  LGB fold {fold_idx} score: {sc:.4f}\")\n",
    "        \n",
    "        # Train XGBoost\n",
    "        if use_xgb:\n",
    "            try:\n",
    "                dtr_x = xgb.DMatrix(X_tr, label=y_tr_log)\n",
    "                dval_x = xgb.DMatrix(X_val, label=y_val_log)\n",
    "                xgb_params = {\n",
    "                    \"objective\": \"reg:squarederror\",\n",
    "                    \"eta\": 0.02,\n",
    "                    \"max_depth\": 7,\n",
    "                    \"subsample\": 0.8,\n",
    "                    \"colsample_bytree\": 0.8,\n",
    "                    \"seed\": SEED\n",
    "                }\n",
    "                xbst = xgb.train(\n",
    "                    xgb_params, dtr_x, num_boost_round=1200,\n",
    "                    evals=[(dval_x, \"val\")], \n",
    "                    early_stopping_rounds=80, \n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                p_val_log = xbst.predict(xgb.DMatrix(X_val))\n",
    "                p_val = np.expm1(np.clip(p_val_log, -20, 50))\n",
    "                p_test = np.expm1(np.clip(xbst.predict(xgb.DMatrix(X_test)), -20, 50))\n",
    "                \n",
    "                oof_preds[vl_mask] = (oof_preds[vl_mask] + p_val) / 2.0\n",
    "                test_preds_per_model.append(p_test)\n",
    "                model_info.append((\"xgb_fold\", fold_idx, xbst))\n",
    "                sc = two_stage_score(y_val_orig, p_val)\n",
    "                model_scores.append((\"xgb_fold\", fold_idx, sc))\n",
    "                print(f\"  XGB fold {fold_idx} score: {sc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  XGB skipped (error): {e}\")\n",
    "        \n",
    "        # Train CatBoost\n",
    "        if use_cat:\n",
    "            try:\n",
    "                cat = CatBoostRegressor(verbose=0, random_seed=SEED)\n",
    "                cat.fit(X_tr, y_tr_log)\n",
    "                p_val_log = cat.predict(X_val)\n",
    "                p_val = np.expm1(np.clip(p_val_log, -20, 50))\n",
    "                p_test = np.expm1(np.clip(cat.predict(X_test), -20, 50))\n",
    "                \n",
    "                oof_preds[vl_mask] = (oof_preds[vl_mask] + p_val) / 2.0\n",
    "                test_preds_per_model.append(p_test)\n",
    "                model_info.append((\"cat_fold\", fold_idx, cat))\n",
    "                sc = two_stage_score(y_val_orig, p_val)\n",
    "                model_scores.append((\"cat_fold\", fold_idx, sc))\n",
    "                print(f\"  CatBoost fold {fold_idx} score: {sc:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  CatBoost skipped (error): {e}\")\n",
    "    \n",
    "    # Aggregate test predictions\n",
    "    if len(test_preds_per_model) == 0:\n",
    "        raise RuntimeError(f\"No models produced test predictions for target {target_col}\")\n",
    "    \n",
    "    preds_stack = np.vstack(test_preds_per_model)\n",
    "    \n",
    "    # Compute weights based on CV scores\n",
    "    model_scores_arr = np.array([s for (_, _, s) in model_scores], dtype=float)\n",
    "    raw_weights = np.clip(model_scores_arr, 0.0, None)\n",
    "    \n",
    "    if raw_weights.sum() == 0:\n",
    "        raw_weights = np.ones_like(raw_weights)\n",
    "    \n",
    "    # Apply weight decay\n",
    "    decay = 0.98\n",
    "    decay_factors = np.array([decay**i for i in range(len(raw_weights))])\n",
    "    raw_weights = raw_weights * decay_factors\n",
    "    weights = raw_weights / raw_weights.sum()\n",
    "    \n",
    "    # Compute weighted geometric mean\n",
    "    pred_test_wgme = wgeom(preds_stack, weights)\n",
    "    pred_test_arith = preds_stack.mean(axis=0)\n",
    "    \n",
    "    return oof_preds, preds_stack, weights, pred_test_wgme, pred_test_arith, model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a1028",
   "metadata": {
    "papermill": {
     "duration": 0.003782,
     "end_time": "2025-10-07T14:30:04.112296",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.108514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Train Models\n",
    "Train separate models for price, area, and direct amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2039c3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:30:04.120694Z",
     "iopub.status.busy": "2025-10-07T14:30:04.120516Z",
     "iopub.status.idle": "2025-10-07T14:37:17.685353Z",
     "shell.execute_reply": "2025-10-07T14:37:17.684314Z"
    },
    "papermill": {
     "duration": 433.570542,
     "end_time": "2025-10-07T14:37:17.686598",
     "exception": false,
     "start_time": "2025-10-07T14:30:04.116056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING MODELS\n",
      "============================================================\n",
      "✓ Will train price and area models separately\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training PRICE model...\n",
      "------------------------------------------------------------\n",
      "Fold 0 | train months 11, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[839]\tvalid_0's l1: 0.015153\n",
      "  LGB fold 0 score: 0.9839\n",
      "  XGB fold 0 score: 0.9822\n",
      "  CatBoost fold 0 score: 0.9680\n",
      "Fold 1 | train months 22, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[919]\tvalid_0's l1: 0.0162717\n",
      "  LGB fold 1 score: 0.9839\n",
      "  XGB fold 1 score: 0.9875\n",
      "  CatBoost fold 1 score: 0.9826\n",
      "Fold 2 | train months 33, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1458]\tvalid_0's l1: 0.0184729\n",
      "  LGB fold 2 score: 0.9821\n",
      "  XGB fold 2 score: 0.9874\n",
      "  CatBoost fold 2 score: 0.9845\n",
      "Fold 3 | train months 44, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[503]\tvalid_0's l1: 0.0141943\n",
      "  LGB fold 3 score: 0.9859\n",
      "  XGB fold 3 score: 0.9890\n",
      "  CatBoost fold 3 score: 0.9884\n",
      "Fold 4 | train months 55, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[491]\tvalid_0's l1: 0.0111308\n",
      "  LGB fold 4 score: 0.9889\n",
      "  XGB fold 4 score: 0.9925\n",
      "  CatBoost fold 4 score: 0.9902\n",
      "✓ Price model done. Models trained: 15\n",
      "  Weights: [0.076 0.075 0.072 0.072 0.071 0.069 0.068 0.067 0.065 0.064 0.063 0.061\n",
      " 0.06  0.059 0.058]\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training AREA model...\n",
      "------------------------------------------------------------\n",
      "Fold 0 | train months 11, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[330]\tvalid_0's l1: 0.0315773\n",
      "  LGB fold 0 score: 0.9696\n",
      "  XGB fold 0 score: 0.9778\n",
      "  CatBoost fold 0 score: 0.9210\n",
      "Fold 1 | train months 22, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid_0's l1: 0.0225732\n",
      "  LGB fold 1 score: 0.9778\n",
      "  XGB fold 1 score: 0.9841\n",
      "  CatBoost fold 1 score: 0.9561\n",
      "Fold 2 | train months 33, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[899]\tvalid_0's l1: 0.0153613\n",
      "  LGB fold 2 score: 0.9847\n",
      "  XGB fold 2 score: 0.9888\n",
      "  CatBoost fold 2 score: 0.9661\n",
      "Fold 3 | train months 44, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[591]\tvalid_0's l1: 0.0130125\n",
      "  LGB fold 3 score: 0.9870\n",
      "  XGB fold 3 score: 0.9900\n",
      "  CatBoost fold 3 score: 0.9741\n",
      "Fold 4 | train months 55, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[448]\tvalid_0's l1: 0.0116788\n",
      "  LGB fold 4 score: 0.9883\n",
      "  XGB fold 4 score: 0.9910\n",
      "  CatBoost fold 4 score: 0.9782\n",
      "✓ Area model done. Models trained: 15\n",
      "  Weights: [0.076 0.075 0.069 0.072 0.071 0.068 0.068 0.067 0.064 0.065 0.063 0.061\n",
      " 0.061 0.06  0.058]\n",
      "\n",
      "------------------------------------------------------------\n",
      "Training DIRECT AMOUNT model...\n",
      "------------------------------------------------------------\n",
      "Fold 0 | train months 11, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1500]\tvalid_0's l1: 0.168584\n",
      "  LGB fold 0 score: 0.8545\n",
      "  XGB fold 0 score: 0.8443\n",
      "  CatBoost fold 0 score: 0.8493\n",
      "Fold 1 | train months 22, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[562]\tvalid_0's l1: 0.0786839\n",
      "  LGB fold 1 score: 0.9219\n",
      "  XGB fold 1 score: 0.9250\n",
      "  CatBoost fold 1 score: 0.9239\n",
      "Fold 2 | train months 33, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[809]\tvalid_0's l1: 0.062959\n",
      "  LGB fold 2 score: 0.9385\n",
      "  XGB fold 2 score: 0.9420\n",
      "  CatBoost fold 2 score: 0.9467\n",
      "Fold 3 | train months 44, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[791]\tvalid_0's l1: 0.0549144\n",
      "  LGB fold 3 score: 0.9463\n",
      "  XGB fold 3 score: 0.9479\n",
      "  CatBoost fold 3 score: 0.9493\n",
      "Fold 4 | train months 55, val months 11\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[880]\tvalid_0's l1: 0.0511121\n",
      "  LGB fold 4 score: 0.9491\n",
      "  XGB fold 4 score: 0.9539\n",
      "  CatBoost fold 4 score: 0.9601\n",
      "✓ Direct amount model done. Models trained: 15\n",
      "  Weights: [0.071 0.069 0.068 0.072 0.071 0.069 0.069 0.068 0.067 0.066 0.064 0.063\n",
      " 0.062 0.061 0.06 ]\n",
      "\n",
      "============================================================\n",
      "ALL MODELS TRAINED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if we can do price x area approach\n",
    "target_price_col = \"price_new_house_transactions\"\n",
    "target_area_col = \"area_new_house_transactions\"\n",
    "\n",
    "do_price_area = (target_price_col in m.columns and target_area_col in m.columns)\n",
    "\n",
    "if do_price_area:\n",
    "    print(f\"✓ Will train price and area models separately\")\n",
    "else:\n",
    "    print(f\"⚠ Missing price/area columns, using direct amount model only\")\n",
    "\n",
    "# ===== Train Price Model =====\n",
    "if do_price_area:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Training PRICE model...\")\n",
    "    print(\"-\"*60)\n",
    "    (oof_price, preds_stack_price, weights_price, \n",
    "     pred_price_wgme, pred_price_arith, info_price) = train_models_target(\n",
    "        m, features, target_price_col, target_log=True\n",
    "    )\n",
    "    print(f\"✓ Price model done. Models trained: {preds_stack_price.shape[0]}\")\n",
    "    print(f\"  Weights: {weights_price.round(3)}\")\n",
    "else:\n",
    "    preds_stack_price = None\n",
    "    pred_price_wgme = None\n",
    "\n",
    "# ===== Train Area Model =====\n",
    "if do_price_area:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Training AREA model...\")\n",
    "    print(\"-\"*60)\n",
    "    (oof_area, preds_stack_area, weights_area, \n",
    "     pred_area_wgme, pred_area_arith, info_area) = train_models_target(\n",
    "        m, features, target_area_col, target_log=True\n",
    "    )\n",
    "    print(f\"✓ Area model done. Models trained: {preds_stack_area.shape[0]}\")\n",
    "    print(f\"  Weights: {weights_area.round(3)}\")\n",
    "else:\n",
    "    preds_stack_area = None\n",
    "    pred_area_wgme = None\n",
    "\n",
    "# ===== Train Direct Amount Model =====\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Training DIRECT AMOUNT model...\")\n",
    "print(\"-\"*60)\n",
    "(oof_amt, preds_stack_amt, weights_amt, \n",
    " pred_amt_wgme, pred_amt_arith, info_amt) = train_models_target(\n",
    "    m, features, \"amount_new_house_transactions\", target_log=True\n",
    ")\n",
    "print(f\"✓ Direct amount model done. Models trained: {preds_stack_amt.shape[0]}\")\n",
    "print(f\"  Weights: {weights_amt.round(3)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL MODELS TRAINED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b35aa8",
   "metadata": {
    "papermill": {
     "duration": 0.005852,
     "end_time": "2025-10-07T14:37:17.698915",
     "exception": false,
     "start_time": "2025-10-07T14:37:17.693063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Combine Predictions\n",
    "\n",
    "Create multiple prediction candidates and compute meta-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bd33c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:37:17.711774Z",
     "iopub.status.busy": "2025-10-07T14:37:17.711548Z",
     "iopub.status.idle": "2025-10-07T14:37:17.725307Z",
     "shell.execute_reply": "2025-10-07T14:37:17.724454Z"
    },
    "papermill": {
     "duration": 0.02166,
     "end_time": "2025-10-07T14:37:17.726502",
     "exception": false,
     "start_time": "2025-10-07T14:37:17.704842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMBINING PREDICTIONS\n",
      "============================================================\n",
      "✓ Created candidate: price×area WGME\n",
      "✓ Created candidate: direct amount WGME\n",
      "✓ Created candidate: arithmetic blend (50/50)\n",
      "✓ Created candidate: arithmetic mean of all amount models\n",
      "\n",
      "Total candidates: 4\n",
      "\n",
      "------------------------------------------------------------\n",
      "Computing meta-weights based on prediction stability...\n",
      "------------------------------------------------------------\n",
      "  pricexarea_wgme:\n",
      "    Mean: 720,955.85\n",
      "    Std: 7,609.27\n",
      "    Q1: 711,189.87, Q99: 736,353.76\n",
      "    Stability: 0.4913\n",
      "  direct_amt_wgme:\n",
      "    Mean: 103.58\n",
      "    Std: 0.45\n",
      "    Q1: 103.14, Q99: 105.00\n",
      "    Stability: 0.4955\n",
      "  blend_arith_avg:\n",
      "    Mean: 360,529.72\n",
      "    Std: 3,804.85\n",
      "    Q1: 355,646.55, Q99: 368,229.38\n",
      "    Stability: 0.4913\n",
      "  amt_arith_models_mean:\n",
      "    Mean: 108.19\n",
      "    Std: 0.50\n",
      "    Q1: 107.75, Q99: 109.86\n",
      "    Stability: 0.4952\n",
      "\n",
      "------------------------------------------------------------\n",
      "Meta-weights (based on stability):\n",
      "  pricexarea_wgme: 0.2490\n",
      "  direct_amt_wgme: 0.2511\n",
      "  blend_arith_avg: 0.2490\n",
      "  amt_arith_models_mean: 0.2509\n",
      "------------------------------------------------------------\n",
      "\n",
      "✓ Final ensemble prediction created\n",
      "  Mean: 269,318.05\n",
      "  Median: 269,118.66\n",
      "  Min: 265,670.57\n",
      "  Max: 277,075.65\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMBINING PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create candidate predictions\n",
    "candidates = {}\n",
    "\n",
    "# Candidate 1: Price x Area (if available)\n",
    "if do_price_area and pred_price_wgme is not None and pred_area_wgme is not None:\n",
    "    pred_amount_from_price_area = pred_price_wgme * pred_area_wgme\n",
    "    candidates[\"pricexarea_wgme\"] = pred_amount_from_price_area\n",
    "    print(f\"✓ Created candidate: price×area WGME\")\n",
    "\n",
    "# Candidate 2: Direct amount WGME\n",
    "candidates[\"direct_amt_wgme\"] = pred_amt_wgme\n",
    "print(f\"✓ Created candidate: direct amount WGME\")\n",
    "\n",
    "# Candidate 3: Blend of price×area and direct amount\n",
    "if \"pricexarea_wgme\" in candidates:\n",
    "    candidates[\"blend_arith_avg\"] = 0.5 * pred_amt_wgme + 0.5 * pred_amount_from_price_area\n",
    "    print(f\"✓ Created candidate: arithmetic blend (50/50)\")\n",
    "\n",
    "# Candidate 4: Simple arithmetic mean across all amount models\n",
    "candidates[\"amt_arith_models_mean\"] = preds_stack_amt.mean(axis=0)\n",
    "print(f\"✓ Created candidate: arithmetic mean of all amount models\")\n",
    "\n",
    "print(f\"\\nTotal candidates: {len(candidates)}\")\n",
    "\n",
    "# ===== Compute Meta-Weights Based on Stability =====\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Computing meta-weights based on prediction stability...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "cand_names = list(candidates.keys())\n",
    "cand_stability = {}\n",
    "\n",
    "for k, v in candidates.items():\n",
    "    q1, q99 = np.nanpercentile(v, [1, 99])\n",
    "    # Higher stability score = more stable predictions\n",
    "    stability = 1.0 / (1.0 + (q99 / (q1 + 1e-9)))\n",
    "    cand_stability[k] = stability\n",
    "    \n",
    "    mean_val = np.nanmean(v)\n",
    "    std_val = np.nanstd(v)\n",
    "    print(f\"  {k}:\")\n",
    "    print(f\"    Mean: {mean_val:,.2f}\")\n",
    "    print(f\"    Std: {std_val:,.2f}\")\n",
    "    print(f\"    Q1: {q1:,.2f}, Q99: {q99:,.2f}\")\n",
    "    print(f\"    Stability: {stability:.4f}\")\n",
    "\n",
    "# Normalize stability scores into weights\n",
    "raw_meta = np.array([cand_stability[k] for k in cand_names], dtype=float)\n",
    "if raw_meta.sum() == 0:\n",
    "    raw_meta = np.ones_like(raw_meta)\n",
    "\n",
    "meta_weights_arr = raw_meta / raw_meta.sum()\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Meta-weights (based on stability):\")\n",
    "for name, weight in zip(cand_names, meta_weights_arr):\n",
    "    print(f\"  {name}: {weight:.4f}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# ===== Create Final Ensemble Prediction =====\n",
    "final_test_pred = np.zeros_like(list(candidates.values())[0])\n",
    "for i, k in enumerate(cand_names):\n",
    "    final_test_pred += meta_weights_arr[i] * candidates[k]\n",
    "\n",
    "print(f\"\\n✓ Final ensemble prediction created\")\n",
    "print(f\"  Mean: {np.mean(final_test_pred):,.2f}\")\n",
    "print(f\"  Median: {np.median(final_test_pred):,.2f}\")\n",
    "print(f\"  Min: {np.min(final_test_pred):,.2f}\")\n",
    "print(f\"  Max: {np.max(final_test_pred):,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba70e96",
   "metadata": {
    "papermill": {
     "duration": 0.006202,
     "end_time": "2025-10-07T14:37:17.739112",
     "exception": false,
     "start_time": "2025-10-07T14:37:17.732910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Post-Processing\n",
    "Apply multiple refinement steps to improve predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f033b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:37:17.752349Z",
     "iopub.status.busy": "2025-10-07T14:37:17.752108Z",
     "iopub.status.idle": "2025-10-07T14:37:17.804660Z",
     "shell.execute_reply": "2025-10-07T14:37:17.803786Z"
    },
    "papermill": {
     "duration": 0.060843,
     "end_time": "2025-10-07T14:37:17.806009",
     "exception": false,
     "start_time": "2025-10-07T14:37:17.745166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "POST-PROCESSING\n",
      "============================================================\n",
      "Initial predictions - Mean: 269,318.05\n",
      "\n",
      "✓ Step 1: Scaling down by 10,000 (detected large mean: 269,318)\n",
      "✓ Step 2: Floored 0 tiny predictions to 0\n",
      "✓ Step 3: Applied sector median fallback to 0 rows\n",
      "✓ Step 4: Applied seasonality bump to 0 rows (factor: 0.05)\n",
      "✓ Step 5: Applied 3-month centered rolling smoothing per sector\n",
      "✓ Step 6: Clipped 0 outliers (Q1=26.57, Q99=27.51)\n",
      "✓ Step 7: Applied final safety floor to 0 rows\n",
      "\n",
      "------------------------------------------------------------\n",
      "Final prediction statistics:\n",
      "  Mean: 26.93\n",
      "  Median: 26.91\n",
      "  Std: 0.28\n",
      "  Min: 26.57\n",
      "  Max: 27.71\n",
      "  Zeros: 0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"POST-PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create submission dataframe\n",
    "test_ids = test[\"id\"].values\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test_ids, \n",
    "    \"new_house_transaction_amount\": final_test_pred\n",
    "})\n",
    "\n",
    "print(f\"Initial predictions - Mean: {sub['new_house_transaction_amount'].mean():,.2f}\")\n",
    "\n",
    "# ===== Step 1: Unit Check and Scaling =====\n",
    "mean_pred = sub[\"new_house_transaction_amount\"].mean()\n",
    "if mean_pred > 1e5:\n",
    "    print(f\"\\n✓ Step 1: Scaling down by 10,000 (detected large mean: {mean_pred:,.0f})\")\n",
    "    sub[\"new_house_transaction_amount\"] /= 10000.0\n",
    "else:\n",
    "    print(f\"\\n✓ Step 1: No scaling needed (mean: {mean_pred:,.2f})\")\n",
    "\n",
    "# ===== Step 2: Floor Tiny Predictions =====\n",
    "tiny_mask = sub[\"new_house_transaction_amount\"] < 1.0\n",
    "tiny_count = tiny_mask.sum()\n",
    "sub.loc[tiny_mask, \"new_house_transaction_amount\"] = 0.0\n",
    "print(f\"✓ Step 2: Floored {tiny_count} tiny predictions to 0\")\n",
    "\n",
    "# ===== Step 3: Sector Median Fallback =====\n",
    "if \"amount_new_house_transactions\" in m.columns:\n",
    "    sector_median = m.groupby(\"sector\")[\"amount_new_house_transactions\"].median().to_dict()\n",
    "    sub[\"sector\"] = sub[\"id\"].str.extract(r\"sector\\s*(\\d+)\")[0].astype(float)\n",
    "    sub[\"sector_median\"] = sub[\"sector\"].map(sector_median)\n",
    "    \n",
    "    fallback_mask = (sub[\"new_house_transaction_amount\"] == 0.0) & (sub[\"sector_median\"].notna())\n",
    "    fallback_count = fallback_mask.sum()\n",
    "    sub.loc[fallback_mask, \"new_house_transaction_amount\"] = sub.loc[fallback_mask, \"sector_median\"] * 0.8\n",
    "    print(f\"✓ Step 3: Applied sector median fallback to {fallback_count} rows\")\n",
    "else:\n",
    "    print(f\"⚠ Step 3: Skipped (no sector median available)\")\n",
    "\n",
    "# ===== Step 4: Seasonality Bump =====\n",
    "last_vals = m.sort_values([\"sector\", \"month_str\"]).groupby(\"sector\")[\"amount_new_house_transactions\"].last().to_dict()\n",
    "sub[\"last_val\"] = sub[\"sector\"].map(last_vals)\n",
    "\n",
    "seasonal_bump_factor = 0.05\n",
    "mask_bump = sub[\"last_val\"].notna()\n",
    "bump_count = mask_bump.sum()\n",
    "\n",
    "sub.loc[mask_bump, \"new_house_transaction_amount\"] = (\n",
    "    sub.loc[mask_bump, \"new_house_transaction_amount\"] * (1 + seasonal_bump_factor) + \n",
    "    sub.loc[mask_bump, \"last_val\"] * seasonal_bump_factor\n",
    ")\n",
    "print(f\"✓ Step 4: Applied seasonality bump to {bump_count} rows (factor: {seasonal_bump_factor})\")\n",
    "\n",
    "# ===== Step 5: Sector-Level Smoothing =====\n",
    "sub[\"month\"] = sub[\"id\"].str.extract(r\"(\\d{4}\\s+\\w+)\")[0]\n",
    "sub[\"sector_int\"] = sub[\"sector\"].astype(int)\n",
    "sub = sub.sort_values([\"sector_int\", \"month\"])\n",
    "\n",
    "sub[\"smooth3\"] = sub.groupby(\"sector_int\")[\"new_house_transaction_amount\"].transform(\n",
    "    lambda s: s.rolling(3, min_periods=1, center=True).mean()\n",
    ")\n",
    "sub[\"new_house_transaction_amount\"] = np.clip(sub[\"smooth3\"], 0, None)\n",
    "print(f\"✓ Step 5: Applied 3-month centered rolling smoothing per sector\")\n",
    "\n",
    "# ===== Step 6: Quantile-Based Clipping =====\n",
    "q1, q99 = np.nanpercentile(sub[\"new_house_transaction_amount\"].clip(0), [1, 99])\n",
    "before_clip = sub[\"new_house_transaction_amount\"].copy()\n",
    "sub[\"new_house_transaction_amount\"] = sub[\"new_house_transaction_amount\"].clip(\n",
    "    lower=q1 * 0.5, \n",
    "    upper=q99 * 1.5\n",
    ")\n",
    "clip_count = (before_clip != sub[\"new_house_transaction_amount\"]).sum()\n",
    "print(f\"✓ Step 6: Clipped {clip_count} outliers (Q1={q1:.2f}, Q99={q99:.2f})\")\n",
    "\n",
    "# ===== Step 7: Final Safety Floor =====\n",
    "final_floor_mask = sub[\"new_house_transaction_amount\"] < 1.0\n",
    "final_floor_count = final_floor_mask.sum()\n",
    "sub.loc[final_floor_mask, \"new_house_transaction_amount\"] = 0.0\n",
    "print(f\"✓ Step 7: Applied final safety floor to {final_floor_count} rows\")\n",
    "\n",
    "# Final statistics\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Final prediction statistics:\")\n",
    "print(f\"  Mean: {sub['new_house_transaction_amount'].mean():,.2f}\")\n",
    "print(f\"  Median: {sub['new_house_transaction_amount'].median():,.2f}\")\n",
    "print(f\"  Std: {sub['new_house_transaction_amount'].std():,.2f}\")\n",
    "print(f\"  Min: {sub['new_house_transaction_amount'].min():,.2f}\")\n",
    "print(f\"  Max: {sub['new_house_transaction_amount'].max():,.2f}\")\n",
    "print(f\"  Zeros: {(sub['new_house_transaction_amount'] == 0).sum()}\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892d9ef",
   "metadata": {
    "papermill": {
     "duration": 0.006011,
     "end_time": "2025-10-07T14:37:17.818611",
     "exception": false,
     "start_time": "2025-10-07T14:37:17.812600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 12. Save Results and Diagonstics\n",
    "Create submission file and display OOF performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223bb1c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-07T14:37:17.831613Z",
     "iopub.status.busy": "2025-10-07T14:37:17.831395Z",
     "iopub.status.idle": "2025-10-07T14:37:17.864669Z",
     "shell.execute_reply": "2025-10-07T14:37:17.863938Z"
    },
    "papermill": {
     "duration": 0.041132,
     "end_time": "2025-10-07T14:37:17.865748",
     "exception": false,
     "start_time": "2025-10-07T14:37:17.824616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      "✓ Saved submission to: /kaggle/working/submission_v7_ensemble.csv\n",
      "  Rows: 1152\n",
      "  Columns: ['id', 'new_house_transaction_amount']\n",
      "\n",
      "First 10 rows of submission:\n",
      "                     id  new_house_transaction_amount\n",
      "0     2024 Aug_sector 1                     27.707565\n",
      "384   2024 Dec_sector 1                     27.707565\n",
      "288   2024 Nov_sector 1                     27.707565\n",
      "192   2024 Oct_sector 1                     27.707565\n",
      "96    2024 Sep_sector 1                     27.707565\n",
      "768   2025 Apr_sector 1                     27.707565\n",
      "576   2025 Feb_sector 1                     27.707565\n",
      "480   2025 Jan_sector 1                     27.707565\n",
      "1056  2025 Jul_sector 1                     27.707565\n",
      "960   2025 Jun_sector 1                     27.707565\n",
      "\n",
      "Last 10 rows of submission:\n",
      "                      id  new_house_transaction_amount\n",
      "383   2024 Nov_sector 96                     26.641753\n",
      "287   2024 Oct_sector 96                     26.641753\n",
      "191   2024 Sep_sector 96                     26.641753\n",
      "863   2025 Apr_sector 96                     26.641753\n",
      "671   2025 Feb_sector 96                     26.641753\n",
      "575   2025 Jan_sector 96                     26.641753\n",
      "1151  2025 Jul_sector 96                     26.641753\n",
      "1055  2025 Jun_sector 96                     26.641753\n",
      "767   2025 Mar_sector 96                     26.641753\n",
      "959   2025 May_sector 96                     26.641753\n",
      "\n",
      "============================================================\n",
      "OUT-OF-FOLD DIAGNOSTICS\n",
      "============================================================\n",
      "\n",
      "Direct Amount Model OOF Performance:\n",
      "  Two-Stage Score: 0.770484\n",
      "  MAE: 6,967.64\n",
      "  Fraction with APE > 1.0: 0.0039 (0.39%)\n",
      "  MAPE (on good predictions): 0.228629\n",
      "  Correlation: 0.8772\n",
      "\n",
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "  1. Upload 'submission_v7_ensemble.csv' to Kaggle\n",
      "  2. Check leaderboard score\n",
      "  3. Iterate on features/models if needed\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Keep only required columns\n",
    "final_sub = sub[[\"id\", \"new_house_transaction_amount\"]].copy()\n",
    "\n",
    "# Save submission file\n",
    "out_path = os.path.join(OUT_DIR, \"submission_v7_ensemble.csv\")\n",
    "final_sub.to_csv(out_path, index=False)\n",
    "print(f\"✓ Saved submission to: {out_path}\")\n",
    "print(f\"  Rows: {len(final_sub)}\")\n",
    "print(f\"  Columns: {list(final_sub.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 rows of submission:\")\n",
    "print(final_sub.head(10))\n",
    "\n",
    "print(\"\\nLast 10 rows of submission:\")\n",
    "print(final_sub.tail(10))\n",
    "\n",
    "# ===== OOF Diagnostics =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUT-OF-FOLD DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    if 'oof_amt' in globals() and \"amount_new_house_transactions\" in m.columns:\n",
    "        oof_pred = oof_amt\n",
    "        y_true = m[\"amount_new_house_transactions\"].values\n",
    "        \n",
    "        # Compute two-stage score\n",
    "        oof_score = two_stage_score(y_true, oof_pred)\n",
    "        oof_mae = mean_absolute_error(y_true, oof_pred)\n",
    "        \n",
    "        print(f\"\\nDirect Amount Model OOF Performance:\")\n",
    "        print(f\"  Two-Stage Score: {oof_score:.6f}\")\n",
    "        print(f\"  MAE: {oof_mae:,.2f}\")\n",
    "        \n",
    "        # Additional metrics\n",
    "        ape = np.abs(oof_pred - y_true) / np.maximum(np.abs(y_true), 1e-12)\n",
    "        frac_bad = np.mean(ape > 1.0)\n",
    "        print(f\"  Fraction with APE > 1.0: {frac_bad:.4f} ({frac_bad*100:.2f}%)\")\n",
    "        \n",
    "        if frac_bad <= 0.3:\n",
    "            mape_good = np.mean(ape[ape <= 1.0])\n",
    "            print(f\"  MAPE (on good predictions): {mape_good:.6f}\")\n",
    "        \n",
    "        # Correlation\n",
    "        corr = np.corrcoef(y_true, oof_pred)[0, 1]\n",
    "        print(f\"  Correlation: {corr:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠ OOF predictions not available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not compute OOF diagnostics: {e}\")\n",
    "\n",
    "# ===== Summary =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Upload 'submission_v7_ensemble.csv' to Kaggle\")\n",
    "print(\"  2. Check leaderboard score\")\n",
    "print(\"  3. Iterate on features/models if needed\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13320609,
     "sourceId": 111876,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 446.912975,
   "end_time": "2025-10-07T14:37:18.690407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-07T14:29:51.777432",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
