{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 111876,
     "databundleVersionId": 13320609,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## 1. Setup & Imports",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import os, gc, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import Ridge, RidgeCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GroupKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Try LGB/XGB\n",
    "LGB_AVAILABLE = False\n",
    "XGB_AVAILABLE = False\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.getcwd())\n",
    "INPUT_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "OUT_DIR = os.path.join(BASE_DIR, \"results\", \"advanced_v2_outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"LGB:\", LGB_AVAILABLE, \"XGB:\", XGB_AVAILABLE)\n"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:15:17.620966Z",
     "iopub.execute_input": "2025-10-05T14:15:17.621335Z",
     "iopub.status.idle": "2025-10-05T14:15:17.662079Z",
     "shell.execute_reply.started": "2025-10-05T14:15:17.621311Z",
     "shell.execute_reply": "2025-10-05T14:15:17.661055Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:24.060215Z",
     "start_time": "2025-10-05T14:36:22.912735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB: True XGB: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Competition metric & utilities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Two-stage metric and helper utilities\n\ndef two_stage_score_from_arrays(y_true, y_pred):\n    eps = 1e-12\n    y_true = np.array(y_true, dtype=float)\n    y_pred = np.array(y_pred, dtype=float)\n    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps)\n    frac_bad = np.mean(ape > 1.0)\n    if frac_bad > 0.30:\n        return 0.0\n    good_mask = (ape <= 1.0)\n    if good_mask.sum() == 0:\n        return 0.0\n    mape = np.mean(ape[good_mask])\n    scaled = mape / good_mask.mean()\n    return 1.0 - scaled\n\ndef evaluate_preds(y_true, y_pred, name=\"eval\"):\n    score = two_stage_score_from_arrays(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    frac_bad = np.mean(np.abs(y_pred - y_true) / np.maximum(np.abs(y_true),1e-12) > 1.0)\n    print(f\"{name} | two-stage score: {score:.6f} | MAE: {mae:.4f} | frac_bad: {frac_bad:.4f}\")\n    return {\"score\":score, \"mae\":mae, \"frac_bad\":frac_bad}\n\ndef save_df(df, fname):\n    path = os.path.join(OUT_DIR, fname)\n    df.to_csv(path, index=False)\n    print(\"Saved:\", path)\n    return path\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:15:36.500735Z",
     "iopub.execute_input": "2025-10-05T14:15:36.501886Z",
     "iopub.status.idle": "2025-10-05T14:15:36.510509Z",
     "shell.execute_reply.started": "2025-10-05T14:15:36.501852Z",
     "shell.execute_reply": "2025-10-05T14:15:36.509261Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:28.658825Z",
     "start_time": "2025-10-05T14:36:28.653819Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "## 3. Load base datasets (raw CSVs)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 3: Load base files from INPUT_DIR\n",
    "files_needed = [\n",
    " \"train/new_house_transactions.csv\",\n",
    " \"train/new_house_transactions_nearby_sectors.csv\",\n",
    " \"train/pre_owned_house_transactions.csv\",\n",
    " \"train/pre_owned_house_transactions_nearby_sectors.csv\",\n",
    " \"train/land_transactions.csv\",\n",
    " \"train/land_transactions_nearby_sectors.csv\",\n",
    " \"train/sector_POI.csv\",\n",
    " \"train/city_search_index.csv\",\n",
    " \"train/city_indexes.csv\",\n",
    " \"test.csv\",\n",
    " \"sample_submission.csv\"\n",
    "]\n",
    "\n",
    "data = {}\n",
    "for f in files_needed:\n",
    "    # Adjust path for train/ subdirectory\n",
    "    if f.startswith('train/'):\n",
    "        fp = os.path.join(INPUT_DIR, f)\n",
    "    else:\n",
    "        fp = os.path.join(INPUT_DIR, f)\n",
    "\n",
    "    if os.path.exists(fp):\n",
    "        print(\"Loading\", f)\n",
    "        data[f] = pd.read_csv(fp)\n",
    "    else:\n",
    "        # Fallback for files that might be in train subfolder\n",
    "        fp_alt = os.path.join(INPUT_DIR, 'train', f)\n",
    "        if os.path.exists(fp_alt):\n",
    "            print(\"Loading\", f, \"from train/\")\n",
    "            data[f] = pd.read_csv(fp_alt)\n",
    "        else:\n",
    "            print(\"Missing\", f, \" — check path\")\n",
    "            data[f] = None\n",
    "\n",
    "# quick shapes\n",
    "for k,v in data.items():\n",
    "    if v is None: continue\n",
    "    print(k, v.shape)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:16:04.971632Z",
     "iopub.execute_input": "2025-10-05T14:16:04.972097Z",
     "iopub.status.idle": "2025-10-05T14:16:05.068533Z",
     "shell.execute_reply.started": "2025-10-05T14:16:04.972064Z",
     "shell.execute_reply": "2025-10-05T14:16:05.067409Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:30.910575Z",
     "start_time": "2025-10-05T14:36:30.836644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/new_house_transactions.csv\n",
      "Loading train/new_house_transactions_nearby_sectors.csv\n",
      "Loading train/pre_owned_house_transactions.csv\n",
      "Loading train/pre_owned_house_transactions_nearby_sectors.csv\n",
      "Loading train/land_transactions.csv\n",
      "Loading train/land_transactions_nearby_sectors.csv\n",
      "Loading train/sector_POI.csv\n",
      "Loading train/city_search_index.csv\n",
      "Loading train/city_indexes.csv\n",
      "Loading test.csv\n",
      "Loading sample_submission.csv\n",
      "train/new_house_transactions.csv (5433, 11)\n",
      "train/new_house_transactions_nearby_sectors.csv (5360, 11)\n",
      "train/pre_owned_house_transactions.csv (5360, 6)\n",
      "train/pre_owned_house_transactions_nearby_sectors.csv (5427, 6)\n",
      "train/land_transactions.csv (5896, 6)\n",
      "train/land_transactions_nearby_sectors.csv (5025, 6)\n",
      "train/sector_POI.csv (86, 142)\n",
      "train/city_search_index.csv (4020, 4)\n",
      "train/city_indexes.csv (7, 74)\n",
      "test.csv (1152, 2)\n",
      "sample_submission.csv (1152, 2)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Clean each base dataset (no merging)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Clean base datasets individually and save cleaned copies (no merging)\ncleaned = {}\n\n# Transaction tables: numeric fill 0, strip strings\nfor key in [\"train/new_house_transactions.csv\",\"train/pre_owned_house_transactions.csv\",\"train/land_transactions.csv\",\n            \"train/new_house_transactions_nearby_sectors.csv\",\"train/pre_owned_house_transactions_nearby_sectors.csv\",\"train/land_transactions_nearby_sectors.csv\"]:\n    df = data.get(key)\n    if df is None:\n        cleaned[key] = None\n        continue\n    numcols = df.select_dtypes(include=[np.number]).columns.tolist()\n    df[numcols] = df[numcols].fillna(0)\n    for c in df.select_dtypes(include=['object']).columns:\n        df[c] = df[c].astype(str).str.strip()\n    cleaned[key] = df\n    save_df(df, os.path.basename(key).replace(\".csv\",\"\") + \"_clean.csv\")\n\n# POI: drop >70% missing, fill rest 0\npoi = data.get(\"train/sector_POI.csv\")\nif poi is not None:\n    miss = poi.isna().mean()\n    drop = miss[miss>0.7].index.tolist()\n    poi_clean = poi.drop(columns=drop).fillna(0)\n    cleaned[\"train/sector_POI.csv\"] = poi_clean\n    save_df(poi_clean, \"sector_POI_clean.csv\")\nelse:\n    cleaned[\"train/sector_POI.csv\"] = None\n\n# city_search: aggregate if repeated keywords\ncsi = data.get(\"train/city_search_index.csv\")\nif csi is not None:\n    if \"search_volume\" in csi.columns:\n        csi_agg = csi.groupby(\"month\", as_index=False)[\"search_volume\"].sum().rename(columns={\"search_volume\":\"city_search_volume\"})\n    else:\n        # choose numeric candidate\n        cand = [c for c in csi.columns if c.lower().count(\"search\") or c.lower().count(\"volume\")]\n        if cand:\n            csi_agg = csi.groupby(\"month\", as_index=False)[cand[0]].sum().rename(columns={cand[0]:\"city_search_volume\"})\n        else:\n            csi_agg = csi.copy()\n    cleaned[\"train/city_search_index.csv\"] = csi_agg\n    save_df(csi_agg, \"city_search_index_clean.csv\")\nelse:\n    cleaned[\"train/city_search_index.csv\"] = None\n\n# city_indexes: drop >80% missing, ffill/bfill\nci = data.get(\"train/city_indexes.csv\")\nif ci is not None:\n    missci = ci.isna().mean()\n    dropci = missci[missci>0.8].index.tolist()\n    ci_clean = ci.drop(columns=dropci).fillna(method='ffill').fillna(method='bfill')\n    cleaned[\"train/city_indexes.csv\"] = ci_clean\n    save_df(ci_clean, \"city_indexes_clean.csv\")\nelse:\n    cleaned[\"train/city_indexes.csv\"] = None\n\n# save test and sample submission\nif data.get(\"test.csv\") is not None:\n    save_df(data[\"test.csv\"], \"test_clean.csv\")\nif data.get(\"sample_submission.csv\") is not None:\n    save_df(data[\"sample_submission.csv\"], \"sample_submission_clean.csv\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:16:15.875962Z",
     "iopub.execute_input": "2025-10-05T14:16:15.876309Z",
     "iopub.status.idle": "2025-10-05T14:16:16.145398Z",
     "shell.execute_reply.started": "2025-10-05T14:16:15.876282Z",
     "shell.execute_reply": "2025-10-05T14:16:16.144362Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:32.743268Z",
     "start_time": "2025-10-05T14:36:32.622031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\new_house_transactions_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\pre_owned_house_transactions_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\land_transactions_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\new_house_transactions_nearby_sectors_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\pre_owned_house_transactions_nearby_sectors_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\land_transactions_nearby_sectors_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\sector_POI_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\city_search_index_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\city_indexes_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\test_clean.csv\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\sample_submission_clean.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## 5. Aggregate base transaction tables to month+sector (for modeling)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Aggregate (sum) numeric columns by month & sector per table\ndef agg_month_sector(df):\n    if df is None or 'month' not in df.columns or 'sector' not in df.columns:\n        return None\n    numcols = df.select_dtypes(include=[np.number]).columns.tolist()\n    if not numcols:\n        return None\n    return df.groupby(['month','sector'])[numcols].sum().reset_index()\n\nagg_new = agg_month_sector(cleaned[\"train/new_house_transactions.csv\"])\nagg_pre = agg_month_sector(cleaned[\"train/pre_owned_house_transactions.csv\"])\nagg_land = agg_month_sector(cleaned[\"train/land_transactions.csv\"])\nagg_new_near = agg_month_sector(cleaned[\"train/new_house_transactions_nearby_sectors.csv\"])\nagg_pre_near = agg_month_sector(cleaned[\"train/pre_owned_house_transactions_nearby_sectors.csv\"])\nagg_land_near = agg_month_sector(cleaned[\"train/land_transactions_nearby_sectors.csv\"])\n\nfor name, df in [(\"agg_new\",agg_new),(\"agg_pre\",agg_pre),(\"agg_land\",agg_land),\n                 (\"agg_new_near\",agg_new_near),(\"agg_pre_near\",agg_pre_near),(\"agg_land_near\",agg_land_near)]:\n    if df is not None:\n        save_df(df, f\"{name}.csv\")\n        print(name, df.shape)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:16:32.215987Z",
     "iopub.execute_input": "2025-10-05T14:16:32.216405Z",
     "iopub.status.idle": "2025-10-05T14:16:32.449985Z",
     "shell.execute_reply.started": "2025-10-05T14:16:32.216381Z",
     "shell.execute_reply": "2025-10-05T14:16:32.448824Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:34.533589Z",
     "start_time": "2025-10-05T14:36:34.422700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\agg_new.csv\n",
      "agg_new (5433, 11)\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\agg_pre.csv\n",
      "agg_pre (5360, 6)\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\agg_land.csv\n",
      "agg_land (5896, 6)\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\agg_new_near.csv\n",
      "agg_new_near (5360, 11)\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\agg_pre_near.csv\n",
      "agg_pre_near (5427, 6)\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\agg_land_near.csv\n",
      "agg_land_near (5025, 6)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Build modeling table by merging aggregates + POI + city search + city indexes (but built from base)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 6: Merge aggregates into modeling table (no use of prior merged files)\nif agg_new is None:\n    raise RuntimeError(\"new_house transactions empty - cannot proceed\")\n\nmodel = agg_new.copy()\ndef merge_pref(base, other, pref):\n    if other is None: return base\n    o = other.copy()\n    for c in o.columns:\n        if c not in ['month','sector']:\n            o.rename(columns={c: f\"{pref}_{c}\"}, inplace=True)\n    return base.merge(o, on=['month','sector'], how='left')\n\nmodel = merge_pref(model, agg_pre, 'pre')\nmodel = merge_pref(model, agg_land, 'land')\nmodel = merge_pref(model, agg_new_near, 'new_near')\nmodel = merge_pref(model, agg_pre_near, 'pre_near')\nmodel = merge_pref(model, agg_land_near, 'land_near')\n\n# merge city_search\nif cleaned[\"train/city_search_index.csv\" ] is not None:\n    model = model.merge(cleaned[\"train/city_search_index.csv\"], on='month', how='left')\n\n# merge POI\nif cleaned[\"train/sector_POI.csv\"] is not None:\n    poi = cleaned[\"train/sector_POI.csv\"].copy()\n    # ensure sector types align\n    try:\n        poi['sector'] = poi['sector'].astype(model['sector'].dtype)\n    except:\n        poi['sector'] = poi['sector'].astype(str)\n        model['sector'] = model['sector'].astype(str)\n    model = model.merge(poi, on='sector', how='left')\n\n# merge city index by year if exists\nci = cleaned[\"train/city_indexes.csv\"]\nif ci is not None:\n    if 'city_indicator_data_year' in ci.columns:\n        ci2 = ci.rename(columns={'city_indicator_data_year':'year'})\n        model['year'] = model['month'].astype(str).str[:4].astype(int)\n        keep = ['year'] + [c for c in ci2.select_dtypes(include=[np.number]).columns if c!='year']\n        ci2 = ci2[keep]\n        ci2 = ci2.rename(columns={c: f\"cityidx_{c}\" for c in ci2.columns if c!='year'})\n        model = model.merge(ci2, on='year', how='left')\n\n# fill numeric NaNs with 0\nnumcols = model.select_dtypes(include=[np.number]).columns.tolist()\nmodel[numcols] = model[numcols].fillna(0)\n\nsave_df(model, \"modeling_table_from_base.csv\")\nprint(\"Model table shape:\", model.shape)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:16:43.855829Z",
     "iopub.execute_input": "2025-10-05T14:16:43.856192Z",
     "iopub.status.idle": "2025-10-05T14:16:45.542939Z",
     "shell.execute_reply.started": "2025-10-05T14:16:43.856158Z",
     "shell.execute_reply": "2025-10-05T14:16:45.541798Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:37.007271Z",
     "start_time": "2025-10-05T14:36:36.283556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\modeling_table_from_base.csv\n",
      "Model table shape: (6419, 251)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Feature engineering (lags, rolling, ratios) using the modeling table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 7: Feature engineering\nm = model.sort_values(['sector','month']).reset_index(drop=True)\n# month extraction\nm['month_str'] = m['month'].astype(str)\nm['month_num'] = m['month_str'].str.extract(r'-(\\d{2})$')[0].fillna('0').astype(int)\nm['season'] = ((m['month_num']%12)//3)+1\n\n# key columns to create lags for\nkey_cols = [c for c in ['num_new_house_transactions','area_new_house_transactions','price_new_house_transactions','amount_new_house_transactions'] if c in m.columns]\nfor col in key_cols:\n    for lag in [1,3,6]:\n        m[f\"{col}_lag{lag}\"] = m.groupby('sector')[col].shift(lag).fillna(0)\n    m[f\"{col}_roll3\"] = m.groupby('sector')[col].rolling(3, min_periods=1).mean().reset_index(level=0, drop=True)\n\n# supply-demand ratio\nif 'num_new_house_available_for_sale' in m.columns and 'num_new_house_transactions' in m.columns:\n    m['supply_demand_ratio'] = m['num_new_house_available_for_sale'] / (m['num_new_house_transactions'] + 1)\n    m['supply_demand_ratio'] = m['supply_demand_ratio'].replace(np.inf,0).fillna(0)\n\n# log transforms for heavy-tail columns\nnum_cols = m.select_dtypes(include=[np.number]).columns.tolist()\nfor c in num_cols:\n    if c in ['month_num','year']: continue\n    m[f\"{c}_log1p\"] = np.log1p(m[c].clip(lower=0))\n\n# log target\nif 'amount_new_house_transactions' not in m.columns:\n    # attempt to build from num * price\n    if 'num_new_house_transactions' in m.columns and 'price_new_house_transactions' in m.columns:\n        m['amount_new_house_transactions'] = m['num_new_house_transactions'] * m['price_new_house_transactions']\n    else:\n        m['amount_new_house_transactions'] = 0\nm['y_log1p'] = np.log1p(m['amount_new_house_transactions'].clip(lower=0))\n\nsave_df(m, \"modeling_table_engineered_from_base.csv\")\nprint(\"Engineered shape:\", m.shape)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:16:58.746629Z",
     "iopub.execute_input": "2025-10-05T14:16:58.747194Z",
     "iopub.status.idle": "2025-10-05T14:17:03.330197Z",
     "shell.execute_reply.started": "2025-10-05T14:16:58.747156Z",
     "shell.execute_reply": "2025-10-05T14:17:03.329253Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:41.049348Z",
     "start_time": "2025-10-05T14:36:39.142907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\modeling_table_engineered_from_base.csv\n",
      "Engineered shape: (6419, 538)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Feature selection (use combined ranking if available) and prepare X/y",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell 8: Feature selection\n",
    "fr_path = os.path.join(INPUT_DIR, \"results\", \"feature_ranking_combined.csv\")\n",
    "if os.path.exists(fr_path):\n",
    "    fr = pd.read_csv(fr_path)\n",
    "    cand_feats = [f for f in fr['feature'].tolist() if f in m.columns]\n",
    "    print(\"Loaded external feature ranking, candidate features:\", len(cand_feats))\n",
    "else:\n",
    "    cand_feats = [c for c in m.select_dtypes(include=[np.number]).columns if c not in ['amount_new_house_transactions','y_log1p','month_num','year']]\n",
    "    print(\"Fallback numeric candidate features:\", len(cand_feats))\n",
    "\n",
    "# Keep features with >1 unique value\n",
    "features = [f for f in cand_feats if m[f].nunique()>1]\n",
    "# Prioritize lags and core new-house features\n",
    "priority = [f for f in features if ('lag' in f or 'roll' in f or 'new_house' in f or 'new' in f or 'amount_new_house' in f or 'num_new_house' in f)]\n",
    "others = [f for f in features if f not in priority]\n",
    "final_features = (priority[:60] + others[:40])[:120]\n",
    "print(\"Final features count:\", len(final_features))\n",
    "print(\"Sample features:\", final_features[:20])\n",
    "\n",
    "X = m[final_features].fillna(0)\n",
    "y = m['amount_new_house_transactions'].values\n",
    "y_log = m['y_log1p'].values\n",
    "\n",
    "# encode object features if any\n",
    "for c in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[c] = le.fit_transform(X[c].astype(str))\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:17:14.590855Z",
     "iopub.execute_input": "2025-10-05T14:17:14.591210Z",
     "iopub.status.idle": "2025-10-05T14:17:14.727419Z",
     "shell.execute_reply.started": "2025-10-05T14:17:14.591179Z",
     "shell.execute_reply": "2025-10-05T14:17:14.726126Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:36:44.181948Z",
     "start_time": "2025-10-05T14:36:44.151409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded external feature ranking, candidate features: 294\n",
      "Final features count: 87\n",
      "Sample features: ['amount_new_house_transactions_log1p', 'amount_new_house_transactions_lag3', 'area_new_house_transactions_lag1', 'amount_new_house_transactions_lag3_log1p', 'amount_new_house_transactions_lag1', 'num_new_house_transactions', 'price_new_house_transactions_lag3_log1p', 'num_new_house_transactions_roll3', 'new_near_price_new_house_transactions_nearby_sectors_log1p', 'new_near_total_price_per_unit_new_house_transactions_nearby_sectors', 'price_new_house_transactions_lag3', 'price_new_house_transactions_lag1', 'area_new_house_transactions_lag3', 'num_new_house_transactions_lag1_log1p', 'price_new_house_transactions_lag1_log1p', 'new_near_total_price_per_unit_new_house_transactions_nearby_sectors_log1p', 'area_new_house_transactions', 'area_new_house_transactions_lag3_log1p', 'price_new_house_transactions_log1p', 'total_price_per_unit_new_house_transactions_log1p']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Rolling CV training (two-stage) and OOF evaluation (LightGBM + XGBoost stacking)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# Cell 9: Rolling CV training for two-stage with stacking meta-model\n# ============================================================\n\nmonths = sorted(m['month'].unique().tolist())\nn_months = len(months)\nN_FOLDS = 5\nfold_months = []\n\n# --- build rolling time-based folds ---\nfor i in range(N_FOLDS):\n    train_end = int((i + 1) * n_months / (N_FOLDS + 1))\n    val_start = train_end\n    val_end = min(train_end + max(1, n_months // (N_FOLDS + 1)), n_months)\n    if train_end == 0 or val_start >= val_end:\n        continue\n    fold_months.append((months[:train_end], months[val_start:val_end]))\nprint(\"folds:\", len(fold_months))\n\n# --- allocate arrays ---\noof_preds = np.zeros(len(X))\noof_p_zero = np.zeros(len(X))\nmeta_train = []        # meta features per fold\nmeta_idx = []\ntest_meta_preds = []   # store test predictions per fold\n\n# --- prepare test features aligned with final_features ---\ntest_df = data[\"test.csv\"].copy()\nfor f in final_features:\n    if f not in test_df.columns:\n        test_df[f] = 0\nX_test = test_df[final_features].fillna(0)\n\n# ============================================================\n# CV loop\n# ============================================================\nfor fold, (train_ms, val_ms) in enumerate(fold_months):\n    print(f\"\\n================ Fold {fold} ================\")\n    train_idx = m['month'].isin(train_ms)\n    val_idx = m['month'].isin(val_ms)\n\n    X_tr, X_val = X.loc[train_idx], X.loc[val_idx]\n    y_tr_log, y_val_log = y_log[train_idx], y_log[val_idx]\n    y_tr_orig, y_val_orig = y[train_idx], y[val_idx]\n\n    # ------------------------------------------------------------\n    # 1️⃣ Classifier: predict whether amount_new_house_transactions == 0\n    # ------------------------------------------------------------\n    y_clf_tr = pd.Series((y_tr_orig == 0).astype(int))\n    y_clf_val = pd.Series((y_val_orig == 0).astype(int))\n\n    if y_clf_tr.nunique() > 1 and LGB_AVAILABLE:\n        dtr_clf = lgb.Dataset(X_tr, label=y_clf_tr)\n        dval_clf = lgb.Dataset(X_val, label=y_clf_val, reference=dtr_clf)\n        clf = lgb.train(\n            {\n                'objective': 'binary',\n                'metric': 'auc',\n                'learning_rate': 0.05,\n                'num_leaves': 64,\n                'verbosity': -1,\n                'seed': SEED\n            },\n            dtr_clf,\n            num_boost_round=1000,\n            valid_sets=[dtr_clf, dval_clf],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(0)\n            ]\n        )\n        p_zero_val = clf.predict(X_val, num_iteration=clf.best_iteration)\n        p_zero_test = clf.predict(X_test, num_iteration=clf.best_iteration)\n\n    elif y_clf_tr.nunique() > 1:\n        lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n        lr.fit(X_tr.fillna(0), y_clf_tr)\n        p_zero_val = lr.predict_proba(X_val.fillna(0))[:, 1]\n        p_zero_test = lr.predict_proba(X_test.fillna(0))[:, 1]\n\n    else:\n        # Fallback: only one class present\n        const_prob = float(y_clf_tr.iloc[0])\n        print(f\"⚠️  Fold {fold}: Only one class in training (all {const_prob}). Using constant fallback.\")\n        p_zero_val = np.full(len(X_val), const_prob, dtype=float)\n        p_zero_test = np.full(len(X_test), const_prob, dtype=float)\n\n    # ------------------------------------------------------------\n    # 2️⃣ Regressor: predict transaction amount (log-scale)\n    # ------------------------------------------------------------\n    reg_train_mask = train_idx & (m['amount_new_house_transactions'] > 0)\n\n    if reg_train_mask.sum() >= max(50, int(0.1 * train_idx.sum())) and LGB_AVAILABLE:\n        dtr = lgb.Dataset(X.loc[reg_train_mask], label=y_log[reg_train_mask])\n        dval = lgb.Dataset(X_val, label=y_val_log, reference=dtr)\n        reg = lgb.train(\n            {\n                'objective': 'regression',\n                'metric': 'mae',\n                'learning_rate': 0.03,\n                'num_leaves': 128,\n                'feature_fraction': 0.8,\n                'bagging_fraction': 0.8,\n                'verbosity': -1,\n                'seed': SEED\n            },\n            dtr,\n            num_boost_round=2000,\n            valid_sets=[dtr, dval],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(0)\n            ]\n        )\n        p_reg_val_log = reg.predict(X_val, num_iteration=reg.best_iteration)\n        p_reg_test_log = reg.predict(X_test, num_iteration=reg.best_iteration)\n\n    elif XGB_AVAILABLE:\n        dtr = xgb.DMatrix(X.loc[train_idx], label=y_log[train_idx])\n        dval = xgb.DMatrix(X_val, label=y_val_log)\n        reg = xgb.train(\n            {\n                'objective': 'reg:squarederror',\n                'eta': 0.03,\n                'max_depth': 7,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'seed': SEED\n            },\n            dtr,\n            num_boost_round=1000,\n            evals=[(dval, 'val')],\n            early_stopping_rounds=50,\n            verbose_eval=False\n        )\n        p_reg_val_log = reg.predict(xgb.DMatrix(X_val))\n        p_reg_test_log = reg.predict(xgb.DMatrix(X_test))\n\n    else:\n        rf = RandomForestRegressor(n_estimators=400, random_state=SEED, n_jobs=-1)\n        rf.fit(X_tr.fillna(0), y_log[train_idx])\n        p_reg_val_log = rf.predict(X_val.fillna(0))\n        p_reg_test_log = rf.predict(X_test.fillna(0))\n\n    # Convert log predictions back to original scale\n    p_reg_val = np.expm1(np.clip(p_reg_val_log, -20, 50))\n    p_reg_test = np.expm1(np.clip(p_reg_test_log, -20, 50))\n\n    # ------------------------------------------------------------\n    # 3️⃣ Combine classifier + regressor for final predictions\n    # ------------------------------------------------------------\n    p_final_val = (1 - p_zero_val) * p_reg_val\n\n    # store OOF and meta features\n    oof_preds[val_idx] = p_reg_val\n    oof_p_zero[val_idx] = p_zero_val\n    meta_train.append(np.vstack([p_reg_val, p_zero_val]).T)\n    meta_idx.append(np.where(val_idx)[0])\n    test_meta_preds.append(np.vstack([p_reg_test, p_zero_test]).T)\n\n    # evaluate fold\n    evaluate_preds(y_val_orig, p_final_val, name=f\"fold{fold}\")\n\n# ============================================================\n# Build meta matrices for stacking\n# ============================================================\nmeta_X = np.zeros((len(X), 2))  # [reg_pred, p_zero]\nfor idxs, arr in zip(meta_idx, meta_train):\n    meta_X[idxs, :] = arr\n\n# Average test meta predictions\ntest_meta_avg = np.mean(np.stack(test_meta_preds, axis=2), axis=2)  # (n_test, 2)\n\n# Final OOF evaluation (combined)\noof_combined = (1 - oof_p_zero) * oof_preds\nevaluate_preds(y, oof_combined, name=\"OOF\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:20:16.113140Z",
     "iopub.execute_input": "2025-10-05T14:20:16.113489Z",
     "iopub.status.idle": "2025-10-05T14:20:44.002776Z",
     "shell.execute_reply.started": "2025-10-05T14:20:16.113469Z",
     "shell.execute_reply": "2025-10-05T14:20:44.000736Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:37:02.586634Z",
     "start_time": "2025-10-05T14:36:50.010520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folds: 5\n",
      "\n",
      "================ Fold 0 ================\n",
      "⚠️  Fold 0: Only one class in training (all 0.0). Using constant fallback.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[366]\ttraining's l1: 0.0125041\tvalid_1's l1: 0.0460823\n",
      "fold0 | two-stage score: 0.954802 | MAE: 2627.1292 | frac_bad: 0.0011\n",
      "\n",
      "================ Fold 1 ================\n",
      "⚠️  Fold 1: Only one class in training (all 0.0). Using constant fallback.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[383]\ttraining's l1: 0.00704008\tvalid_1's l1: 0.0283713\n",
      "fold1 | two-stage score: 0.972292 | MAE: 2667.5879 | frac_bad: 0.0000\n",
      "\n",
      "================ Fold 2 ================\n",
      "⚠️  Fold 2: Only one class in training (all 0.0). Using constant fallback.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[529]\ttraining's l1: 0.00350936\tvalid_1's l1: 0.0198539\n",
      "fold2 | two-stage score: 0.980271 | MAE: 884.1121 | frac_bad: 0.0000\n",
      "\n",
      "================ Fold 3 ================\n",
      "⚠️  Fold 3: Only one class in training (all 0.0). Using constant fallback.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's l1: 0.00511719\tvalid_1's l1: 0.0166496\n",
      "fold3 | two-stage score: 0.983658 | MAE: 942.0963 | frac_bad: 0.0000\n",
      "\n",
      "================ Fold 4 ================\n",
      "⚠️  Fold 4: Only one class in training (all 0.0). Using constant fallback.\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[440]\ttraining's l1: 0.00345758\tvalid_1's l1: 0.0138403\n",
      "fold4 | two-stage score: 0.986132 | MAE: 512.7657 | frac_bad: 0.0000\n",
      "OOF | two-stage score: 0.832490 | MAE: 5145.5481 | frac_bad: 0.0002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.8324899331512069,\n",
       " 'mae': 5145.548145114742,\n",
       " 'frac_bad': 0.00015578750584203146}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Train stacking meta-model (Ridge) on meta features and produce final test predictions & submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Stacking meta-model (Ridge) and final submission\n# Use only rows where we have meta features (all rows should have)\n# meta_X columns: [reg_pred, p_zero] for each training row\nmeta_y = y  # original-scale target\n\n# Fit Ridge on meta features; but target for meta should be original-scale target\nridge = RidgeCV(alphas=[0.1,1.0,10.0], cv=5)\nridge.fit(meta_X, meta_y)\nmeta_test_pred = ridge.predict(test_meta_avg)  # direct predicted amounts\n\n# Alternatively, blend: final = (1 - mean p_zero) * mean reg_pred then adjust with ridge residual\n# But using ridge on meta is straightforward\n# Evaluate OOF after stacking (predict meta_X)\noof_stack = ridge.predict(meta_X)\nevaluate_preds(y, oof_stack, name=\"OOF_stacked\")\n\n# Prepare test submission predictions\nfinal_test_preds = meta_test_pred\n# Postprocess: clip negative & floor tiny values to 0 to avoid huge percentage errors on zeros\nfinal_test_preds = np.clip(final_test_preds, 0, None)\nfinal_test_preds = np.where(final_test_preds < 1.0, 0.0, final_test_preds)\n\n# Build submission respecting original test id format\ntest_original = data[\"test.csv\"].copy()\nif 'id' in test_original.columns:\n    sub = pd.DataFrame({'id': test_original['id'], 'new_house_transaction_amount': final_test_preds})\nelse:\n    if {'month','sector'}.issubset(test_original.columns):\n        sub = pd.DataFrame({'id': test_original['month'].astype(str) + \"_sector \" + test_original['sector'].astype(str),\n                            'new_house_transaction_amount': final_test_preds})\n    else:\n        sub = pd.DataFrame({'id': [f\"sample_{i}\" for i in range(len(final_test_preds))],\n                            'new_house_transaction_amount': final_test_preds})\n\nsave_df(sub, \"submission_advanced_v2_stacked.csv\")\nprint(\"Submission saved. Upload to Kaggle.\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-10-05T14:22:00.145665Z",
     "iopub.execute_input": "2025-10-05T14:22:00.146387Z",
     "iopub.status.idle": "2025-10-05T14:22:00.227391Z",
     "shell.execute_reply.started": "2025-10-05T14:22:00.146360Z",
     "shell.execute_reply": "2025-10-05T14:22:00.226336Z"
    },
    "ExecuteTime": {
     "end_time": "2025-10-05T14:37:06.249359Z",
     "start_time": "2025-10-05T14:37:06.202895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF_stacked | two-stage score: 0.592481 | MAE: 8009.7168 | frac_bad: 0.2014\n",
      "Saved: C:\\Users\\Mitudru\\Documents\\ML Project\\realestateprediction\\results\\advanced_v2_outputs\\submission_advanced_v2_stacked.csv\n",
      "Submission saved. Upload to Kaggle.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ]
}
